<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[Zabbix] zabbix监控域名证书过期时间]]></title>
    <url>%2F2018%2F12%2F17%2Fzabbix%E7%9B%91%E6%8E%A7%E5%9F%9F%E5%90%8D%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E6%97%B6%E9%97%B4%2F</url>
    <content type="text"><![CDATA[1、编辑脚本more /usr/local/zabbix/script/check_sslcert_expire_script.sh12345678910111213141516#!/bin/bash#传入第一参数指定输入的域名domain=$1#传入的第二个参数指定https的端口port=$2#获取证书到期时间expired_date=`echo |openssl s_client -servername $&#123;domain&#125; -connect $&#123;domain&#125;:$&#123;port&#125; 2&gt;/dev/null | openssl x509 -noout -dates |sed -n &apos;s/notAfter=//p&apos;`#$&#123;expired_date&#125;参数一定要加上双引号，否则按多个参数处理#与当前系统日期时间做对比，查看证书过期时间if [ -n &quot;$&#123;expired_date&#125;&quot; ];then expired_seconds=`date &apos;+%s&apos; --date &quot;$&#123;expired_date&#125;&quot;` now_seconds=`date &apos;+%s&apos;` echo &quot;($&#123;expired_seconds&#125;-$&#123;now_seconds&#125;)/24/3600&quot; | bcelse :fi 2、修改zabbix_agentd.conf配置文件，添加下面一行1UserParameter=check_ssl_cert_expire[*],sh /usr/local/zabbix/script/check_sslcert_expire_script.sh $1 $2 重新启动agent 3、检查获取参数是否正常1234/usr/local/zabbix/bin/zabbix_get -s 127.0.0.1 -p 10050 -k check_ssl_cert_expire[www.baidu.com,443]159/usr/local/zabbix/bin/zabbix_get -s 127.0.0.1 -p 10050 -k check_ssl_cert_expire[www.masterdax.com,443]85 4、打开zabbix,web控制台添加模板，监控项设置触发器，证书到期时触发报警]]></content>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Oracle] 为oracle-sqlplus安装rlwrap]]></title>
    <url>%2F2018%2F12%2F17%2Foracle%E5%AE%89%E8%A3%85rlwrap%2F</url>
    <content type="text"><![CDATA[1、安装rlwrap-0.42-1.1.x86_64.rpm方式下载rlwrap-0.42-1.1.x86_64.rpm1wget ftp://ftp.pbone.net/mirror/ftp5.gwdg.de/pub/opensuse/repositories/home%3A/Ledest%3A/misc/CentOS_7/src/rlwrap-0.42-1.1.x86_64.rpm 2、安装rlwarp1yum localinstall rlwrap-0.42-1.1.x86_64.rpm 3、修改环境变量文件grep ‘rlwrap’ ~/.bash_profilealias sql=’rlwrap sqlplus “/ as sysdba”‘alias rman=’rlwrap rman’ 4、如果找不到x86_64类型的文件，下载 rlwrap-0.42-1.1.src.rpm1wget ftp://ftp.pbone.net/mirror/ftp5.gwdg.de/pub/opensuse/repositories/home%3A/Ledest%3A/misc/CentOS_7/src/rlwrap-0.42-1.1.src.rpm 5、安装依赖12yum install rpm-build -yyum install readline* libtermcap-devel* -y 6、安装rlwarp1234567891011121314shell &gt; rpm -i rlwrap-0.42-1.1.src.rpm warning: rlwrap-0.42-1.1.src.rpm: Header V3 RSA/SHA1 Signature, key ID 93680782: NOKEYshell &gt; cd rpmbuild/SOURCESshell &gt; tar -zxvf rlwrap-0.42.tar.gz shell &gt; cd rlwrap-0.42shell &gt; lsaclocal.m4 BUGS completions configure COPYING filters Makefile.am NEWS README test toolsAUTHORS ChangeLog config.h.in configure.ac doc INSTALL Makefile.in PLEA src TODOshell &gt; ./configureshell &gt; make &amp;&amp; make install#### 7、之后修改环境变量 grep ‘rlwrap’ ~/.bash_profilealias sql=’rlwrap sqlplus “/ as sysdba”‘alias rman=’rlwrap rman’`]]></content>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Oracle] centos7安装rac报错ohasd failed to start]]></title>
    <url>%2F2018%2F12%2F17%2Fcentos7%E5%AE%89%E8%A3%85rac%E6%8A%A5%E9%94%99ohasd%20failed%20to%20start%2F</url>
    <content type="text"><![CDATA[1、具体报错信息安装完成grid之后，在执行/u01/app/11.2.0.4/grid/root.sh命令时报以下错误：12ohasd failed to startFailed to start the Clusterware. Last 20 lines of the alert log follow 2、单独在linux 7中为ohasd设置一个服务。1、创建服务ohas.service的服务文件并赋予权限12touch /usr/lib/systemd/system/ohas.servicechmod 777 /usr/lib/systemd/system/ohas.service 2、往ohas.service服务文件添加启动ohasd的相关信息vi /usr/lib/systemd/system/ohas.service12345678910[Unit]Description=Oracle High Availability ServicesAfter=syslog.target[Service]ExecStart=/etc/init.d/init.ohasd run &gt;/dev/null 2&gt;&amp;1 Type=simpleRestart=always[Install]WantedBy=multi-user.target 3、重新加载守护进程1systemctl daemon-reload 设置守护进程自动启动1systemctl enable ohas.service 手工启动ohas服务1systemctl start ohas.service 4、重新运行root.sh脚本1sh root.sh 5：查看ohas服务状态1systemctl status ohas.service]]></content>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Oracle] oracle udev配置]]></title>
    <url>%2F2018%2F12%2F17%2Foracle%20udev%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1、添加裸设备增加磁盘vda,vdb,vdc在节点1上分别对vda，vdb，vdc三块盘分区123fdisk /dev/vdafdisk /dev/vdbfdisk /dev/vdc 节点1查看设备信息：12345678910[root@rac1 ~]# ll /dev/vd*brw-rw---- 1 root disk 252, 0 Dec 14 10:56 /dev/vdabrw-rw---- 1 root disk 252, 1 Dec 14 10:56 /dev/vda1brw-rw---- 1 root disk 252, 2 Dec 14 10:56 /dev/vda2brw-rw---- 1 root disk 252, 16 Dec 14 11:41 /dev/vdbbrw-rw---- 1 root disk 252, 17 Dec 14 11:41 /dev/vdb1brw-rw---- 1 root disk 252, 32 Dec 14 11:34 /dev/vdcbrw-rw---- 1 root disk 252, 33 Dec 14 11:34 /dev/vdc1brw-rw---- 1 root disk 252, 48 Dec 14 11:34 /dev/vddbrw-rw---- 1 root disk 252, 49 Dec 14 11:34 /dev/vdd1 节点2查看设备信息：1234567[root@rac2 ~]# ll /dev/vd*brw-rw---- 1 root disk 252, 0 Dec 14 10:56 /dev/vdabrw-rw---- 1 root disk 252, 1 Dec 14 10:56 /dev/vda1brw-rw---- 1 root disk 252, 2 Dec 14 10:56 /dev/vda2brw-rw---- 1 root disk 252, 16 Dec 14 10:56 /dev/vdbbrw-rw---- 1 root disk 252, 32 Dec 14 10:56 /dev/vdcbrw-rw---- 1 root disk 252, 48 Dec 14 10:56 /dev/vdd major device number可以看作是设备驱动程序，被同一设备驱动程序管理的设备有相同的major device number.这个数字实际是Kernel中device driver table 的索引，这个表保存着不同设备驱动程序。（kvm虚拟机virto的驱动对应的major device number值为252，scsi的驱动对应的major device number值为8，裸设备为162）minor device number用来代表被访问的具体设备。就是说Kernel根据major device number 找到设备驱动程序，然后再从minor device number 获得设备位置等属性。 2、修改配置文件节点1vim /etc/udev/rules.d/60-raw.rules12345678ACTION==&quot;add&quot;, KERNEL==&quot;/dev/vdb1&quot;,RUN+=&quot;/bin/raw /dev/raw/raw1 %N&quot;ACTION==&quot;add&quot;, ENV&#123;MAJOR&#125;==&quot;252&quot;,ENV&#123;MINOR&#125;==&quot;17&quot;,RUN+=&quot;/bin/raw /dev/raw/raw1 %M %m&quot;ACTION==&quot;add&quot;, KERNEL==&quot;/dev/vdc1&quot;,RUN+=&quot;/bin/raw /dev/raw/raw2 %N&quot;ACTION==&quot;add&quot;, ENV&#123;MAJOR&#125;==&quot;252&quot;,ENV&#123;MINOR&#125;==&quot;33&quot;,RUN+=&quot;/bin/raw /dev/raw/raw2 %M %m&quot;ACTION==&quot;add&quot;, KERNEL==&quot;/dev/vdd1&quot;,RUN+=&quot;/bin/raw /dev/raw/raw3 %N&quot;ACTION==&quot;add&quot;, ENV&#123;MAJOR&#125;==&quot;252&quot;,ENV&#123;MINOR&#125;==&quot;49&quot;,RUN+=&quot;/bin/raw /dev/raw/raw3 %M %m&quot;KERNEL==&quot;raw[1-3]&quot;, OWNER=&quot;grid&quot;, GROUP=&quot;oinstall&quot;, MODE=&quot;640&quot; 节点2vim /etc/udev/rules.d/60-raw.rules12345678ACTION==&quot;add&quot;, KERNEL==&quot;/dev/vdb1&quot;,RUN+=&quot;/bin/raw /dev/raw/raw1 %N&quot;ACTION==&quot;add&quot;, ENV&#123;MAJOR&#125;==&quot;252&quot;,ENV&#123;MINOR&#125;==&quot;17&quot;,RUN+=&quot;/bin/raw /dev/raw/raw1 %M %m&quot;ACTION==&quot;add&quot;, KERNEL==&quot;/dev/vdc1&quot;,RUN+=&quot;/bin/raw /dev/raw/raw2 %N&quot;ACTION==&quot;add&quot;, ENV&#123;MAJOR&#125;==&quot;252&quot;,ENV&#123;MINOR&#125;==&quot;33&quot;,RUN+=&quot;/bin/raw /dev/raw/raw2 %M %m&quot;ACTION==&quot;add&quot;, KERNEL==&quot;/dev/vdd1&quot;,RUN+=&quot;/bin/raw /dev/raw/raw3 %N&quot;ACTION==&quot;add&quot;, ENV&#123;MAJOR&#125;==&quot;252&quot;,ENV&#123;MINOR&#125;==&quot;49&quot;,RUN+=&quot;/bin/raw /dev/raw/raw3 %M %m&quot;KERNEL==&quot;raw[1-3]&quot;, OWNER=&quot;grid&quot;, GROUP=&quot;oinstall&quot;, MODE=&quot;640&quot; 上述操作需要在双节点执行！且确保在双节点均可以看到裸设备文件，以及grid(或者oracle)用户具有对裸设备的权限 3、启动udev12345678910centos6使用start_udevcentos7之后使用/sbin/udevadm control --reload-rules/sbin/udevadm trigger或者/sbin/udevadm trigger --type=devices --action=change查看设备状态：ll /dev/raw/raw*在配置过程中，启动udev之后没有看到设备，重启服务器后才生效。 4、使用scis驱动方式添加磁盘，用udev根据uuid方式绑定磁盘：添加sda，sdb，sdc三块盘，获取共享磁盘的uuid123456[root@rac1 ~]# /usr/lib/udev/scsi_id -g -u /dev/sda0QEMU_QEMU_HARDDISK_drive-scsi0-0-0-0[root@rac1 ~]# /usr/lib/udev/scsi_id -g -u /dev/sdb0QEMU_QEMU_HARDDISK_drive-scsi0-0-0-2[root@rac1 ~]# /usr/lib/udev/scsi_id -g -u /dev/sdc0QEMU_QEMU_HARDDISK_drive-scsi0-0-0-1 5、按以下格式写入[/etc/udev/rules.d/99-my-asmdevices.rules]文件，每个设备一行，中间不允许换行节点1：vim /etc/udev/rules.d/99-my-asmdevices.rules123KERNEL==&quot;sd*[!0-9]&quot;, ENV&#123;DEVTYPE&#125;==&quot;disk&quot;, SUBSYSTEM==&quot;block&quot;, PROGRAM==&quot;/usr/lib/udev/scsi_id -g -u -d $devnode&quot;, RESULT==&quot;0QEMU_QEMU_HARDDISK_drive-scsi0-0-0-0&quot;, RUN+=&quot;/bin/sh -c &apos;mknod /dev/asmdisk-vote b $major $minor; chown grid:asmadmin /dev/asmdisk-vote; chmod 0660 /dev/asmdisk-vote&apos;&quot;KERNEL==&quot;sd*[!0-9]&quot;, ENV&#123;DEVTYPE&#125;==&quot;disk&quot;, SUBSYSTEM==&quot;block&quot;, PROGRAM==&quot;/usr/lib/udev/scsi_id -g -u -d $devnode&quot;, RESULT==&quot;0QEMU_QEMU_HARDDISK_drive-scsi0-0-0-2&quot;, RUN+=&quot;/bin/sh -c &apos;mknod /dev/asmdisk-arch b $major $minor; chown grid:asmadmin /dev/asmdisk-arch; chmod 0660 /dev/asmdisk-arch&apos;&quot;KERNEL==&quot;sd*[!0-9]&quot;, ENV&#123;DEVTYPE&#125;==&quot;disk&quot;, SUBSYSTEM==&quot;block&quot;, PROGRAM==&quot;/usr/lib/udev/scsi_id -g -u -d $devnode&quot;, RESULT==&quot;0QEMU_QEMU_HARDDISK_drive-scsi0-0-0-1&quot;, RUN+=&quot;/bin/sh -c &apos;mknod /dev/asmdisk-data b $major $minor; chown grid:asmadmin /dev/asmdisk-data; chmod 0660 /dev/asmdisk-data&apos;&quot; 节点2：vim /etc/udev/rules.d/99-my-asmdevices.rules123KERNEL==&quot;sd*[!0-9]&quot;, ENV&#123;DEVTYPE&#125;==&quot;disk&quot;, SUBSYSTEM==&quot;block&quot;, PROGRAM==&quot;/usr/lib/udev/scsi_id -g -u -d $devnode&quot;, RESULT==&quot;0QEMU_QEMU_HARDDISK_drive-scsi0-0-0-0&quot;, RUN+=&quot;/bin/sh -c &apos;mknod /dev/asmdisk-vote b $major $minor; chown grid:asmadmin /dev/asmdisk-vote; chmod 0660 /dev/asmdisk-vote&apos;&quot;KERNEL==&quot;sd*[!0-9]&quot;, ENV&#123;DEVTYPE&#125;==&quot;disk&quot;, SUBSYSTEM==&quot;block&quot;, PROGRAM==&quot;/usr/lib/udev/scsi_id -g -u -d $devnode&quot;, RESULT==&quot;0QEMU_QEMU_HARDDISK_drive-scsi0-0-0-2&quot;, RUN+=&quot;/bin/sh -c &apos;mknod /dev/asmdisk-arch b $major $minor; chown grid:asmadmin /dev/asmdisk-arch; chmod 0660 /dev/asmdisk-arch&apos;&quot;KERNEL==&quot;sd*[!0-9]&quot;, ENV&#123;DEVTYPE&#125;==&quot;disk&quot;, SUBSYSTEM==&quot;block&quot;, PROGRAM==&quot;/usr/lib/udev/scsi_id -g -u -d $devnode&quot;, RESULT==&quot;0QEMU_QEMU_HARDDISK_drive-scsi0-0-0-1&quot;, RUN+=&quot;/bin/sh -c &apos;mknod /dev/asmdisk-data b $major $minor; chown grid:asmadmin /dev/asmdisk-data; chmod 0660 /dev/asmdisk-data&apos;&quot; 6、启动udev1234567centos6使用start_udevcentos7之后使用/sbin/udevadm control --reload-rules/sbin/udevadm trigger或者/sbin/udevadm trigger --type=devices --action=change 查看设备状态1234[root@rac1 ~]# ll /dev/asmdisk-*brw-rw---- 1 grid asmadmin 8, 16 Dec 14 14:18 /dev/asmdisk-archbrw-rw---- 1 grid asmadmin 8, 32 Dec 14 14:18 /dev/asmdisk-databrw-rw---- 1 grid asmadmin 8, 0 Dec 14 14:18 /dev/asmdisk-vote 7、使用udevadm查看设备信息udevadm info -a -p /sys/block/sda123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899Udevadm info starts with the device specified by the devpath and thenwalks up the chain of parent devices. It prints for every devicefound, all possible attributes in the udev rules key format.A rule to match, can be composed by the attributes of the deviceand the attributes from one single parent device. looking at device &apos;/devices/pci0000:00/0000:00:0d.0/virtio5/host2/target2:0:0/2:0:0:0/block/sda&apos;: KERNEL==&quot;sda&quot; SUBSYSTEM==&quot;block&quot; DRIVER==&quot;&quot; ATTR&#123;ro&#125;==&quot;0&quot; ATTR&#123;size&#125;==&quot;41943040&quot; ATTR&#123;stat&#125;==&quot; 521604 4 820941 5583621 355886 1462 1263929 7839050 0 9373468 13420933&quot; ATTR&#123;range&#125;==&quot;16&quot; ATTR&#123;discard_alignment&#125;==&quot;0&quot; ATTR&#123;events&#125;==&quot;&quot; ATTR&#123;ext_range&#125;==&quot;256&quot; ATTR&#123;events_poll_msecs&#125;==&quot;-1&quot; ATTR&#123;alignment_offset&#125;==&quot;0&quot; ATTR&#123;inflight&#125;==&quot; 0 0&quot; ATTR&#123;removable&#125;==&quot;0&quot; ATTR&#123;capability&#125;==&quot;50&quot; ATTR&#123;events_async&#125;==&quot;&quot; looking at parent device &apos;/devices/pci0000:00/0000:00:0d.0/virtio5/host2/target2:0:0/2:0:0:0&apos;: KERNELS==&quot;2:0:0:0&quot; SUBSYSTEMS==&quot;scsi&quot; DRIVERS==&quot;sd&quot; ATTRS&#123;rev&#125;==&quot;1.5.&quot; ATTRS&#123;type&#125;==&quot;0&quot; ATTRS&#123;scsi_level&#125;==&quot;6&quot; ATTRS&#123;model&#125;==&quot;QEMU HARDDISK &quot; ATTRS&#123;state&#125;==&quot;running&quot; ATTRS&#123;unpriv_sgio&#125;==&quot;0&quot; ATTRS&#123;queue_type&#125;==&quot;none&quot; ATTRS&#123;iodone_cnt&#125;==&quot;0xd660c&quot; ATTRS&#123;iorequest_cnt&#125;==&quot;0xd664c&quot; ATTRS&#123;device_busy&#125;==&quot;0&quot; ATTRS&#123;evt_capacity_change_reported&#125;==&quot;0&quot; ATTRS&#123;timeout&#125;==&quot;30&quot; ATTRS&#123;evt_media_change&#125;==&quot;0&quot; ATTRS&#123;ioerr_cnt&#125;==&quot;0x18&quot; ATTRS&#123;queue_depth&#125;==&quot;128&quot; ATTRS&#123;vendor&#125;==&quot;QEMU &quot; ATTRS&#123;evt_soft_threshold_reached&#125;==&quot;0&quot; ATTRS&#123;device_blocked&#125;==&quot;0&quot; ATTRS&#123;evt_mode_parameter_change_reported&#125;==&quot;0&quot; ATTRS&#123;evt_lun_change_reported&#125;==&quot;0&quot; ATTRS&#123;evt_inquiry_change_reported&#125;==&quot;0&quot; ATTRS&#123;dh_state&#125;==&quot;detached&quot; ATTRS&#123;iocounterbits&#125;==&quot;32&quot; ATTRS&#123;inquiry&#125;==&quot;&quot; ATTRS&#123;vpd_pg83&#125;==&quot;&quot; ATTRS&#123;eh_timeout&#125;==&quot;10&quot; looking at parent device &apos;/devices/pci0000:00/0000:00:0d.0/virtio5/host2/target2:0:0&apos;: KERNELS==&quot;target2:0:0&quot; SUBSYSTEMS==&quot;scsi&quot; DRIVERS==&quot;&quot; looking at parent device &apos;/devices/pci0000:00/0000:00:0d.0/virtio5/host2&apos;: KERNELS==&quot;host2&quot; SUBSYSTEMS==&quot;scsi&quot; DRIVERS==&quot;&quot; looking at parent device &apos;/devices/pci0000:00/0000:00:0d.0/virtio5&apos;: KERNELS==&quot;virtio5&quot; SUBSYSTEMS==&quot;virtio&quot; DRIVERS==&quot;virtio_scsi&quot; ATTRS&#123;device&#125;==&quot;0x0008&quot; ATTRS&#123;features&#125;==&quot;0110000000000000000000000000110000000000000000000000000000000000&quot; ATTRS&#123;status&#125;==&quot;0x00000007&quot; ATTRS&#123;vendor&#125;==&quot;0x1af4&quot; looking at parent device &apos;/devices/pci0000:00/0000:00:0d.0&apos;: KERNELS==&quot;0000:00:0d.0&quot; SUBSYSTEMS==&quot;pci&quot; DRIVERS==&quot;virtio-pci&quot; ATTRS&#123;irq&#125;==&quot;10&quot; ATTRS&#123;subsystem_vendor&#125;==&quot;0x1af4&quot; ATTRS&#123;broken_parity_status&#125;==&quot;0&quot; ATTRS&#123;class&#125;==&quot;0x010000&quot; ATTRS&#123;driver_override&#125;==&quot;(null)&quot; ATTRS&#123;consistent_dma_mask_bits&#125;==&quot;64&quot; ATTRS&#123;dma_mask_bits&#125;==&quot;64&quot; ATTRS&#123;local_cpus&#125;==&quot;ff&quot; ATTRS&#123;device&#125;==&quot;0x1004&quot; ATTRS&#123;enable&#125;==&quot;1&quot; ATTRS&#123;msi_bus&#125;==&quot;&quot; ATTRS&#123;local_cpulist&#125;==&quot;0-7&quot; ATTRS&#123;vendor&#125;==&quot;0x1af4&quot; ATTRS&#123;subsystem_device&#125;==&quot;0x0008&quot; ATTRS&#123;numa_node&#125;==&quot;-1&quot; ATTRS&#123;d3cold_allowed&#125;==&quot;0&quot; looking at parent device &apos;/devices/pci0000:00&apos;: KERNELS==&quot;pci0000:00&quot; SUBSYSTEMS==&quot;&quot; DRIVERS==&quot;&quot;]]></content>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql-AUTO_INCREMENT参数详解]]></title>
    <url>%2F2018%2F11%2F30%2Fmysql-AUTO_INCREMENT%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1、auto_increment的性质就是为新行生成一个唯一标识,例如1234567891011121314151617181920212223#创建测试表 CREATE TABLE animals ( id MEDIUMINT NOT NULL AUTO_INCREMENT, name CHAR(30) NOT NULL, PRIMARY KEY (id));#插入测试数据:INSERT INTO animals (name) VALUES (&apos;dog&apos;),(&apos;cat&apos;),(&apos;penguin&apos;), (&apos;lax&apos;),(&apos;whale&apos;),(&apos;ostrich&apos;);#查看测试结果：SELECT * FROM animals;+----+---------+| id | name |+----+---------+| 1 | dog || 2 | cat || 3 | penguin || 4 | lax || 5 | whale || 6 | ostrich |+----+---------+6 rows in set (0.00 sec) 2、上面的auto_increment列没有指定插入的值，mysql会自动序列数。也可以显示指定0或者null值来生成序列值，但是当sql_mode中no_auto_value_on_zero被启用时，如果auto_increment列被指定为0，则该值按0处理。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#查看sql_mode模式show variables like &apos;%sql_mode%&apos;;+---------------+------------------------------------------------------------------------------------------------------------------------+| Variable_name | Value |+---------------+------------------------------------------------------------------------------------------------------------------------+| sql_mode | STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |+---------------+------------------------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec)#插入数据insert into animals(id,name) values(0,&apos;pig&apos;);insert into animals(id,name) values(null,&apos;monkey&apos;); [aaaa]&gt; SELECT * FROM animals;+----+---------+| id | name |+----+---------+| 1 | dog || 2 | cat || 3 | penguin || 4 | lax || 5 | whale || 6 | ostrich || 7 | pig || 8 | monkey |+----+---------+8 rows in set (0.00 sec)#自动序列依次递增#修改sql_mode模式set @@sql_mode=&apos;NO_AUTO_VALUE_ON_ZERO,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&apos;;show variables like &apos;%sql_mode%&apos;;+---------------+----------------------------------------------------------------------------------------------------------------------------------------------+| Variable_name | Value |+---------------+----------------------------------------------------------------------------------------------------------------------------------------------+| sql_mode | NO_AUTO_VALUE_ON_ZERO,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |+---------------+----------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)#再次插入数据insert into animals(id,name) values(0,&apos;lion&apos;);insert into animals(id,name) values(null,&apos;tiger&apos;);root@db 01:38: [aaaa]&gt; SELECT * FROM animals;+----+---------+| id | name |+----+---------+| 0 | lion || 1 | dog || 2 | cat || 3 | penguin || 4 | lax || 5 | whale || 6 | ostrich || 7 | pig || 8 | monkey || 9 | tiger |+----+---------+10 rows in set (0.00 sec) 当sql_mode中NO_AUTO_VALUE_ON_ZERO为启用状态，插入的数值为0时，按0处理。12345678910111213#再次插入数据提示主键重复。root@db 01:41: [aaaa]&gt; insert into animals(id,name) values(0,&apos;giraffe&apos;);ERROR 1062 (23000): Duplicate entry &apos;0&apos; for key &apos;PRIMARY&apos;#修改sql_mode为原来配置set @@sql_mode=&apos;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&apos;;show variables like &apos;%sql_mode%&apos;;+---------------+------------------------------------------------------------------------------------------------------------------------+| Variable_name | Value |+---------------+------------------------------------------------------------------------------------------------------------------------+| sql_mode | STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |+---------------+------------------------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec) 3、当向自动增长列插入其他数据，序列值被重置，以便下一个自动生成的值从auto_increment对应列的最大值开始，例如：1234567891011121314151617181920INSERT INTO animals (id,name) VALUES(100,&apos;rabbit&apos;);INSERT INTO animals (id,name) VALUES(NULL,&apos;mouse&apos;);SELECT * FROM animals;+-----+---------+| id | name |+-----+---------+| 0 | lion || 1 | dog || 2 | cat || 3 | penguin || 4 | lax || 5 | whale || 6 | ostrich || 7 | pig || 8 | monkey || 9 | tiger || 100 | rabbit || 101 | mouse |+-----+---------+12 rows in set (0.00 sec) 当手动插入id为100的值之后，再次默认插入自动增长的值变为101。测试插入位于最小值和最大值之间的值：12345678910111213141516171819202122INSERT INTO animals (id,name) VALUES(50,&apos;giraffe&apos;);INSERT INTO animals (id,name) VALUES(NULL,&apos;tortoise&apos;);SELECT * FROM animals;+-----+----------+| id | name |+-----+----------+| 0 | lion || 1 | dog || 2 | cat || 3 | penguin || 4 | lax || 5 | whale || 6 | ostrich || 7 | pig || 8 | monkey || 9 | tiger || 50 | giraffe || 100 | rabbit || 101 | mouse || 102 | tortoise |+-----+----------+14 rows in set (0.00 sec) 手动插入位于最小值和最大值之间的值，对生成的序列值没有任何影响。 4、更新innodb类型的表中auto_increment的列值，不会重置auto_increment序列，但是myisam和ndb类型的表会重置。先以innodb类型表为例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#更新了auto_increment中列的值小于当前最大值update animals set id=10 where id=2;SELECT * FROM animals;+-----+----------+| id | name |+-----+----------+| 0 | lion || 1 | dog || 3 | penguin || 4 | lax || 5 | whale || 6 | ostrich || 7 | pig || 8 | monkey || 9 | tiger || 10 | cat || 50 | giraffe || 100 | rabbit || 101 | mouse || 102 | tortoise |+-----+----------+14 rows in set (0.00 sec)INSERT INTO animals (id,name) VALUES(NULL,&apos;donkey&apos;);SELECT * FROM animals;+-----+----------+| id | name |+-----+----------+| 0 | lion || 1 | dog || 3 | penguin || 4 | lax || 5 | whale || 6 | ostrich || 7 | pig || 8 | monkey || 9 | tiger || 10 | cat || 50 | giraffe || 100 | rabbit || 101 | mouse || 102 | tortoise || 103 | donkey |+-----+----------+15 rows in set (0.00 sec)#更新了auto_increment中列的值大于当前最大值update animals set id=110 where id=3;SELECT * FROM animals;+-----+----------+| id | name |+-----+----------+| 0 | lion || 1 | dog || 4 | lax || 5 | whale || 6 | ostrich || 7 | pig || 8 | monkey || 9 | tiger || 10 | cat || 50 | giraffe || 100 | rabbit || 101 | mouse || 102 | tortoise || 103 | donkey || 110 | penguin |+-----+----------+15 rows in set (0.00 sec)INSERT INTO animals (id,name) VALUES(NULL,&apos;cow&apos;); SELECT * FROM animals;+-----+----------+| id | name |+-----+----------+| 0 | lion || 1 | dog || 4 | lax || 5 | whale || 6 | ostrich || 7 | pig || 8 | monkey || 9 | tiger || 10 | cat || 50 | giraffe || 100 | rabbit || 101 | mouse || 102 | tortoise || 103 | donkey || 104 | cow || 110 | penguin |+-----+----------+16 rows in set (0.00 sec) auto_increment序列值没有更新，这里面有个大坑，随着序列不断增长，当增长到110是，就会报逐渐重复的错误了，因此自动增长列中的值不要更改。123456789101112131415161718192021222324252627282930313233343536373839404142root@db 02:33: [aaaa]&gt; INSERT INTO animals (id,name) VALUES(NULL,&apos;cow&apos;);Query OK, 1 row affected (0.00 sec)root@db 02:36: [aaaa]&gt; INSERT INTO animals (id,name) VALUES(NULL,&apos;cow&apos;);Query OK, 1 row affected (0.00 sec)root@db 02:36: [aaaa]&gt; INSERT INTO animals (id,name) VALUES(NULL,&apos;cow&apos;);Query OK, 1 row affected (0.01 sec)root@db 02:36: [aaaa]&gt; INSERT INTO animals (id,name) VALUES(NULL,&apos;cow&apos;);Query OK, 1 row affected (0.01 sec)root@db 02:36: [aaaa]&gt; INSERT INTO animals (id,name) VALUES(NULL,&apos;cow&apos;);Query OK, 1 row affected (0.00 sec)root@db 02:36: [aaaa]&gt; INSERT INTO animals (id,name) VALUES(NULL,&apos;cow&apos;);ERROR 1062 (23000): Duplicate entry &apos;110&apos; for key &apos;PRIMARY&apos;root@db 02:36: [aaaa]&gt; INSERT INTO animals (id,name) VALUES(NULL,&apos;cow&apos;);Query OK, 1 row affected (0.00 sec)SELECT * FROM animals;+-----+----------+| id | name |+-----+----------+| 0 | lion || 1 | dog || 4 | lax || 5 | whale || 6 | ostrich || 7 | pig || 8 | monkey || 9 | tiger || 10 | cat || 50 | giraffe || 100 | rabbit || 101 | mouse || 102 | tortoise || 103 | donkey || 104 | cow || 105 | cow || 106 | cow || 107 | cow || 108 | cow || 109 | cow || 110 | penguin || 111 | cow |+-----+----------+22 rows in set (0.00 sec) 下面以myisam类型表为例：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#创建测试表CREATE TABLE animals_myisam ( id MEDIUMINT NOT NULL AUTO_INCREMENT, name CHAR(30) NOT NULL, PRIMARY KEY (id)) ENGINE=MyISAM;#插入测试数据INSERT INTO animals_myisam (name) VALUES (&apos;dog&apos;),(&apos;cat&apos;), (&apos;penguin&apos;),(&apos;lax&apos;),(&apos;whale&apos;), (&apos;ostrich&apos;);SELECT * FROM animals_myisam;+----+---------+| id | name |+----+---------+| 1 | dog || 2 | cat || 3 | penguin || 4 | lax || 5 | whale || 6 | ostrich |+----+---------+6 rows in set (0.00 sec)#更新了auto_increment中列的值小于当前最大值update animals_myisam set id=10 where id=3;INSERT INTO animals_myisam (name) VALUES (&apos;mouse&apos;);SELECT * FROM animals_myisam;+----+---------+| id | name |+----+---------+| 1 | dog || 2 | cat || 10 | penguin || 4 | lax || 5 | whale || 6 | ostrich || 11 | mouse |+----+---------+7 rows in set (0.00 sec)update animals_myisam set id=8 where id=4;INSERT INTO animals_myisam (name) VALUES (&apos;donkey&apos;);root@db 03:51: [aaaa]&gt; SELECT * FROM animals_myisam;+----+---------+| id | name |+----+---------+| 1 | dog || 2 | cat || 10 | penguin || 8 | lax || 5 | whale || 6 | ostrich || 11 | mouse || 12 | donkey |+----+---------+8 rows in set (0.00 sec)#更新了auto_increment中列的值大于当前最大值update animals_myisam set id=51 where id=4;INSERT INTO animals_myisam (name) VALUES (&apos;horse&apos;);root@db 02:41: [aaaa]&gt; SELECT * FROM animals_myisam;+----+---------+| id | name |+----+---------+| 1 | dog || 2 | cat || 10 | penguin || 51 | lax || 5 | whale || 6 | ostrich || 11 | mouse || 52 | horse |+----+---------+8 rows in set (0.00 sec) 对于myisam类型的表，当对auto_increment列更新时，无论更新后的值大于或者小于当前自增列的值，都按当前列的最大值处理。 5、可以通过mysql函数last_insert_id查看最后插入的值：1234567select last_insert_id();+------------------+| last_insert_id() |+------------------+| 53 |+------------------+1 row in set (0.00 sec) 6、当auto_increment列值达到最大值后，再也无法生成后面的值，当再次插入的时候将会报错，提示主键重复。比如MEDIUMINT类型的最大值为8388607123456789101112131415161718192021222324252627282930313233343536INSERT INTO animals (id,name) VALUES(8388606,&apos;whale&apos;);root@db 02:54: [aaaa]&gt; INSERT INTO animals (id,name) VALUES(null,&apos;elephant&apos;);Query OK, 1 row affected (0.00 sec)root@db 02:55: [aaaa]&gt; INSERT INTO animals (id,name) VALUES(null,&apos;rhinoceros&apos;);ERROR 1062 (23000): Duplicate entry &apos;8388607&apos; for key &apos;PRIMARY&apos;root@db 02:55: [aaaa]&gt; INSERT INTO animals (id,name) VALUES(null,&apos;rhinoceros&apos;);ERROR 1062 (23000): Duplicate entry &apos;8388607&apos; for key &apos;PRIMARY&apos;root@db 02:55: [aaaa]&gt; SELECT * FROM animals;+---------+----------+| id | name |+---------+----------+| 0 | lion || 1 | dog || 4 | lax || 5 | whale || 6 | ostrich || 7 | pig || 8 | monkey || 9 | tiger || 10 | cat || 50 | giraffe || 100 | rabbit || 101 | mouse || 102 | tortoise || 103 | donkey || 104 | cow || 105 | cow || 106 | cow || 107 | cow || 108 | cow || 109 | cow || 110 | penguin || 111 | cow || 8388606 | whale || 8388607 | elephant |+---------+----------+ 当插入的值达到8388607值时，没有办法再插入数据了，这时需要修改数据类型123456789101112131415161718192021222324252627282930313233343536alter table animals modify id int(10) NOT NULL AUTO_INCREMENT;INSERT INTO animals (id,name) VALUES(null,&apos;rhinoceros&apos;);SELECT * FROM animals;root@db 02:57: [aaaa]&gt; INSERT INTO animals (id,name) VALUES(null,&apos;rhinoceros&apos;);Query OK, 1 row affected (0.00 sec)root@db 02:58: [aaaa]&gt; SELECT * FROM animals;+---------+------------+| id | name |+---------+------------+| 0 | lion || 1 | dog || 4 | lax || 5 | whale || 6 | ostrich || 7 | pig || 8 | monkey || 9 | tiger || 10 | cat || 50 | giraffe || 100 | rabbit || 101 | mouse || 102 | tortoise || 103 | donkey || 104 | cow || 105 | cow || 106 | cow || 107 | cow || 108 | cow || 109 | cow || 110 | penguin || 111 | cow || 8388606 | whale || 8388607 | elephant || 8388608 | rhinoceros |+---------+------------+ 修改完成之后，再次插入成功,下面是关于数据类型的最小值和最大值的对比123456Type Storage(Bytes) MinimumValueSigned MinimumValueUnsigned MaximumValueSigned MaximumValueUnsignedTINYINT 1 -128 0 127 255SMALLINT 2 -32768 0 32767 65535MEDIUMINT 3 -8388608 0 8388607 16777215INT 4 -2147483648 0 2147483647 4294967295BIGINT 8 -2^63 0 2^63-1 2^64-1 7、想要修改auto_increment的起始值，可以找alter table，比如：1ALTER TABLE animals AUTO_INCREMENT = 100; 先使用truncate清除表数据,truncate会初始化起始值,delete语句不会初始化,先执行delete操作看一下12345678910111213root@db 02:58: [aaaa]&gt; delete from animals;Query OK, 25 rows affected (0.01 sec)root@db 03:04: [aaaa]&gt; SELECT * FROM animals;Empty set (0.00 sec)root@db 03:04: [aaaa]&gt; insert into animals (name) values(&apos;dog&apos;);Query OK, 1 row affected (0.00 sec)root@db 03:04: [aaaa]&gt; SELECT * FROM animals;+---------+------+| id | name |+---------+------+| 8388609 | dog |+---------+------+1 row in set (0.00 sec) auto_increment的值继续增长，没有从1开始。执行truncate语句12345678910111213root@db 03:04: [aaaa]&gt; truncate table animals;Query OK, 0 rows affected (0.01 sec)root@db 03:05: [aaaa]&gt; SELECT * FROM animals;Empty set (0.00 sec)root@db 03:05: [aaaa]&gt; insert into animals (name) values(&apos;dog&apos;);Query OK, 1 row affected (0.01 sec)root@db 03:05: [aaaa]&gt; SELECT * FROM animals;+----+------+| id | name |+----+------+| 1 | dog |+----+------+1 row in set (0.00 sec) auto_increment序列值从1重新开始计算。下面使用alter table修改auto_increment的起始值：1234567891011121314151617root@db 03:05: [aaaa]&gt; truncate table animals;Query OK, 0 rows affected (0.05 sec)root@db 03:06: [aaaa]&gt; alter table animals auto_increment=99;Query OK, 0 rows affected (0.01 sec)Records: 0 Duplicates: 0 Warnings: 0root@db 03:07: [aaaa]&gt; insert into animals (name) values(&apos;dog&apos;);Query OK, 1 row affected (0.01 sec)root@db 03:07: [aaaa]&gt; insert into animals (name) values(&apos;cat&apos;);Query OK, 1 row affected (0.01 sec)root@db 03:08: [aaaa]&gt; SELECT * FROM animals;+-----+------+| id | name |+-----+------+| 99 | dog || 100 | cat |+-----+------+2 rows in set (0.00 sec) auto_increment起始值从99开始计算。另外使用alter table设置的auto_increment列值，要大于auto_increment列中的最大值才会有效，如果设置的值小于auto_increment列中的最大值，该值会被重置为当前列的最大值+1。如果设置系统级的步长(auto_increment_increment)和初始值(auto_increment_offset)但是auto_increment_offset一定不能大于auto_increment_increment的值，否则auto_increment_offset会被忽略：1234567891011121314151617181920212223242526272829root@db 04:18: [aaaa]&gt; truncate table animals;Query OK, 0 rows affected (0.02 sec)root@db 04:18: [aaaa]&gt; set @@auto_increment_offset=5;Query OK, 0 rows affected (0.00 sec)root@db 04:18: [aaaa]&gt; set @@auto_increment_increment=7;Query OK, 0 rows affected (0.00 sec)root@db 04:19: [aaaa]&gt; root@db 04:20: [aaaa]&gt; root@db 04:20: [aaaa]&gt; INSERT INTO animals (name) VALUES -&gt; (&apos;dog&apos;),(&apos;cat&apos;),(&apos;penguin&apos;), -&gt; (&apos;lax&apos;),(&apos;whale&apos;),(&apos;ostrich&apos;);Query OK, 6 rows affected (0.01 sec)Records: 6 Duplicates: 0 Warnings: 0root@db 04:20: [aaaa]&gt; select * from animals;+----+---------+| id | name |+----+---------+| 5 | dog || 12 | cat || 19 | penguin || 26 | lax || 33 | whale || 40 | ostrich |+----+---------+6 rows in set (0.00 sec) 8、对于myisam中的自增列,如果自增列在复合索引中,自增列会先匹配前缀,根据前缀条件搜索到的结果在自增,计算方法：s MAX(auto_increment_column) + 1 WHERE prefix=given-prefix。例如：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364drop table animals_myisam;CREATE TABLE animals_myisam ( grp ENUM(&apos;fish&apos;,&apos;mammal&apos;,&apos;bird&apos;) NOT NULL, id MEDIUMINT NOT NULL AUTO_INCREMENT, name CHAR(30) NOT NULL, PRIMARY KEY (grp,id)) ENGINE=MyISAM;INSERT INTO animals_myisam (grp,name) VALUES (&apos;mammal&apos;,&apos;dog&apos;),(&apos;mammal&apos;,&apos;cat&apos;), (&apos;bird&apos;,&apos;penguin&apos;),(&apos;fish&apos;,&apos;lax&apos;),(&apos;mammal&apos;,&apos;whale&apos;), (&apos;bird&apos;,&apos;ostrich&apos;);root@db 03:12: [aaaa]&gt; SELECT * FROM animals_myisam ORDER BY grp,id;+--------+----+---------+| grp | id | name |+--------+----+---------+| fish | 1 | lax || mammal | 1 | dog || mammal | 2 | cat || mammal | 3 | whale || bird | 1 | penguin || bird | 2 | ostrich |+--------+----+---------+6 rows in set (0.00 sec)root@db 03:12: [aaaa]&gt; INSERT INTO animals_myisam (grp,name) VALUES -&gt; (&apos;mammal&apos;,&apos;dog&apos;),(&apos;mammal&apos;,&apos;cat&apos;), -&gt; (&apos;bird&apos;,&apos;penguin&apos;),(&apos;fish&apos;,&apos;lax&apos;),(&apos;mammal&apos;,&apos;whale&apos;), -&gt; (&apos;bird&apos;,&apos;ostrich&apos;);Query OK, 6 rows affected (0.00 sec)Records: 6 Duplicates: 0 Warnings: 0root@db 03:13: [aaaa]&gt; INSERT INTO animals_myisam (grp,name) VALUES -&gt; (&apos;mammal&apos;,&apos;dog&apos;),(&apos;mammal&apos;,&apos;cat&apos;), -&gt; (&apos;bird&apos;,&apos;penguin&apos;),(&apos;fish&apos;,&apos;lax&apos;),(&apos;mammal&apos;,&apos;whale&apos;), -&gt; (&apos;bird&apos;,&apos;ostrich&apos;);Query OK, 6 rows affected (0.00 sec)Records: 6 Duplicates: 0 Warnings: 0root@db 03:13: [aaaa]&gt; SELECT * FROM animals_myisam ORDER BY grp,id;+--------+----+---------+| grp | id | name |+--------+----+---------+| fish | 1 | lax || fish | 2 | lax || fish | 3 | lax || mammal | 1 | dog || mammal | 2 | cat || mammal | 3 | whale || mammal | 4 | dog || mammal | 5 | cat || mammal | 6 | whale || mammal | 7 | dog || mammal | 8 | cat || mammal | 9 | whale || bird | 1 | penguin || bird | 2 | ostrich || bird | 3 | penguin || bird | 4 | ostrich || bird | 5 | penguin || bird | 6 | ostrich |+--------+----+---------+18 rows in set (0.00 sec) 在这种情况下，删除带有包括最大值的增长列，auto_increment序列值会被复用。12345678910111213141516171819202122232425262728293031323334353637383940delete from animals_myisam where grp=&apos;mammal&apos; and id &gt; 2;root@db 03:23: [aaaa]&gt; SELECT * FROM animals_myisam ORDER BY grp,id;+--------+----+---------+| grp | id | name |+--------+----+---------+| fish | 1 | lax || fish | 2 | lax || fish | 3 | lax || mammal | 1 | dog || mammal | 2 | cat || bird | 1 | penguin || bird | 2 | ostrich || bird | 3 | penguin || bird | 4 | ostrich || bird | 5 | penguin || bird | 6 | ostrich |+--------+----+---------+11 rows in set (0.00 sec)#再次插入数据INSERT INTO animals_myisam (grp,name) VALUES (&apos;mammal&apos;,&apos;dog&apos;),(&apos;mammal&apos;,&apos;cat&apos;);root@db 03:23: [aaaa]&gt; SELECT * FROM animals_myisam ORDER BY grp,id;+--------+----+---------+| grp | id | name |+--------+----+---------+| fish | 1 | lax || fish | 2 | lax || fish | 3 | lax || mammal | 1 | dog || mammal | 2 | cat || mammal | 3 | dog || mammal | 4 | cat || bird | 1 | penguin || bird | 2 | ostrich || bird | 3 | penguin || bird | 4 | ostrich || bird | 5 | penguin || bird | 6 | ostrich |+--------+----+---------+13 rows in set (0.00 sec) 以上情况是在id没有单独索引的情况下发生的，如果id单独的索引，那么插入的值会变成单一的值，不会跟grp列有任何关系。123456789101112131415161718192021222324252627282930create index id_ind on animals_myisam(id);INSERT INTO animals_myisam (grp,name) VALUES (&apos;mammal&apos;,&apos;dog&apos;),(&apos;mammal&apos;,&apos;cat&apos;), (&apos;bird&apos;,&apos;penguin&apos;),(&apos;fish&apos;,&apos;lax&apos;),(&apos;mammal&apos;,&apos;whale&apos;), (&apos;bird&apos;,&apos;ostrich&apos;);select * from animals_myisam order by mammal;root@db 03:29: [aaaa]&gt; select * from animals_myisam order by grp;+--------+----+---------+| grp | id | name |+--------+----+---------+| fish | 10 | lax || fish | 1 | lax || fish | 2 | lax || fish | 3 | lax || mammal | 2 | cat || mammal | 11 | whale || mammal | 8 | cat || mammal | 7 | dog || mammal | 3 | dog || mammal | 4 | cat || mammal | 1 | dog || bird | 5 | penguin || bird | 6 | ostrich || bird | 4 | ostrich || bird | 3 | penguin || bird | 2 | ostrich || bird | 9 | penguin || bird | 1 | penguin || bird | 12 | ostrich |+--------+----+---------+]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] optimize table运行是否支持online ddl]]></title>
    <url>%2F2018%2F11%2F27%2Foptimize%20table%E8%BF%90%E8%A1%8C%E6%98%AF%E5%90%A6%E6%94%AF%E6%8C%81online%20ddl%2F</url>
    <content type="text"><![CDATA[0、相应官方文档1234（online ddl）官方文档提示当存在fulltext索引表进行优化表操作(optimize table)时是不支持online ddl的https://dev.mysql.com/doc/refman/5.7/en/innodb-online-ddl-operations.html#online-ddl-table-operations（optimize table）官方文档提示当存在索引时，对表进行优化不支持在线ddlhttps://dev.mysql.com/doc/refman/5.7/en/optimize-table.html 下面分别对有主键索引、fulltext索引、无索引情况先查看是否支持online ddl; 1、带有主键索引查看表结构123456789101112root@db 15:44: [aaaa]&gt; show create table optimize_tb;+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| optimize_tb | CREATE TABLE `optimize_tb` ( `id` int(7) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec) 插入测试数据1234567891011root@db 15:44: [aaaa]&gt; insert into optimize_tb (select * from testfororder limit 1000000);Query OK, 1000000 rows affected (42.98 sec)Records: 1000000 Duplicates: 0 Warnings: 0root@db 15:46: [aaaa]&gt; select count(*) from optimize_tb;+----------+| count(*) |+----------+| 1000000 |+----------+1 row in set (0.28 sec) 查看文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize-rw-r-----. 1 mysql mysql 8.5K 11月 27 15:44 optimize_tb.frm-rw-r-----. 1 mysql mysql 48M 11月 27 15:46 optimize_tb.ibd 删除前50万数据12345678910root@db 15:47: [aaaa]&gt; delete from optimize_tb limit 500000;Query OK, 500000 rows affected (8.72 sec)root@db 15:47: [aaaa]&gt; select count(*) from optimize_tb;+----------+| count(*) |+----------+| 500000 |+----------+1 row in set (0.11 sec) 查看数据文件没有任何变化123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize-rw-r-----. 1 mysql mysql 8.5K 11月 27 15:44 optimize_tb.frm-rw-r-----. 1 mysql mysql 48M 11月 27 15:47 optimize_tb.ibd session1执行优化表操作12345678root@db 15:48: [aaaa]&gt; optimize table optimize_tb;+------------------+----------+----------+-------------------------------------------------------------------+| Table | Op | Msg_type | Msg_text |+------------------+----------+----------+-------------------------------------------------------------------+| aaaa.optimize_tb | optimize | note | Table does not support optimize, doing recreate + analyze instead || aaaa.optimize_tb | optimize | status | OK |+------------------+----------+----------+-------------------------------------------------------------------+2 rows in set (16.34 sec) session2执行插入语句操作12345678root@db 15:47: [aaaa]&gt; insert into optimize_tb(id,name,age) values(8166638,&apos;lucy1&apos;,24);Query OK, 1 row affected (0.18 sec)root@db 15:48: [aaaa]&gt; insert into optimize_tb(id,name,age) values(8166638,&apos;lucy1&apos;,24);ERROR 1062 (23000): Duplicate entry &apos;8166638&apos; for key &apos;PRIMARY&apos;root@db 15:48: [aaaa]&gt; insert into optimize_tb(id,name,age) values(8166639,&apos;lucy1&apos;,24);Query OK, 1 row affected (0.25 sec)root@db 15:48: [aaaa]&gt; insert into optimize_tb(id,name,age) values(8166640,&apos;lucy1&apos;,24);Query OK, 1 row affected (0.25 sec) 最后查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize-rw-r-----. 1 mysql mysql 8.5K 11月 27 15:48 optimize_tb.frm-rw-r-----. 1 mysql mysql 28M 11月 27 15:48 optimize_tb.ibd 数据插入成功，在有主键的情况下，使用optimize table可支持online ddl。 2、带有fulltext索引创建测试表123456789101112131415root@db 15:53: [aaaa]&gt; create table optimize_tb1 like optimize_tb;Query OK, 0 rows affected (0.65 sec)root@db 15:53: [aaaa]&gt; show create table optimize_tb1;+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| optimize_tb1 | CREATE TABLE `optimize_tb1` ( `id` int(7) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.04 sec) 删除主键索引12root@db 15:53: [aaaa]&gt; alter table optimize_tb1 drop primary key;ERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key 因为存在自增约束，报错，需要删除约束12345678910111213141516171819root@db 15:54: [aaaa]&gt; alter table optimize_tb1 modify id int(7);Query OK, 0 rows affected (2.84 sec)Records: 0 Duplicates: 0 Warnings: 0root@db 15:54: [aaaa]&gt; alter table optimize_tb1 drop primary key;Query OK, 0 rows affected (1.45 sec)Records: 0 Duplicates: 0 Warnings: 0root@db 15:54: [aaaa]&gt; show create table optimize_tb1;+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+| optimize_tb1 | CREATE TABLE `optimize_tb1` ( `id` int(7) NOT NULL, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 插入数据1234567891011root@db 15:55: [aaaa]&gt; insert into optimize_tb1 (select * from testfororder limit 1000000);Query OK, 1000000 rows affected (26.04 sec)Records: 1000000 Duplicates: 0 Warnings: 0root@db 15:56: [aaaa]&gt; select count(*) from optimize_tb1;+----------+| count(*) |+----------+| 1000000 |+----------+1 row in set (0.48 sec) 创建fulltext 索引123root@db 15:56: [aaaa]&gt; create fulltext index name_fullind on optimize_tb1(name);Query OK, 0 rows affected, 1 warning (1 min 12.58 sec)Records: 0 Duplicates: 0 Warnings: 1 查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize_tb1-rw-r-----. 1 mysql mysql 8.5K 11月 27 15:56 optimize_tb1.frm-rw-r-----. 1 mysql mysql 92M 11月 27 15:58 optimize_tb1.ibd 删除前50万数据12345678910root@db 15:58: [aaaa]&gt; delete from optimize_tb1 limit 500000;Query OK, 500000 rows affected (1 min 13.37 sec)root@db 15:59: [aaaa]&gt; select count(*) from optimize_tb1;+----------+| count(*) |+----------+| 500000 |+----------+1 row in set (0.50 sec) 查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize_tb1-rw-r-----. 1 mysql mysql 8.5K 11月 27 15:56 optimize_tb1.frm-rw-r-----. 1 mysql mysql 92M 11月 27 15:59 optimize_tb1.ibd session1对表进行优化12345678root@db 15:59: [aaaa]&gt; optimize table optimize_tb1;+-------------------+----------+----------+-------------------------------------------------------------------+| Table | Op | Msg_type | Msg_text |+-------------------+----------+----------+-------------------------------------------------------------------+| aaaa.optimize_tb1 | optimize | note | Table does not support optimize, doing recreate + analyze instead || aaaa.optimize_tb1 | optimize | status | OK |+-------------------+----------+----------+-------------------------------------------------------------------+2 rows in set (1 min 13.65 sec) session2对表插入数据12root@db 16:00: [aaaa]&gt; insert into optimize_tb1(id,name,age) values(8166638,&apos;lucy1&apos;,24);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction session3查看数据库线程123456789root@db 16:00: [(none)]&gt; show processlist;+----+------+-----------+------+---------+------+---------------------------------+------------------------------------------------------------------+| Id | User | Host | db | Command | Time | State | Info |+----+------+-----------+------+---------+------+---------------------------------+------------------------------------------------------------------+| 5 | root | localhost | aaaa | Query | 41 | copy to tmp table | optimize table optimize_tb1 || 7 | root | localhost | aaaa | Query | 40 | Waiting for table metadata lock | insert into optimize_tb1(id,name,age) values(8166638,&apos;lucy1&apos;,24) || 8 | root | localhost | NULL | Query | 0 | starting | show processlist |+----+------+-----------+------+---------+------+---------------------------------+------------------------------------------------------------------+3 rows in set (0.00 sec) 查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize_tb1-rw-r-----. 1 mysql mysql 8.5K 11月 27 16:00 optimize_tb1.frm-rw-r-----. 1 mysql mysql 48M 11月 27 16:02 optimize_tb1.ibd 当表存在fulltext索引是，执行optimize table会锁表，其他会话连接进来无法做增删改操作。 3、表不存在任何索引创建测试表123456789101112131415root@db 16:04: [aaaa]&gt; create table optimize_tb2 like optimize_tb;Query OK, 0 rows affected (0.60 sec)root@db 16:04: [aaaa]&gt; show create table optimize_tb2;+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| optimize_tb2 | CREATE TABLE `optimize_tb2` ( `id` int(7) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec) 删除主键索引12345678910111213141516171819root@db 16:04: [aaaa]&gt; alter table optimize_tb2 modify id int(7);Query OK, 0 rows affected (1.60 sec)Records: 0 Duplicates: 0 Warnings: 0root@db 16:04: [aaaa]&gt; alter table optimize_tb2 drop primary key;Query OK, 0 rows affected (3.07 sec)Records: 0 Duplicates: 0 Warnings: 0root@db 16:04: [aaaa]&gt; show create table optimize_tb2;+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+| optimize_tb2 | CREATE TABLE `optimize_tb2` ( `id` int(7) NOT NULL, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 插入数据123456789101112root@db 16:04: [aaaa]&gt; insert into optimize_tb2 (select * from testfororder limit 1000000);Query OK, 1000000 rows affected (11.85 sec)Records: 1000000 Duplicates: 0 Warnings: 0root@db 16:06: [aaaa]&gt; root@db 16:07: [aaaa]&gt; select count(*) from optimize_tb2;+----------+| count(*) |+----------+| 1000000 |+----------+1 row in set (0.41 sec) 查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize_tb2-rw-r-----. 1 mysql mysql 8.5K 11月 27 16:04 optimize_tb2.frm-rw-r-----. 1 mysql mysql 56M 11月 27 16:06 optimize_tb2.ibd 删除前50万数据12345678910root@db 16:07: [aaaa]&gt; delete from optimize_tb2 limit 500000;Query OK, 500000 rows affected (2.50 sec)root@db 16:11: [aaaa]&gt; select count(*) from optimize_tb2;+----------+| count(*) |+----------+| 500000 |+----------+1 row in set (0.29 sec) 查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize_tb2-rw-r-----. 1 mysql mysql 8.5K 11月 27 16:04 optimize_tb2.frm-rw-r-----. 1 mysql mysql 56M 11月 27 16:11 optimize_tb2.ibd session1对表进行优化12345678root@db 16:11: [aaaa]&gt; optimize table optimize_tb2;+-------------------+----------+----------+-------------------------------------------------------------------+| Table | Op | Msg_type | Msg_text |+-------------------+----------+----------+-------------------------------------------------------------------+| aaaa.optimize_tb2 | optimize | note | Table does not support optimize, doing recreate + analyze instead || aaaa.optimize_tb2 | optimize | status | OK |+-------------------+----------+----------+-------------------------------------------------------------------+2 rows in set (22.04 sec session2对表插入数据12345678910root@db 16:11: [aaaa]&gt; insert into optimize_tb2(id,name,age) values(8166638,&apos;lucy1&apos;,24);Query OK, 1 row affected (0.35 sec)root@db 16:11: [aaaa]&gt; insert into optimize_tb2(id,name,age) values(8166638,&apos;lucy1&apos;,24);Query OK, 1 row affected (0.44 sec)root@db 16:11: [aaaa]&gt; insert into optimize_tb2(id,name,age) values(8166638,&apos;lucy1&apos;,24);Query OK, 1 row affected (0.24 sec)root@db 16:11: [aaaa]&gt; insert into optimize_tb2(id,name,age) values(8166638,&apos;lucy1&apos;,24);Query OK, 1 row affected (0.38 sec)root@db 16:11: [aaaa]&gt; insert into optimize_tb2(id,name,age) values(8166638,&apos;lucy1&apos;,24);Query OK, 1 row affected (0.20 sec) 查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize_tb2-rw-r-----. 1 mysql mysql 8.5K 11月 27 16:11 optimize_tb2.frm-rw-r-----. 1 mysql mysql 32M 11月 27 16:12 optimize_tb2.ibd 在没有任何索引的情况下，insert语句成功。 4、当mysql环境变量 old_alter_table为enabled（默认为off）或者 mysql启动参数添加了skip-new选项时，运行optimize table命令，会使用table copy的方式重建表，此时online ddl是不可用的，下面添加skip-new参数重新进行测试。在/etc/my.cnf添加参数skip-new，重新启动mysql：12345[root@dax-mysql-master aaaa]# more /etc/my.cnf|grep skip-newskip-new[root@dax-mysql-master aaaa]# /etc/init.d/mysql restartShutting down MySQL....... SUCCESS! Starting MySQL..... SUCCESS! 删除上面的测试表，再次进行测试12root@db 16:21: [aaaa]&gt; drop table optimize_tb,optimize_tb1,optimize_tb2;Query OK, 0 rows affected (1.58 sec) 5、带有主键索引创建测试表12root@db 16:23: [aaaa]&gt; create table optimize_tb like testfororder;Query OK, 0 rows affected (1.29 sec) 插入测试数据123456789101112131415161718192021222324root@db 16:23: [aaaa]&gt; insert into optimize_tb (select * from testfororder limit 1000000);Query OK, 1000000 rows affected (33.65 sec)Records: 1000000 Duplicates: 0 Warnings: 0root@db 16:24: [aaaa]&gt; show create table optimize_tb;+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| optimize_tb | CREATE TABLE `optimize_tb` ( `id` int(7) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=8166638 DEFAULT CHARSET=utf8mb4 |+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)root@db 16:25: [aaaa]&gt; select count(*) from optimize_tb;+----------+| count(*) |+----------+| 1000000 |+----------+1 row in set (0.25 sec) 查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize-rw-r-----. 1 mysql mysql 8.5K 11月 27 16:23 optimize_tb.frm-rw-r-----. 1 mysql mysql 48M 11月 27 16:24 optimize_tb.ibd 删除前50万数据12345678910root@db 16:26: [aaaa]&gt; delete from optimize_tb limit 500000;Query OK, 500000 rows affected (9.18 sec)root@db 16:26: [aaaa]&gt; select count(*) from optimize_tb;+----------+| count(*) |+----------+| 500000 |+----------+1 row in set (0.13 sec) 查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize-rw-r-----. 1 mysql mysql 8.5K 11月 27 16:23 optimize_tb.frm-rw-r-----. 1 mysql mysql 48M 11月 27 16:26 optimize_tb.ibd session1执行优化表操作123root@db 16:27: [aaaa]&gt; optimize table optimize_tb;Query OK, 500000 rows affected (26.01 sec)Records: 500000 Duplicates: 0 Warnings: 0 session2执行插入操作12root@db 16:27: [aaaa]&gt; insert into optimize_tb(id,name,age) values(8166638,&apos;lucy1&apos;,24);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction session3查看mysql线程12345678root@db 16:27: [(none)]&gt; show processlist;+----+------+-----------+------+---------+------+---------------------------------+-----------------------------------------------------------------+| Id | User | Host | db | Command | Time | State | Info |+----+------+-----------+------+---------+------+---------------------------------+-----------------------------------------------------------------+| 2 | root | localhost | aaaa | Query | 19 | copy to tmp table | optimize table optimize_tb || 3 | root | localhost | aaaa | Query | 11 | Waiting for table metadata lock | insert into optimize_tb(id,name,age) values(8166638,&apos;lucy1&apos;,24) || 4 | root | localhost | NULL | Query | 0 | starting | show processlist |+----+------+-----------+------+---------+------+---------------------------------+-----------------------------------------------------------------+ 查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize-rw-r-----. 1 mysql mysql 8.5K 11月 27 16:27 optimize_tb.frm-rw-r-----. 1 mysql mysql 28M 11月 27 16:27 optimize_tb.ibd 在存在主键索引的情况下，数据插入失败，提示锁等待超时。 6、表中带有fulltext索引创建测试表123456789101112131415root@db 16:31: [aaaa]&gt; create table optimize_tb1 like optimize_tb;Query OK, 0 rows affected (1.07 sec)root@db 16:31: [aaaa]&gt; show create table optimize_tb1;+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| optimize_tb1 | CREATE TABLE `optimize_tb1` ( `id` int(7) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec) 删除主键索引12345678910111213141516171819root@db 16:31: [aaaa]&gt; alter table optimize_tb1 modify id int(7);Query OK, 0 rows affected (1.13 sec)Records: 0 Duplicates: 0 Warnings: 0root@db 16:31: [aaaa]&gt; alter table optimize_tb1 drop primary key;Query OK, 0 rows affected (1.41 sec)Records: 0 Duplicates: 0 Warnings: 0root@db 16:31: [aaaa]&gt; show create table optimize_tb1;+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+| optimize_tb1 | CREATE TABLE `optimize_tb1` ( `id` int(7) NOT NULL, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 插入数据1234567891011root@db 16:32: [aaaa]&gt; insert into optimize_tb1 (select * from testfororder limit 1000000);Query OK, 1000000 rows affected (14.49 sec)Records: 1000000 Duplicates: 0 Warnings: 0root@db 16:32: [aaaa]&gt; select count(*) from optimize_tb1;+----------+| count(*) |+----------+| 1000000 |+----------+1 row in set (0.44 sec) 创建fulltext 索引123root@db 16:33: [aaaa]&gt; create fulltext index name_fullind on optimize_tb1(name);Query OK, 0 rows affected, 1 warning (57.22 sec)Records: 0 Duplicates: 0 Warnings: 1 查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize_tb1-rw-r-----. 1 mysql mysql 8.5K 11月 27 16:33 optimize_tb1.frm-rw-r-----. 1 mysql mysql 92M 11月 27 16:34 optimize_tb1.ibd 删除前50万数据12345678910root@db 16:34: [aaaa]&gt; delete from optimize_tb1 limit 500000;Query OK, 500000 rows affected (59.90 sec)root@db 16:35: [aaaa]&gt; select count(*) from optimize_tb1;+----------+| count(*) |+----------+| 500000 |+----------+1 row in set (0.69 sec) 查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize_tb1-rw-r-----. 1 mysql mysql 8.5K 11月 27 16:33 optimize_tb1.frm-rw-r-----. 1 mysql mysql 92M 11月 27 16:36 optimize_tb1.ibd session1执行对表进行优化操作123root@db 16:36: [aaaa]&gt; optimize table optimize_tb1;Query OK, 500000 rows affected (55.85 sec)Records: 500000 Duplicates: 0 Warnings: 0 session2对表插入数据1234root@db 16:37: [aaaa]&gt; insert into optimize_tb1(id,name,age) values(8166638,&apos;lucy1&apos;,24);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionroot@db 16:37: [aaaa]&gt; insert into optimize_tb1(id,name,age) values(8166638,&apos;lucy1&apos;,24);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction session3查看mysql线程123456789root@db 16:37: [(none)]&gt; show processlist;+----+------+-----------+------+---------+------+---------------------------------+------------------------------------------------------------------+| Id | User | Host | db | Command | Time | State | Info |+----+------+-----------+------+---------+------+---------------------------------+------------------------------------------------------------------+| 2 | root | localhost | aaaa | Query | 9 | copy to tmp table | optimize table optimize_tb1 || 4 | root | localhost | NULL | Query | 0 | starting | show processlist || 5 | root | localhost | aaaa | Query | 8 | Waiting for table metadata lock | insert into optimize_tb1(id,name,age) values(8166638,&apos;lucy1&apos;,24) |+----+------+-----------+------+---------+------+---------------------------------+------------------------------------------------------------------+3 rows in set (0.00 sec) 查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize_tb1-rw-r-----. 1 mysql mysql 8.5K 11月 27 16:37 optimize_tb1.frm-rw-r-----. 1 mysql mysql 48M 11月 27 16:38 optimize_tb1.ibd 在存在fulltext索引的情况下，数据插入失败，提示锁等待超时。 7、表中没有任何索引创建测试表1234567891011121314151617181920212223242526272829303132333435root@db 16:38: [aaaa]&gt; create table optimize_tb2 like optimize_tb;Query OK, 0 rows affected (0.44 sec)root@db 16:39: [aaaa]&gt; show create table optimize_tb2;+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| optimize_tb2 | CREATE TABLE `optimize_tb2` ( `id` int(7) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)root@db 16:40: [aaaa]&gt; alter table optimize_tb2 modify id int(7);Query OK, 0 rows affected (1.39 sec)Records: 0 Duplicates: 0 Warnings: 0root@db 16:40: [aaaa]&gt; alter table optimize_tb2 drop primary key;Query OK, 0 rows affected (1.01 sec)Records: 0 Duplicates: 0 Warnings: 0root@db 16:40: [aaaa]&gt; show create table optimize_tb2;+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+| optimize_tb2 | CREATE TABLE `optimize_tb2` ( `id` int(7) NOT NULL, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec) 插入数据1234567891011root@db 16:40: [aaaa]&gt; insert into optimize_tb2 (select * from testfororder limit 1000000);Query OK, 1000000 rows affected (16.74 sec)Records: 1000000 Duplicates: 0 Warnings: 0root@db 16:41: [aaaa]&gt; select count(*) from optimize_tb2;+----------+| count(*) |+----------+| 1000000 |+----------+1 row in set (0.49 sec) 查看数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize_tb2-rw-r-----. 1 mysql mysql 8.5K 11月 27 16:40 optimize_tb2.frm-rw-r-----. 1 mysql mysql 56M 11月 27 16:41 optimize_tb2.ibd 删除前50万数据12345678910root@db 16:42: [aaaa]&gt; delete from optimize_tb2 limit 500000;Query OK, 500000 rows affected (3.96 sec)root@db 16:42: [aaaa]&gt; select count(*) from optimize_tb2;+----------+| count(*) |+----------+| 500000 |+----------+1 row in set (0.32 sec) 数据文件大小123[root@dax-mysql-master aaaa]# ls -lhtr|grep optimize_tb2-rw-r-----. 1 mysql mysql 8.5K 11月 27 16:40 optimize_tb2.frm-rw-r-----. 1 mysql mysql 56M 11月 27 16:42 optimize_tb2.ibd session1执行优化表操作123root@db 16:42: [aaaa]&gt; optimize table optimize_tb2;Query OK, 500000 rows affected (23.61 sec)Records: 500000 Duplicates: 0 Warnings: 0 session2执行插入数据操作12root@db 16:43: [aaaa]&gt; insert into optimize_tb2(id,name,age) values(8166638,&apos;lucy1&apos;,24);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction session3查看mysql线程123456789root@db 16:43: [(none)]&gt; show processlist;+----+------+-----------+------+---------+------+---------------------------------+------------------------------------------------------------------+| Id | User | Host | db | Command | Time | State | Info |+----+------+-----------+------+---------+------+---------------------------------+------------------------------------------------------------------+| 2 | root | localhost | aaaa | Query | 7 | copy to tmp table | optimize table optimize_tb2 || 4 | root | localhost | NULL | Query | 0 | starting | show processlist || 5 | root | localhost | aaaa | Query | 6 | Waiting for table metadata lock | insert into optimize_tb2(id,name,age) values(8166638,&apos;lucy1&apos;,24) |+----+------+-----------+------+---------+------+---------------------------------+------------------------------------------------------------------+3 rows in set (0.00 sec) 在没有任何索引的情况下，插入数据锁等待 8、结论当mysql环境变量 old_alter_table为enabled（默认为disabled）或者 mysql启动参数添加了skip-new选项时，运行optimize table命令，会使用table copy的方式重建表，无论表中有无索引，此时online ddl是不可用的；当mysql环境变量old_alter_table为disabled（默认为disabled）或者 mysql启动参数没有添加了skip-new选项时，运行optimize table命令，当表中不存在fulltext索引时，online ddl可用，当表中存在fulltext索引是，online ddl不可用。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql-mgr模拟the master has purged binary logs containing GTIDs that the slave requires.', Error_code: 1236]]></title>
    <url>%2F2018%2F11%2F25%2Fmysql-mgr%E6%A8%A1%E6%8B%9Fthe%20master%20has%20purged%20binary%20logs%20containing%20GTIDs%20that%20the%20slave%20requires%2F</url>
    <content type="text"><![CDATA[0、实验环境介绍：mysql-mgr单主模式主节点1、从节点1、从节点2 1、实验一，手动删除主库binlog日志文件从库21执行stop group_replication; 主库：12345678910111213141516171819202122232425262728293031#切换日志，创建表gtid_test10,并查看当前的binlog日志文件：flush logs;create table gtid_test10 (ID int) engine=innodb;Query OK, 0 rows affected (0.04 sec)Query OK, 0 rows affected (0.05 sec)root@db 05:48: [test]&gt; show master status;+---------------------+----------+--------------+------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------------+----------+--------------+------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| mysql-binlog.000029 | 493 | | | 8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-27 |+---------------------+----------+--------------+------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)#切换日志，创建表gtid_test10,并查看当前的binlog日志文件：root@db 05:48: [test]&gt; flush logs;create table gtid_test11 (ID int) engine=innodb;Query OK, 0 rows affected (0.05 sec)Query OK, 0 rows affected (0.05 sec)root@db 05:48: [test]&gt; show master status;+---------------------+----------+--------------+------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------------+----------+--------------+------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| mysql-binlog.000030 | 493 | | | 8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-28 |+---------------------+----------+--------------+------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+#主库手动移除binlog文件：mv mysql-binlog.000029 /tmp/ 从库212345678910111213141516171819202122232425262728执行start group_replication;#查看从库2的日志2018-09-18T05:49:55.983888Z 93 [Note] Plugin group_replication reported: &apos;Establishing group recovery connection with a possible donor. Attempt 1/10&apos;2018-09-18T05:49:55.984034Z 0 [Note] Plugin group_replication reported: &apos;Group membership changed to dax-mysql-slave:3306, dax-mysql-master:3306, dax-mysql-slave2:3306 on view 15371510730966421:31.&apos;2018-09-18T05:49:56.054981Z 93 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;dax-mysql-master&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-09-18T05:49:56.144944Z 93 [Note] Plugin group_replication reported: &apos;Establishing connection to a group replication recovery donor 8182e5ae-af54-11e8-af0e-000d3a801ae2 at dax-mysql-master port: 3306.&apos;2018-09-18T05:49:56.155058Z 95 [Warning] Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the &apos;START SLAVE Syntax&apos; in the MySQL Manual for more information.2018-09-18T05:49:56.157141Z 95 [Note] Slave I/O thread for channel &apos;group_replication_recovery&apos;: connected to master &apos;repl@dax-mysql-master:3306&apos;,replication started in log &apos;FIRST&apos; at position 42018-09-18T05:49:56.179441Z 96 [Note] Slave SQL thread for channel &apos;group_replication_recovery&apos; initialized, starting replication in log &apos;FIRST&apos; at position 0, relay log &apos;./relay-log-group_replication_recovery.000001&apos; position: 42018-09-18T05:49:56.326072Z 95 [ERROR] Error reading packet from server for channel &apos;group_replication_recovery&apos;: Could not open log file (server_errno=1236)2018-09-18T05:49:56.326236Z 95 [ERROR] Slave I/O for channel &apos;group_replication_recovery&apos;: Got fatal error 1236 from master when reading data from binary log: &apos;Could not open log file&apos;, Error_code: 1236#因为已经手动删除了binlog文件，从库2无法从主库获取到binlog文件，报错2018-09-18T05:49:56.326310Z 95 [Note] Slave I/O thread exiting for channel &apos;group_replication_recovery&apos;, read up to log &apos;mysql-binlog.000029&apos;, position 42018-09-18T05:49:56.326422Z 93 [Note] Plugin group_replication reported: &apos;Terminating existing group replication donor connection and purging the corresponding logs.&apos;2018-09-18T05:49:56.347776Z 96 [Note] Error reading relay log event for channel &apos;group_replication_recovery&apos;: slave SQL thread was killed2018-09-18T05:49:56.438760Z 93 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;dax-mysql-master&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-09-18T05:49:56.510091Z 93 [Note] Plugin group_replication reported: &apos;Retrying group recovery connection with another donor. Attempt 2/10&apos;2018-09-18T05:49:56.572561Z 93 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;dax-mysql-slave&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.#从库2开始尝试从从库1节点获取binlog文件，因为只是删除了主库节点的binlog文件，从库1的文件未删除，从库2获取到所需要的文件，开始进行同步。2018-09-18T05:49:56.643413Z 93 [Note] Plugin group_replication reported: &apos;Establishing connection to a group replication recovery donor 66d67181-ba5b-11e8-9c54-000d3a800ed3 at dax-mysql-slave port: 3306.&apos;2018-09-18T05:49:56.643906Z 99 [Warning] Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the &apos;START SLAVE Syntax&apos; in the MySQL Manual for more information.2018-09-18T05:49:56.646363Z 99 [Note] Slave I/O thread for channel &apos;group_replication_recovery&apos;: connected to master &apos;repl@dax-mysql-slave:3306&apos;,replication started in log &apos;FIRST&apos; at position 42018-09-18T05:49:56.666439Z 100 [Note] Slave SQL thread for channel &apos;group_replication_recovery&apos; initialized, starting replication in log &apos;FIRST&apos; at position 0, relay log &apos;./relay-log-group_replication_recovery.000001&apos; position: 42018-09-18T05:49:56.882987Z 93 [Note] Plugin group_replication reported: &apos;Terminating existing group replication donor connection and purging the corresponding logs.&apos;2018-09-18T05:49:56.905507Z 99 [Note] Slave I/O thread killed while reading event for channel &apos;group_replication_recovery&apos;2018-09-18T05:49:56.905535Z 99 [Note] Slave I/O thread exiting for channel &apos;group_replication_recovery&apos;, read up to log &apos;mysql-binlog.000005&apos;, position 105122018-09-18T05:49:56.999936Z 93 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;dax-mysql-slave&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-09-18T05:49:57.084035Z 0 [Note] Plugin group_replication reported: &apos;This server was declared online within the replication group&apos; 同步完成。 2、实验二，使用purge命令删除主库binlog文件从库21stop group_replication: 主库11234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#切换日志，创建表gtid_test12,并查看当前的binlog日志文件：root@db 06:04: [test]&gt; flush logs;create table gtid_test12 (ID int) engine=innodb;Query OK, 0 rows affected (0.03 sec)Query OK, 0 rows affected (0.05 sec)root@db 06:04: [test]&gt; show master status;+---------------------+----------+--------------+------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------------+----------+--------------+------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| mysql-binlog.000033 | 493 | | | 8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-30 |+---------------------+----------+--------------+------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)#切换日志，创建表gtid_test13,并查看当前的binlog日志文件：root@db 06:04: [test]&gt; flush logs;create table gtid_test13 (ID int) engine=innodb;Query OK, 0 rows affected (0.04 sec)Query OK, 0 rows affected (0.05 sec)root@db 06:04: [test]&gt; show master status;+---------------------+----------+--------------+------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------------+----------+--------------+------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| mysql-binlog.000034 | 493 | | | 8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-31 |+---------------------+----------+--------------+------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+#主库清除mysql-binlog.000034之前的所有binlog日志：purge binary logs to &apos;mysql-binlog.000034&apos;;#查看日志log目录下的34之前的日志已经全部清除：ll mysql-binlog.*-rw-r----- 1 mysql mysql 493 Sep 18 06:04 mysql-binlog.000034-rw-r----- 1 mysql mysql 36 Sep 18 06:22 mysql-binlog.index#查看主库gtid信息，gtid_purged已经有记录：show global variables like &apos;%gtid%&apos;;root@db 06:37: [test]&gt; show global variables like &apos;%gtid%&apos;;+---------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Variable_name | Value |+---------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| binlog_gtid_simple_recovery | ON || enforce_gtid_consistency | ON || group_replication_allow_local_disjoint_gtids_join | ON || group_replication_gtid_assignment_block_size | 1000000 || gtid_executed | 8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-31 || gtid_executed_compression_period | 1000 || gtid_mode | ON || gtid_owned | || gtid_purged | 8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-30 || session_track_gtids | OFF |+---------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+10 rows in set (0.01 sec) 从节点21start group_replication; 查看从库2的日志12345678910111213141516171819202122232425262018-09-18T06:38:36.664372Z 107 [Note] Plugin group_replication reported: &apos;Initialized group communication with configuration: group_replication_group_name: &quot;dd412cc2-ba1f-11e8-9ba2-000d3a801ae2&quot;; group_replication_local_address: &quot;dax-mysql-slave2:24901&quot;; group_replication_group_seeds: &quot;dax-mysql-slave:24901,dax-mysql-master:24901,dax-mysql-slave2:24901&quot;; group_replication_bootstrap_group: false; group_replication_poll_spin_loops: 0; group_replication_compression_threshold: 1000000; group_replication_ip_whitelist: &quot;AUTOMATIC&quot;&apos;2018-09-18T06:38:36.664408Z 107 [Note] Plugin group_replication reported: &apos;[GCS] Configured number of attempts to join: 0&apos;2018-09-18T06:38:36.664428Z 107 [Note] Plugin group_replication reported: &apos;[GCS] Configured time between attempts to join: 5 seconds&apos;2018-09-18T06:38:36.664457Z 107 [Note] Plugin group_replication reported: &apos;Member configuration: member_id: 3306101; member_uuid: &quot;c6ac9ccd-b80b-11e8-b968-000d3a801bf4&quot;; single-primary mode: &quot;true&quot;; group_replication_auto_increment_increment: 1; &apos;2018-09-18T06:38:36.664812Z 109 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_applier&apos; executed&apos;. Previous state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 493, master_bind=&apos;&apos;. New state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-09-18T06:38:36.736702Z 107 [Note] Plugin group_replication reported: &apos;Group Replication applier module successfully initialized!&apos;2018-09-18T06:38:36.736749Z 107 [Note] Plugin group_replication reported: &apos;auto_increment_increment is set to 1&apos;2018-09-18T06:38:36.736774Z 107 [Note] Plugin group_replication reported: &apos;auto_increment_offset is set to 3306101&apos;2018-09-18T06:38:36.736708Z 112 [Note] Slave SQL thread for channel &apos;group_replication_applier&apos; initialized, starting replication in log &apos;FIRST&apos; at position 0, relay log &apos;./relay-log-group_replication_applier.000014&apos; position: 42018-09-18T06:38:36.737167Z 0 [Note] Plugin group_replication reported: &apos;XCom protocol version: 3&apos;2018-09-18T06:38:36.737227Z 0 [Note] Plugin group_replication reported: &apos;XCom initialized and ready to accept incoming connections on port 24901&apos;2018-09-18T06:38:38.815550Z 107 [Note] Plugin group_replication reported: &apos;This server is working as secondary member with primary member address dax-mysql-master:3306.&apos;2018-09-18T06:38:38.815711Z 0 [ERROR] Plugin group_replication reported: &apos;Group contains 3 members which is greater than group_replication_auto_increment_increment value of 1. This can lead to an higher rate of transactional aborts.&apos;2018-09-18T06:38:38.816035Z 117 [Note] Plugin group_replication reported: &apos;Establishing group recovery connection with a possible donor. Attempt 1/10&apos;2018-09-18T06:38:38.816125Z 0 [Note] Plugin group_replication reported: &apos;Group membership changed to dax-mysql-slave:3306, dax-mysql-master:3306, dax-mysql-slave2:3306 on view 15371510730966421:33.&apos;2018-09-18T06:38:38.881138Z 117 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;dax-mysql-slave&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.#从节点2选择从从节点1获取binlog文件，因为从节点1文件没有删除，获取binlog文件正常。2018-09-18T06:38:38.959786Z 117 [Note] Plugin group_replication reported: &apos;Establishing connection to a group replication recovery donor 66d67181-ba5b-11e8-9c54-000d3a800ed3 at dax-mysql-slave port: 3306.&apos;2018-09-18T06:38:38.960147Z 119 [Warning] Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the &apos;START SLAVE Syntax&apos; in the MySQL Manual for more information.2018-09-18T06:38:38.962244Z 119 [Note] Slave I/O thread for channel &apos;group_replication_recovery&apos;: connected to master &apos;repl@dax-mysql-slave:3306&apos;,replication started in log &apos;FIRST&apos; at position 42018-09-18T06:38:38.982193Z 120 [Note] Slave SQL thread for channel &apos;group_replication_recovery&apos; initialized, starting replication in log &apos;FIRST&apos; at position 0, relay log &apos;./relay-log-group_replication_recovery.000001&apos; position: 42018-09-18T06:38:39.194864Z 117 [Note] Plugin group_replication reported: &apos;Terminating existing group replication donor connection and purging the corresponding logs.&apos;2018-09-18T06:38:39.218168Z 119 [Note] Slave I/O thread killed while reading event for channel &apos;group_replication_recovery&apos;2018-09-18T06:38:39.218197Z 119 [Note] Slave I/O thread exiting for channel &apos;group_replication_recovery&apos;, read up to log &apos;mysql-binlog.000005&apos;, position 113622018-09-18T06:38:39.313623Z 117 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;dax-mysql-slave&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-09-18T06:38:39.400236Z 0 [Note] Plugin group_replication reported: &apos;This server was declared online within the replication group&apos; 同步完成。 3、实验三，删除主库节点和从节点1的binlog文件。从库21stop group_replication; 主库1：12345678910113切换日志，创建表gtid_test14,并查看当前的binlog日志文件：flush logs;create table gtid_test14 (ID int) engine=innodb;show master status;#切换日志，创建表gtid_test15,并查看当前的binlog日志文件：flush logs;create table gtid_test15 (ID int) engine=innodb;show master status;#清除mysql-binlog.000036之前的日志：purge binary logs to &apos;mysql-binlog.000036&apos;;[root@dax-mysql-master log]# ll mysql-binlog.*-rw-r----- 1 mysql mysql 493 Sep 18 06:53 mysql-binlog.000036-rw-r----- 1 mysql mysql 36 Sep 18 06:56 mysql-binlog.index 从库1删除binlog日志：1234567891011121314151617181920212223242526272829303132333435363738394041424344show master status;+---------------------+----------+--------------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------------+----------+--------------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| mysql-binlog.000005 | 11728 | | | 324a6fd1-ba55-11e8-b3ff-000d3a800ed3:1,8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-34 |+---------------------+----------+--------------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+#切换binlog日志：flush logs;show master status;root@db 06:57: [test]&gt; show master status;+---------------------+----------+--------------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------------+----------+--------------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| mysql-binlog.000006 | 350 | | | 324a6fd1-ba55-11e8-b3ff-000d3a800ed3:1,8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-34 |+---------------------+----------+--------------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+[root@dax-mysql-slave log]# ll total 96-rw-r----- 1 mysql mysql 53478 Sep 18 06:49 error.log-rw-r----- 1 mysql mysql 177 Sep 17 08:39 mysql-binlog.000001-rw-r----- 1 mysql mysql 437 Sep 17 09:21 mysql-binlog.000002-rw-r----- 1 mysql mysql 217 Sep 17 09:25 mysql-binlog.000003-rw-r----- 1 mysql mysql 209 Sep 17 09:26 mysql-binlog.000004-rw-r----- 1 mysql mysql 11774 Sep 18 06:57 mysql-binlog.000005-rw-r----- 1 mysql mysql 350 Sep 18 06:57 mysql-binlog.000006-rw-r----- 1 mysql mysql 216 Sep 18 06:57 mysql-binlog.index-rw-r--r-- 1 mysql mysql 0 Sep 17 08:36 mysqld.log-rw-r----- 1 mysql mysql 1968 Sep 18 06:57 slow.log#删除mysql-binlog.000006之前的所有日志：purge binary logs to &apos;mysql-binlog.000006&apos;;[root@dax-mysql-slave log]# ll total 68-rw-r----- 1 mysql mysql 53478 Sep 18 06:49 error.log-rw-r----- 1 mysql mysql 350 Sep 18 06:57 mysql-binlog.000006-rw-r----- 1 mysql mysql 36 Sep 18 06:58 mysql-binlog.index-rw-r--r-- 1 mysql mysql 0 Sep 17 08:36 mysqld.log-rw-r----- 1 mysql mysql 1968 Sep 18 06:57 slow.log 从库21start group_replicatinon; 查看从库2的日志1234567891011121314151617181920212223242526272829303132333435363738392018-09-18T06:59:20.236425Z 136 [Note] Plugin group_replication reported: &apos;Retrying group recovery connection with another donor. Attempt 2/10&apos;2018-09-18T06:59:20.309947Z 136 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;dax-mysql-slave&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-09-18T06:59:20.391623Z 136 [Note] Plugin group_replication reported: &apos;Establishing connection to a group replication recovery donor 66d67181-ba5b-11e8-9c54-000d3a800ed3 at dax-mysql-slave port: 3306.&apos;2018-09-18T06:59:20.391891Z 142 [Warning] Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the &apos;START SLAVE Syntax&apos; in the MySQL Manual for more information.2018-09-18T06:59:20.394226Z 142 [Note] Slave I/O thread for channel &apos;group_replication_recovery&apos;: connected to master &apos;repl@dax-mysql-slave:3306&apos;,replication started in log &apos;FIRST&apos; at position 42018-09-18T06:59:20.416722Z 143 [Note] Slave SQL thread for channel &apos;group_replication_recovery&apos; initialized, starting replication in log &apos;FIRST&apos; at position 0, relay log &apos;./relay-log-group_replication_recovery.000001&apos; position: 42018-09-18T06:59:20.424400Z 142 [ERROR] Error reading packet from server for channel &apos;group_replication_recovery&apos;: The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires. (server_errno=1236)2018-09-18T06:59:20.424467Z 142 [ERROR] Slave I/O for channel &apos;group_replication_recovery&apos;: Got fatal error 1236 from master when reading data from binary log: &apos;The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires.&apos;, Error_code: 12362018-09-18T06:59:20.424488Z 142 [Note] Slave I/O thread exiting for channel &apos;group_replication_recovery&apos;, read up to log &apos;FIRST&apos;, position 42018-09-18T06:59:20.424640Z 136 [Note] Plugin group_replication reported: &apos;Terminating existing group replication donor connection and purging the corresponding logs.&apos;2018-09-18T06:59:20.436806Z 143 [Note] Error reading relay log event for channel &apos;group_replication_recovery&apos;: slave SQL thread was killed2018-09-18T06:59:20.522269Z 136 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;dax-mysql-slave&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-09-18T06:59:20.603837Z 136 [Note] Plugin group_replication reported: &apos;Retrying group recovery connection with another donor. Attempt 3/10&apos;2018-09-18T07:00:20.679461Z 136 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;dax-mysql-slave&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-09-18T07:00:20.762021Z 136 [Note] Plugin group_replication reported: &apos;Establishing connection to a group replication recovery donor 66d67181-ba5b-11e8-9c54-000d3a800ed3 at dax-mysql-slave port: 3306.&apos;2018-09-18T07:00:20.762368Z 146 [Warning] Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the &apos;START SLAVE Syntax&apos; in the MySQL Manual for more information.2018-09-18T07:00:20.764842Z 146 [Note] Slave I/O thread for channel &apos;group_replication_recovery&apos;: connected to master &apos;repl@dax-mysql-slave:3306&apos;,replication started in log &apos;FIRST&apos; at position 42018-09-18T07:00:20.787728Z 147 [Note] Slave SQL thread for channel &apos;group_replication_recovery&apos; initialized, starting replication in log &apos;FIRST&apos; at position 0, relay log &apos;./relay-log-group_replication_recovery.000001&apos; position: 42018-09-18T07:00:20.794590Z 146 [ERROR] Error reading packet from server for channel &apos;group_replication_recovery&apos;: The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires. (server_errno=1236)2018-09-18T07:00:20.794623Z 146 [ERROR] Slave I/O for channel &apos;group_replication_recovery&apos;: Got fatal error 1236 from master when reading data from binary log: &apos;The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires.&apos;, Error_code: 12362018-09-18T07:00:20.794635Z 146 [Note] Slave I/O thread exiting for channel &apos;group_replication_recovery&apos;, read up to log &apos;FIRST&apos;, position 42018-09-18T07:00:20.794786Z 136 [Note] Plugin group_replication reported: &apos;Terminating existing group replication donor connection and purging the corresponding logs.&apos;2018-09-18T07:00:20.807977Z 147 [Note] Error reading relay log event for channel &apos;group_replication_recovery&apos;: slave SQL thread was killed2018-09-18T07:00:20.895774Z 136 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;dax-mysql-slave&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.#从库2多次尝试从从库1获取binlog日志，没有成功，切换master_host节点，尝试从主库获取binlog日志文件：2018-09-18T07:00:20.980112Z 136 [Note] Plugin group_replication reported: &apos;Retrying group recovery connection with another donor. Attempt 4/10&apos;2018-09-18T07:00:21.052930Z 136 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;dax-mysql-master&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-09-18T07:00:21.141126Z 136 [Note] Plugin group_replication reported: &apos;Establishing connection to a group replication recovery donor 8182e5ae-af54-11e8-af0e-000d3a801ae2 at dax-mysql-master port: 3306.&apos;2018-09-18T07:00:21.141498Z 150 [Warning] Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the &apos;START SLAVE Syntax&apos; in the MySQL Manual for more information.2018-09-18T07:00:21.143773Z 150 [Note] Slave I/O thread for channel &apos;group_replication_recovery&apos;: connected to master &apos;repl@dax-mysql-master:3306&apos;,replication started in log &apos;FIRST&apos; at position 42018-09-18T07:00:21.166993Z 151 [Note] Slave SQL thread for channel &apos;group_replication_recovery&apos; initialized, starting replication in log &apos;FIRST&apos; at position 0, relay log &apos;./relay-log-group_replication_recovery.000001&apos; position: 42018-09-18T07:00:21.171461Z 150 [ERROR] Error reading packet from server for channel &apos;group_replication_recovery&apos;: The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires. (server_errno=1236)2018-09-18T07:00:21.171518Z 150 [ERROR] Slave I/O for channel &apos;group_replication_recovery&apos;: Got fatal error 1236 from master when reading data from binary log: &apos;The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires.&apos;, Error_code: 12362018-09-18T07:00:21.171528Z 150 [Note] Slave I/O thread exiting for channel &apos;group_replication_recovery&apos;, read up to log &apos;FIRST&apos;, position 42018-09-18T07:00:21.171645Z 136 [Note] Plugin group_replication reported: &apos;Terminating existing group replication donor connection and purging the corresponding logs.&apos;2018-09-18T07:00:21.186565Z 151 [Note] Error reading relay log event for channel &apos;group_replication_recovery&apos;: slave SQL thread was killed2018-09-18T07:00:21.271867Z 136 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;dax-mysql-master&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.....[ERROR] Slave I/O for channel &apos;group_replication_recovery&apos;: Got fatal error 1236 from master when reading data from binary log: &apos;The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires.&apos;, Error_code: 1236 因为主库1和从库1的binlog文件都删除，从库2无法同步，最后退出集群。 4、查看主库清除的日志12345678910111213141516171819202122 show global variables like &apos;%gtid%&apos;;+---------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Variable_name | Value |+---------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| binlog_gtid_simple_recovery | ON || enforce_gtid_consistency | ON || group_replication_allow_local_disjoint_gtids_join | ON || group_replication_gtid_assignment_block_size | 1000000 || gtid_executed | 8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-35 || gtid_executed_compression_period | 1000 || gtid_mode | ON || gtid_owned | || gtid_purged | 8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-33 || session_track_gtids | OFF |+---------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+10 rows in set (0.00 sec) 从库2尝试过滤掉缺失的那部分日志，然后重新加入集群：1234stop group_replication;reset master;set global GTID_PURGED=&quot;8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-33&quot;;start group_replication; 再次查看从库2的日志1234567891011121314151617181920212223242526272829303132333435363738392018-09-18T07:08:05.524626Z 125 [Note] @@GLOBAL.GTID_PURGED was changed from &apos;&apos; to &apos;8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-33&apos;.2018-09-18T07:08:05.524661Z 125 [Note] @@GLOBAL.GTID_EXECUTED was changed from &apos;&apos; to &apos;8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-33&apos;.2018-09-18T07:08:20.457841Z 125 [Note] Plugin group_replication reported: &apos;Group communication SSL configuration: group_replication_ssl_mode: &quot;DISABLED&quot;&apos;2018-09-18T07:08:20.457971Z 125 [Note] Plugin group_replication reported: &apos;[GCS] Added automatically IP ranges 10.0.7.51/24,127.0.0.1/8 to the whitelist&apos;2018-09-18T07:08:20.458188Z 125 [Note] Plugin group_replication reported: &apos;[GCS] Translated &apos;dax-mysql-slave2&apos; to 10.0.7.51&apos;2018-09-18T07:08:20.458321Z 125 [Warning] Plugin group_replication reported: &apos;[GCS] Automatically adding IPv4 localhost address to the whitelist. It is mandatory that it is added.&apos;2018-09-18T07:08:20.458401Z 125 [Note] Plugin group_replication reported: &apos;[GCS] SSL was not enabled&apos;2018-09-18T07:08:20.458433Z 125 [Note] Plugin group_replication reported: &apos;Initialized group communication with configuration: group_replication_group_name: &quot;dd412cc2-ba1f-11e8-9ba2-000d3a801ae2&quot;; group_replication_local_address: &quot;dax-mysql-slave2:24901&quot;; group_replication_group_seeds: &quot;dax-mysql-slave:24901,dax-mysql-master:24901,dax-mysql-slave2:24901&quot;; group_replication_bootstrap_group: false; group_replication_poll_spin_loops: 0; group_replication_compression_threshold: 1000000; group_replication_ip_whitelist: &quot;AUTOMATIC&quot;&apos;2018-09-18T07:08:20.458473Z 125 [Note] Plugin group_replication reported: &apos;[GCS] Configured number of attempts to join: 0&apos;2018-09-18T07:08:20.458480Z 125 [Note] Plugin group_replication reported: &apos;[GCS] Configured time between attempts to join: 5 seconds&apos;2018-09-18T07:08:20.458503Z 125 [Note] Plugin group_replication reported: &apos;Member configuration: member_id: 3306101; member_uuid: &quot;c6ac9ccd-b80b-11e8-b968-000d3a801bf4&quot;; single-primary mode: &quot;true&quot;; group_replication_auto_increment_increment: 1; &apos;2018-09-18T07:08:20.458664Z 181 [Note] Plugin group_replication reported: &apos;Detected previous RESET MASTER invocation or an issue exists in the group replication applier relay log. Purging existing applier logs.&apos;2018-09-18T07:08:20.531810Z 181 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_applier&apos; executed&apos;. Previous state master_host=&apos;&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-09-18T07:08:20.632692Z 125 [Note] Plugin group_replication reported: &apos;Group Replication applier module successfully initialized!&apos;2018-09-18T07:08:20.632736Z 125 [Note] Plugin group_replication reported: &apos;auto_increment_increment is set to 1&apos;2018-09-18T07:08:20.632758Z 125 [Note] Plugin group_replication reported: &apos;auto_increment_offset is set to 3306101&apos;2018-09-18T07:08:20.632694Z 184 [Note] Slave SQL thread for channel &apos;group_replication_applier&apos; initialized, starting replication in log &apos;FIRST&apos; at position 0, relay log &apos;./relay-log-group_replication_applier.000001&apos; position: 42018-09-18T07:08:20.632947Z 0 [Note] Plugin group_replication reported: &apos;XCom protocol version: 3&apos;2018-09-18T07:08:20.632977Z 0 [Note] Plugin group_replication reported: &apos;XCom initialized and ready to accept incoming connections on port 24901&apos;2018-09-18T07:08:22.329812Z 125 [Note] Plugin group_replication reported: &apos;This server is working as secondary member with primary member address dax-mysql-master:3306.&apos;2018-09-18T07:08:22.329901Z 0 [ERROR] Plugin group_replication reported: &apos;Group contains 3 members which is greater than group_replication_auto_increment_increment value of 1. This can lead to an higher rate of transactional aborts.&apos;2018-09-18T07:08:22.330145Z 189 [Note] Plugin group_replication reported: &apos;Establishing group recovery connection with a possible donor. Attempt 1/10&apos;2018-09-18T07:08:22.330221Z 0 [Note] Plugin group_replication reported: &apos;Group membership changed to dax-mysql-slave:3306, dax-mysql-master:3306, dax-mysql-slave2:3306 on view 15371510730966421:37.&apos;2018-09-18T07:08:22.394512Z 189 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;dax-mysql-master&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-09-18T07:08:22.468142Z 189 [Note] Plugin group_replication reported: &apos;Establishing connection to a group replication recovery donor 8182e5ae-af54-11e8-af0e-000d3a801ae2 at dax-mysql-master port: 3306.&apos;2018-09-18T07:08:22.468378Z 191 [Warning] Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the &apos;START SLAVE Syntax&apos; in the MySQL Manual for more information.2018-09-18T07:08:22.470340Z 191 [Note] Slave I/O thread for channel &apos;group_replication_recovery&apos;: connected to master &apos;repl@dax-mysql-master:3306&apos;,replication started in log &apos;FIRST&apos; at position 42018-09-18T07:08:22.490866Z 192 [Note] Slave SQL thread for channel &apos;group_replication_recovery&apos; initialized, starting replication in log &apos;FIRST&apos; at position 0, relay log &apos;./relay-log-group_replication_recovery.000001&apos; position: 42018-09-18T07:08:22.632774Z 189 [Note] Plugin group_replication reported: &apos;Terminating existing group replication donor connection and purging the corresponding logs.&apos;2018-09-18T07:08:22.653083Z 191 [Note] Slave I/O thread killed while reading event for channel &apos;group_replication_recovery&apos;2018-09-18T07:08:22.653098Z 191 [Note] Slave I/O thread exiting for channel &apos;group_replication_recovery&apos;, read up to log &apos;mysql-binlog.000036&apos;, position 13812018-09-18T07:08:22.734186Z 189 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;dax-mysql-master&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-09-18T07:08:22.808784Z 0 [Note] Plugin group_replication reported: &apos;This server was declared online within the replication group&apos; 此时从库2与主库、从库1的数据是不一致的，gtid_test14表时不存在的，因此需要使用pt-table-checksum和pt-table-sync去同步，但是该方法在mysql-mgr环境下测试没有成功。另一种方法是直接备份一份主节点的最新备份数据，在从库2上恢复。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Linux] GlusterFS部署详解]]></title>
    <url>%2F2018%2F11%2F25%2FGlusterFS%2F</url>
    <content type="text"><![CDATA[0、安装之前须知：123456789推荐使用xfs，没用xfs推荐使用ext4，其他文件系统也可兼容；生产环境DNS NTP服务必须安装,DNS服务配置可点击[此处](https://zeven0707.github.io/2018/11/22/DNS%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2/)；如果是虚拟机环境最少1G内存；最好配置2个网卡，一个管理接口，一个数据传输接口；如果使用vm克隆额外的机器，确保glusterfs没有安装，如果安装了Gluster,会在每个系统上生产一个uuid，因此如果克隆了一个已经安装了gluster的系统，后面会报错；如果使用物理服务器，建议最低配置（2 CPU’s, 2GB of RAM, 1GBE），板载组件不如附加组件强大；安装官方文档，点击[此处](https://docs.gluster.org/en/latest/Install-Guide/Install/)centos7快速安装指南，点击[此处](https://wiki.centos.org/SpecialInterestGroup/Storage/gluster-Quickstart)/var/log目录最好单独挂在，一旦日志目录无法写入信息，gluster fs会出现古怪现象 1、配置yum源1yum install centos-release-gluster 2、在两个节点上添加磁盘，格式化,并挂在目录12345678#格式化磁盘，创建相应的挂在目录mkfs.xfs -i size=512 /dev/sdb1mkdir -p /gfs/test1#将挂在配置信息写入fstab配置文件，以便重启自动挂载vi /etc/fstab/dev/sdb1 /gfs/test1 xfs defaults 1 2#加载修改的配置信息mount -a &amp;&amp; mount Note: 在CentOS 6操作系统,需要安装xfs文件系统：1yum install xfsprogs 3、安装、配置glusterd服务1yum install glusterfs-server 启动GlusterFS 管理进程123456789101112131415161718#加入开机启动systemctl enable glusterdln -s &apos;/usr/lib/systemd/system/glusterd.service&apos; &apos;/etc/systemd/system/multi-user.target.wants/glusterd.service&apos;#启动glusterdsystemctl start glusterd#查看glusterd状态信息systemctl status glusterd● glusterd.service - GlusterFS, a clustered file-system server Loaded: loaded (/usr/lib/systemd/system/glusterd.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2018-11-15 12:08:54 EST; 15s ago Process: 2808 ExecStart=/usr/sbin/glusterd -p /var/run/glusterd.pid --log-level $LOG_LEVEL $GLUSTERD_OPTIONS (code=exited, status=0/SUCCESS) Main PID: 2810 (glusterd) Tasks: 8 CGroup: /system.slice/glusterd.service └─2810 /usr/sbin/glusterd -p /var/run/glusterd.pid --log-level INFONov 15 12:08:53 node1 systemd[1]: Starting GlusterFS, a clustered file-system server...Nov 15 12:08:54 node1 systemd[1]: Started GlusterFS, a clustered file-system server. 4、防火墙配置默认glusted监听tcp/24007 ，但是你增加一个brick，会开启一个新的端口，通过命令”gluster volume status”可以查询到，因此如果各个节点配置了防火墙，新增一个brick之后，需要注意修改防火墙的端口限制，不然下面的配置受信任池会报错。 5、配置受信任池node1操作，将node2添加到受信任池：12[root@node1 ~]# gluster peer probe node2peer probe: success. 如果防火墙开启可能会提示下面错误：12[root@node1 ~]# gluster peer probe node2peer probe: failed: Probe returned with Transport endpoint is not connected node2操作，将node1添加到受信任池12[root@node2 ~]# gluster peer probe node1peer probe: success. Host node1 port 24007 already in peer list 6、建立GlusterFS volumenode1 and node2操作：1mkdir /gfs/test1/gv0 在任意一个节点上执行即可，不需要重复执行：123456# gluster volume create gv0 replica 2 node1:/gfs/test1/gv0 node2:/gfs/test1/gv0[root@node1 ~]# gluster volume create gv0 replica 2 node1:/gfs/test1/gv0 node2:/gfs/test1/gv0Replica 2 volumes are prone to split-brain. Use Arbiter or Replica 3 to avoid this. See: http://docs.gluster.org/en/latest/Administrator%20Guide/Split%20brain%20and%20ways%20to%20deal%20with%20it/.Do you still want to continue? (y/n) yvolume create: gv0: success: please start the volume to access data 提示只有2个副本有可能出现脑裂的情况，创建带有仲裁节点以上的gluserfs volume至少三个节点，因此两个节点的副本集可以只能忽略这个问题。下面启动创建的gv0卷1234# gluster volume start gv0[root@node1 ~]# gluster volume start gv0volume start: gv0: successConfirm that the volume shows &quot;Started&quot;: 查看启动的gv0卷信息1234567891011121314151617# gluster volume info[root@node1 ~]# gluster volume info Volume Name: gv0Type: ReplicateVolume ID: 79c81f10-0cb8-4f26-a7ab-d21fe19f0bbfStatus: StartedSnapshot Count: 0Number of Bricks: 1 x 2 = 2Transport-type: tcpBricks:Brick1: node1:/gfs/test1/gv0Brick2: node2:/gfs/test1/gv0Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off /var/log/glusterfs如果没有正常启动，可以查看日志 7、测试Glusterfs volume gv0副本集是否生效12345678#挂在到任意空目录mount -t glusterfs node1:/gv0 /mnt#创造测试数据for i in `seq -w 1 100`; do cp -rp /var/log/messages /mnt/copy-test-$i; done#查看生成数据的数量ls /mnt | wc -l#在node1和node2的/gfs/test1/gv0目录下均生成了100个文件ls /gfs/test1/gv0 |wc -l [root@node1 ~]# gluster volume create gv1 disperse 2 node1:/gfs/test1/gv1 node2:/gfs/test1/gv1disperse count must be greater than 2disperse option given twice Usage:volume create [stripe ] [replica [arbiter ]] [disperse []] [disperse-data ] [redundancy ] [transport &lt;tcp|rdma|tcp,rdma&gt;] ?&lt;vg_name&gt;… [force] 8、添加新的节点，因为受信任池已经创建，需要在受信任池内的节点下，将新节点node3进来1234567891011[root@node1 ~]# gluster peer probe node3[root@node1 ~]# gluster peer statusNumber of Peers: 2Hostname: node2Uuid: 588d2f92-f085-4e74-ab63-c6f5aa6ffb24State: Peer in Cluster (Connected)Hostname: node3Uuid: 3495bc9c-7330-4038-ac94-1777ba0286f5State: Peer in Cluster (Connected) 9、创建2节点分布式volume1234567891011121314151617181920212223#创建卷不添加任何参数的情况下，默认模式为分布式：#gluster volume create gv3 node1:/gfs/test1/gv3 node2:/gfs/test1/gv3[root@node1 ~]# gluster volume create gv3 node1:/gfs/test1/gv3 node2:/gfs/test1/gv3volume create: gv1: success: please start the volume to access data# 启动gv3# gluster volume start gv3[root@node1 ~]# gluster volume start gv3[root@node1 ~]# gluster volume info gv3Volume Name: gv3Type: DistributeVolume ID: 02e3163b-1c69-402f-aad5-9cf4dba3267cStatus: StartedSnapshot Count: 0Number of Bricks: 2Transport-type: tcpBricks:Brick1: node1:/gfs/test1/gv3Brick2: node2:/gfs/test1/gv3Options Reconfigured:transport.address-family: inetnfs.disable: on 10、测试Glusterfs volume gv312345678910111213141516#挂载并创建测试数据mount -t glusterfs node1:/gv3 /mntfor i in `seq -w 1 100`; do cp -rp /var/log/messages /mnt/copy-test-$i; done#node1查看数据：[root@node1 gv3]# ls /mnt | wc -l100#node1:查看文件实际位置下的数据：[root@node1 gv3]# ls /gfs/test1/gv3/ |wc -l50#node2：查看文件实际位置下的数据：[root@node2 gv3]# ls /gfs/test1/gv3/ |wc -l50#虽然在节点3没有生成集群卷，在节点3上挂在/gv3也可以看到数据：mount -t glusterfs 127.0.0.1:/gv3 /mnt[root@node3 mnt]# ls /mnt/ |wc -l100 11、创建分布式副本123456789101112131415161718192021222324252627282930# gluster volume create gv4 replica 2 transport tcp node1:/gfs/test1/gv4 node2:/gfs/test1/gv4 node3:/gfs/test1/gv4 node4:/gfs/test1/gv4[root@node1 test1]# gluster volume create gv4 replica 2 transport tcp node1:/gfs/test1/gv4 node2:/gfs/test1/gv4 node3:/gfs/test1/gv4 Replica 2 volumes are prone to split-brain. Use Arbiter or Replica 3 to avoid this. See: http://docs.gluster.org/en/latest/Administrator%20Guide/Split%20brain%20and%20ways%20to%20deal%20with%20it/.Do you still want to continue? (y/n) ynumber of bricks is not a multiple of replica count#创建副本的节点数要和副本个数成倍数关系[root@node1 test1]# gluster volume create gv4 replica 2 transport tcp node1:/gfs/test1/gv4 node2:/gfs/test1/gv4 node3:/gfs/test1/gv4 node4:/gfs/test1/gv4Replica 2 volumes are prone to split-brain. Use Arbiter or Replica 3 to avoid this. See: http://docs.gluster.org/en/latest/Administrator%20Guide/Split%20brain%20and%20ways%20to%20deal%20with%20it/.Do you still want to continue? (y/n) yvolume create: gv4: success: please start the volume to access data[root@node1 test1]# gluster volume start gv4[root@node1 test1]# gluster volume info gv4Volume Name: gv4Type: Distributed-ReplicateVolume ID: e8556b2e-462d-4407-99c4-a6e622754e6cStatus: StartedSnapshot Count: 0Number of Bricks: 2 x 2 = 4Transport-type: tcpBricks:Brick1: node1:/gfs/test1/gv4Brick2: node2:/gfs/test1/gv4Brick3: node3:/gfs/test1/gv4Brick4: node4:/gfs/test1/gv4Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off 12、测试Glusterfs volume gv4:1234567891011121314151617181920212223242526272829303132#挂载并创建测试数据mount -t glusterfs node1:/gv4 /mntfor i in `seq -w 1 100`; do cp -rp /var/log/messages /mnt/copy-test-$i; done#查看数据分布，node1与node2数据相同，node3和node4存储另一半信息[root@node1 test1]# ls /gfs/test1/gv4copy-test-001 copy-test-012 copy-test-021 copy-test-029 copy-test-034 copy-test-048 copy-test-060 copy-test-078 copy-test-086 copy-test-094copy-test-004 copy-test-015 copy-test-022 copy-test-030 copy-test-038 copy-test-051 copy-test-063 copy-test-079 copy-test-087 copy-test-095copy-test-006 copy-test-016 copy-test-023 copy-test-031 copy-test-039 copy-test-052 copy-test-065 copy-test-081 copy-test-088 copy-test-098copy-test-008 copy-test-017 copy-test-024 copy-test-032 copy-test-041 copy-test-054 copy-test-073 copy-test-082 copy-test-090 copy-test-099copy-test-011 copy-test-019 copy-test-028 copy-test-033 copy-test-046 copy-test-057 copy-test-077 copy-test-083 copy-test-093 copy-test-100[root@node2 gv0]# ls /gfs/test1/gv4copy-test-001 copy-test-012 copy-test-021 copy-test-029 copy-test-034 copy-test-048 copy-test-060 copy-test-078 copy-test-086 copy-test-094copy-test-004 copy-test-015 copy-test-022 copy-test-030 copy-test-038 copy-test-051 copy-test-063 copy-test-079 copy-test-087 copy-test-095copy-test-006 copy-test-016 copy-test-023 copy-test-031 copy-test-039 copy-test-052 copy-test-065 copy-test-081 copy-test-088 copy-test-098copy-test-008 copy-test-017 copy-test-024 copy-test-032 copy-test-041 copy-test-054 copy-test-073 copy-test-082 copy-test-090 copy-test-099copy-test-011 copy-test-019 copy-test-028 copy-test-033 copy-test-046 copy-test-057 copy-test-077 copy-test-083 copy-test-093 copy-test-100[root@node3 test]# ls /gfs/test1/gv4copy-test-002 copy-test-010 copy-test-025 copy-test-037 copy-test-045 copy-test-055 copy-test-062 copy-test-069 copy-test-075 copy-test-089copy-test-003 copy-test-013 copy-test-026 copy-test-040 copy-test-047 copy-test-056 copy-test-064 copy-test-070 copy-test-076 copy-test-091copy-test-005 copy-test-014 copy-test-027 copy-test-042 copy-test-049 copy-test-058 copy-test-066 copy-test-071 copy-test-080 copy-test-092copy-test-007 copy-test-018 copy-test-035 copy-test-043 copy-test-050 copy-test-059 copy-test-067 copy-test-072 copy-test-084 copy-test-096copy-test-009 copy-test-020 copy-test-036 copy-test-044 copy-test-053 copy-test-061 copy-test-068 copy-test-074 copy-test-085 copy-test-097[root@node4 ~]# ls /gfs/test1/gv4copy-test-002 copy-test-010 copy-test-025 copy-test-037 copy-test-045 copy-test-055 copy-test-062 copy-test-069 copy-test-075 copy-test-089copy-test-003 copy-test-013 copy-test-026 copy-test-040 copy-test-047 copy-test-056 copy-test-064 copy-test-070 copy-test-076 copy-test-091copy-test-005 copy-test-014 copy-test-027 copy-test-042 copy-test-049 copy-test-058 copy-test-066 copy-test-071 copy-test-080 copy-test-092copy-test-007 copy-test-018 copy-test-035 copy-test-043 copy-test-050 copy-test-059 copy-test-067 copy-test-072 copy-test-084 copy-test-096copy-test-009 copy-test-020 copy-test-036 copy-test-044 copy-test-053 copy-test-061 copy-test-068 copy-test-074 copy-test-085 copy-test-097 13、创建带有仲裁的副本集12345678910111213141516171819202122232425[root@node1 test1]# gluster volume create gv5 replica 2 arbiter 2 transport tcp node1:/gfs/test1/gv5 node2:/gfs/test1/gv5 node3:/gfs/test1/gv5Replica 2 volumes are prone to split-brain. Use Arbiter or Replica 3 to avoid this. See: http://docs.gluster.org/en/latest/Administrator%20Guide/Split%20brain%20and%20ways%20to%20deal%20with%20it/.Do you still want to continue? (y/n) yFor arbiter configuration, replica count must be 3 and arbiter count must be 1. The 3rd brick of the replica will be the arbiter#提示创建仲裁必须是三个副本集[root@node1 test1]# gluster volume create gv5 replica 3 arbiter 1 transport tcp node1:/gfs/test1/gv5 node2:/gfs/test1/gv5 node3:/gfs/test1/gv5#启动gv5[root@node1 test1]# gluster volume start gv5[root@node1 test1]# gluster volume info gv5Volume Name: gv5Type: ReplicateVolume ID: fd4fca20-1bb3-480b-9c24-703dd3e8b508Status: StartedSnapshot Count: 0Number of Bricks: 1 x (2 + 1) = 3Transport-type: tcpBricks:Brick1: node1:/gfs/test1/gv5Brick2: node2:/gfs/test1/gv5Brick3: node3:/gfs/test1/gv5 (arbiter)Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off 14、GlusterFS的常见的卷介绍123456789Distributed：分布式卷，文件通过 hash 算法随机分布到由 bricks 组成的卷上。Replicated: 复制式卷，类似 RAID 1，replica 数必须等于 volume 中 brick 所包含的存储服务器数，可用性高。Striped: 条带式卷，类似 RAID 0，stripe 数必须等于 volume 中 brick 所包含的存储服务器数，文件被分成数据块，以 Round Robin 的方式存储在 bricks 中，并发粒度是数据块，大文件性能好。Distributed Striped: 分布式的条带卷，volume中 brick 所包含的存储服务器数必须是 stripe 的倍数（&gt;=2倍），兼顾分布式和条带式的功能。Distributed Replicated: 分布式的复制卷，volume 中 brick 所包含的存储服务器数必须是 replica 的倍数（&gt;=2倍），兼顾分布式和复制式的功能。分布式复制卷的brick顺序决定了文件分布的位置，一般来说，先是两个brick形成一个复制关系，然后两个复制关系形成分布。企业一般用后两种，大部分会用分布式复制（可用容量为 总容量/复制份数），通过网络传输的话最好用万兆交换机，万兆网卡来做。这样就会优化一部分性能。它们的数据都是通过网络来传输的。查看官方提供的gluster volume，点击[此处](https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/) 15、删除卷123#先停止要删除的卷[root@node01 ~]# gluster volume stop gv1[root@node01 ~]# gluster volume delete gv 16、监控负载123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118volume profile &lt;VOLNAME&gt; &#123;start|info [peek|incremental [peek]|cumulative|clear]|stop&#125; [nfs]#启动性能分析：gluster volume profile gv5 start#显示io信息：gluster volume profile gv5 info#停止性能分析：gluster volume profile gv5 stop#使用top命令查看读取，写入，文件打开调用，读取调用，写入调用等指标：volume top &lt;VOLNAME&gt; &#123;open|read|write|opendir|readdir|clear&#125; [nfs|brick &lt;brick&gt;] [list-cnt &lt;value&gt;] |volume top &lt;VOLNAME&gt; &#123;read-perf|write-perf&#125; [bs &lt;size&gt; count &lt;count&gt;] [brick &lt;brick&gt;] [list-cnt &lt;value&gt;]#查看fd的当前打开数，和最大打开数gluster volume top gv5 open brick node1:/gfs/test1/gv5 list-cnt 10Brick: node1:/gfs/test1/gv5Current open fds: 0, Max open fds: 1, Max openfd time: 2018-11-21 14:10:02.279118#显示文件读取调用数量：gluster volume top gv5 read brick node1:/gfs/test1/gv5 list-cnt 10Brick: node1:/gfs/test1/gv5Count filename=======================1 /copy-test-0021 /copy-test-001#每个brick下面文件被写入的数量列表gluster volume top gv5 write brick node1:/gfs/test1/gv5 list-cnt 10Brick: node1:/gfs/test1/gv5Count filename=======================12 /copy-test-10012 /copy-test-09912 /copy-test-09812 /copy-test-09712 /copy-test-09612 /copy-test-09512 /copy-test-09412 /copy-test-09312 /copy-test-09212 /copy-test-091#当前brick下，目录被打开的数量列表[root@node1 test1]# gluster volume top gv5 opendir brick node1:/gfs/test1/gv5 list-cnt 10Brick: node1:/gfs/test1/gv5Count filename=======================1 /test#当前brick下，目录被读的数量列表[root@node1 test1]# gluster volume top gv5 readdir brick node1:/gfs/test1/gv5 list-cnt 10Brick: node1:/gfs/test1/gv5Count filename=======================2 /test#查看brick的读性能：[root@node1 test1]# gluster volume top gv5 read-perf bs 256 count 1 brick node1:/gfs/test1/gv5 list-cnt 10Brick: node1:/gfs/test1/gv5Throughput 51.20 MBps time 0.0000 secsMBps Filename Time ==== ======== ==== 0 /copy-test-001 2018-11-21 16:14:30.512003 0 /copy-test-003 2018-11-21 16:13:32.481649 0 /copy-test-002 2018-11-21 16:06:19.696071#查看brick的写性能：[root@node1 test1]# gluster volume top gv5 write-perf bs 256 count 1 brick node1:/gfs/test1/gv5 list-cnt 10Brick: node1:/gfs/test1/gv5Throughput 10.67 MBps time 0.0000 secsMBps Filename Time ==== ======== ==== 0 /.copy-test-001.swp 2018-11-21 16:14:53.514516 0 /copy-test-001 2018-11-21 16:14:53.489068 0 /.copy-test-001.swp 2018-11-21 16:14:30.483696 0 /.copy-test-003.swp 2018-11-21 16:12:46.331094 0 /copy-test-100 2018-11-21 14:10:06.65163 0 /copy-test-099 2018-11-21 14:10:05.953871 0 /copy-test-098 2018-11-21 14:10:05.773626 0 /copy-test-097 2018-11-21 14:10:05.620743 0 /copy-test-096 2018-11-21 14:10:05.591005 0 /copy-test-095 2018-11-21 14:10:05.555036#查看单个卷的信息：gluster volume info &lt;VOLNAME&gt;gluster volume info gv5Volume Name: gv5Type: ReplicateVolume ID: e12e23f5-7347-4049-8d77-53cef76b0633Status: StartedSnapshot Count: 0Number of Bricks: 1 x (2 + 1) = 3Transport-type: tcpBricks:Brick1: node1:/gfs/test1/gv5Brick2: node2:/gfs/test1/gv5Brick3: node3:/gfs/test1/gv5 (arbiter)Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off#查看所有卷的信息：gluster volume info all#查看卷状态：gluster volume status [all| []] [detail|clients|mem|inode|fd|callpool]#显示所有卷的状态gluster volume status all#显示卷额外的信息：gluster volume status gv5 detail#显示客户端列表：gluster volume status gv5 clients#显示内存使用情况：gluster volume status gv5 mem#显示卷的inode表gluster volume status gv5 inode#显示卷打开的fd表gluster volume status gv5 fd#显示卷的挂起调用gluster volume status gv5 callpool 17、glusterfs其他信息查看123456789101112131415161718192021222324252627#看下节点有没有在线gluster volume status nfsp#启动完全修复gluster volume heal gv2 full#查看需要修复的文件gluster volume heal gv2 info#查看修复成功的文件gluster volume heal gv2 info healed#查看修复失败的文件gluster volume heal gv2 heal-failed#查看脑裂的文件gluster volume heal gv2 info split-brain#激活quota功能gluster volume quota gv2 enable#关闭quota功能gulster volume quota gv2 disable#目录限制（卷中文件夹的大小）gluster volume quota limit-usage /data/30MB --/gv2/data#quota信息列表gluster volume quota gv2 list#限制目录的quota信息gluster volume quota gv2 list /data#设置信息的超时时间gluster volume set gv2 features.quota-timeout 5#删除某个目录的quota设置gluster volume quota gv2 remove /data备注：quota功能，主要是对挂载点下的某个目录进行空间限额。如：/mnt/gulster/data目录，而不是对组成卷组的空间进行限制。 18、使用zabbix模板监控glusterfs，zabbix官网有自带的模板，CPU、内存、磁盘空间、主机运行时间、系统load。12gfszabbix监控https://github.com/MrCirca/zabbix-glusterfs]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Linux] vsftp启用虚拟用户功能]]></title>
    <url>%2F2018%2F11%2F23%2Fvsftp%E5%90%AF%E7%94%A8%E8%99%9A%E6%8B%9F%E7%94%A8%E6%88%B7%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[0、部署vsftp-server，请先跳转到此处，部署vsfpt-ssl功能，点击此处1、在/etc/vsftpd.conf配置文件添加以下内容12345678#开启虚拟用户的功能guest_enable=YES#指定虚拟用户的宿主用户guest_username=vir#指定虚拟用户配置文件的存放路径user_config_dir=/etc/vsftpd/vuser_conf#虚拟用户和本地用户有相同的权限virtual_use_local_privs=YES 2、建立虚拟用户名文件1234[root@localhost ~]# vim /etc/vsftpd/vsftpd_users.conf#输入奇数行为账号，偶数行为密码f1123456 3、生成认证文件,如果db_load命令不存在先安装(yum install db4 db4-utils)1[root@localhost ~]# db_load -T -t hash -f /etc/vsftpd/vsftpd_users.conf /etc/vsftpd/vsftpd_users.db 4、文件生成之后，修改文件权限1[root@localhost ~]# chmod 600 /etc/vsftpd/vsftpd_users.db 5、编辑认证文件,将其余行注释，只保留第一行和最后两行“/etc/vsftpd/vsftpd_users”根据自己配置的文件设置，.db后缀无需添加1234567891011[root@localhost f2]# more /etc/pam.d/vsftpd#%PAM-1.0#session optional pam_keyinit.so force revoke#auth required pam_listfile.so item=user sense=deny file=/etc/vsftpd/ftpusers onerr=succeed#auth required pam_shells.so#auth include password-auth#account include password-auth#session required pam_loginuid.so#session include password-authauth sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/vsftpd_usersaccount sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/vsftpd_users 6、建立虚拟用户配置文件存放位置12345678910[root@localhost ~]# mkdir /etc/vsftpd/vuser_conf/[root@localhost ~]# vim /etc/vsftpd/vuser_conf/f1#添加以下内容local_root=/var/ftp/f1write_enable=YESanon_umask=022anon_world_readable_only=NOanon_upload_enable=YESanon_mkdir_write_enable=YESanon_other_write_enable=YES 7、将f1用户加入/etc/vsftpd/chroot_list，/etc/vsftpd/vsftpd.user_list允许f1用户登录1234[root@localhost vsftpd]# more /etc/vsftpd/chroot_list |grep f1f1[root@localhost vsftpd]# more /etc/vsftpd/vsftpd.user_list |grep f1f1 8、重新启动vsftp1[root@localhost ~]# systemctl vsftpd restart 9、测试用户能否正常登录：1234567891011121314[root@localhost surgeftp_2.3f2_linux64]# ./sslftp 192.168.168.120Connected to 192.168.168.120220 (vsFTPd 3.0.2)234 Proceed with negotiation.starting SSL/TLSsslinit 3Negotiated secure protocol TLSv1.2, using an AES cipher.200 PBSZ set to 0.200 PROT now Private.(secure) User: f1331 Please specify the password.(secure) Password: ******Connection problem SSLTCP:525:ssl_read tcp:-1000:SSL failure. (SSL_ERROR_SSL):error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version numberChannel open, login Failed! 提示的错误没找到任何相关文档，尝试用tcpdump抓包看看交互情况：1215:50:39.932746 IP 192.168.168.120.21 &gt; 192.168.168.121.39398: Flags [P.], seq 1806:1816, ack 772, win 243, options [nop,nop,TS val 513318660 ecr 1441172002], length 10: FTP: 500 OOPS: [!ftp]15:50:39.932769 IP 192.168.168.120.21 &gt; 192.168.168.121.39398: Flags [P.], seq 1816:1851, ack 772, win 243, options [nop,nop,TS val 513318660 ecr 1441172002], length 35: FTP: cannot change directory:/var/ftp/f1[!ftp] 提示f1的目录不存在，尝试创建f1的目录，并修改权限：12mkdir -p /var/ftp/f1chown ftp1.ftp1 /var/ftp/f1 再次登录，正常12345678910111213141516171819[root@localhost surgeftp_2.3f2_linux64]# ./sslftp 192.168.168.120Connected to 192.168.168.120220 (vsFTPd 3.0.2)234 Proceed with negotiation.starting SSL/TLSsslinit 3Negotiated secure protocol TLSv1.2, using an AES cipher.200 PBSZ set to 0.200 PROT now Private.(secure) User: f1331 Please specify the password.(secure) Password: ******230 Login successful.Type in &quot;save&quot; to save login details to /root/.netrcsslftp&gt; ls226 Directory send OK.sslftp&gt; exit221 Goodbye.Channel Closed. 10、新增f2用户，修改vsftpd_users.conf配置文件，添加f2用户123456[root@localhost ~]# vim /etc/vsftpd/vsftpd_users.conf#输入奇数行为账号，偶数行为密码f1123456f2123456 11、重新生成认证文件1[root@localhost ~]# db_load -T -t hash -f /etc/vsftpd/vsftpd_users.conf /etc/vsftpd/vsftpd_users.db 12、将f2用户加入/etc/vsftpd/chroot_list，/etc/vsftpd/vsftpd.user_list允许f2用户登录123456[root@localhost vsftpd]# more /etc/vsftpd/chroot_list |grep -E &quot;f1|f2&quot;f1f2[root@localhost vsftpd]# more /etc/vsftpd/vsftpd.user_list |grep -E &quot;f1|f2&quot;f1f2 13、添加f2配置文件vim /etc/vsftpd/vuser_conf/f21234567local_root=/var/ftp/f2write_enable=YESanon_umask=022anon_world_readable_only=NOanon_upload_enable=YESanon_mkdir_write_enable=YESanon_other_write_enable=YES 14、创建f2目录，并修改权限12mkdir -p /var/ftp/f2chown ftp1.ftp1 /var/ftp/f2 15、重新启动vsftpd1[root@localhost ~]# systemctl vsftpd restart 16、测试登录vsftpd是否正常123456789101112131415[root@localhost surgeftp_2.3f2_linux64]# ./sslftp 192.168.168.120Connected to 192.168.168.120220 (vsFTPd 3.0.2)234 Proceed with negotiation.starting SSL/TLSsslinit 3Negotiated secure protocol TLSv1.2, using an AES cipher.200 PBSZ set to 0.200 PROT now Private.(secure) User: f2 331 Please specify the password.(secure) Password: ******230 Login successful.Type in &quot;save&quot; to save login details to /root/.netrcsslftp&gt;]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Linux] DNS服务部署(单点&主从)]]></title>
    <url>%2F2018%2F11%2F22%2FDNS%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[1、关闭防火墙开启相应的服务12[root@localhost ~]#setenforce 0[root@localhost ~]#iptables -F 2、主节点安装dns1yum -y install bind bind-chroot bind-utils 3、修改配置文件3.1、/etc/named.conf—–主配置文件,服务器主要运行参数修改dns主配置文件：123456789options &#123; listen-on port 53 &#123; 10.0.30.95;127.0.0.1 &#125;; listen-on-v6 port 53 &#123; ::1; &#125;; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; allow-query &#123; any; &#125;; forwarders &#123; 8.8.8.8; &#125;; 3.2、/etc/named.rfc1912.zones—–区域文件，主要指定要解析哪个域名vim /etc/named.rfc1912.zones123456789101112// 首先对文件中正向解析的区域进行修改zone &quot;node&quot; IN &#123; type master; file &quot;node.conf&quot;; allow-update &#123; none; &#125;;&#125;;// 设置好域名格式以及相应的数据文件接下来进行反向解析区域的配置zone &quot;30.0.10.in -addr.arpa&quot; IN&#123; type master; file &quot;node.txt&quot;; allow-update &#123; none; &#125;; &#125; 3.3、配置完成之后校验一下文件配置是否正确：123[root@node4 named]# named-checkconf[root@node4 named]# 没有任何输出表示 /etc/named.conf没有语法错误 3.4、/var/named/xxx.xx ——-数据文件，用来正向和反向的解析主文件及区域文件修改完成后接下来进行数据文件的修改,切换到/var/named目录下,对数据文件进行相应的选项配置：12cd /var/named/cp named.empty node.conf 首先对其正向解析的数据文件进行配置vim node.com1234567891011$TTL 3H@ IN SOA gfs.com rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns.gfs.com.dns.gfs.com A 10.0.30.95node1.gfs.com 1 A 10.0.30.51 AAAA ::1 对反向解析的数据文件进行配置，反向解析数据文件里面只有SOA、NS、PTR资源记录，所有A记录都要改为PTR记录，名称为IP地址，IP地址可以写全也可以简写，如果写全则是IP地址反写加上.in-addr.arpa.例如：116.2.16.172.in-addr.arpa. PTR资源记录的值为域名。1cp named.empty node.txt vim node.txt12345678910$TTL 3H@ IN SOA gfs.com rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns.gfs.com.95 PTR dns.gfs.com.51 PTR node1.gfs.com. 3.5、解析文件参数详解1234567891011121314151617181920212223242526区域解析库文件第一个记录必须是SOA记录，必须有NS记录并且正解区域要有NS记录的A记录，反解则不需要有NS记录对应的A记录。$TTL表示宏定义，TTL(Time- To-Live)，dns记录在本地DNS服务器上保留的时间@符号可代表区域文件/etc/named.conf里面定义的区域名称，即：&quot;gfs.com.&quot;。2018030422 ;标识序列号，十进制数字，不能超过10位，通常使用日期2H ;刷新时间，即每隔多久到主服务器检查一次，此处为2小时，实验环境可以设置小一点4M ;重试时间，应该小于刷新时间，此处为4分钟，实验环境可以设置小一点1D ;过期时间，此处为1天2D ;主服务器挂后，从服务器至多工作的时间，此处为2天)这个文件里所有的域名结尾的点号一定不能省略。区域解析库文件是放在/var/named目录下，由named进程是以named用户运行，因此区域解析库文件的属组应设置为named。（1）A记录（Address）正向解析A记录是将一个主机名（全称域名FQDN）和一个IP地址关联起来。这也是大多数客户端程序默认的查询类型。（2）PTR记录（Pointer）反向解析PTR记录将一个IP地址对应到主机名（全称域名FQDN）。这些记录保存在in-addr.arpa域中。（3）CNAME记录(Canonical Name)别名别名记录，也称为规范名字(Canonical Name)。这种记录允许您将多个名字映射到同一台计算机。（4）MX记录（Mail eXchange）MX记录是邮件交换记录，它指向一个邮件服务器，用于电子邮件系统发邮件时根据 收信人的地址后缀来定位邮件服务器。MX记录也叫做邮件路由记录，用户可以将该域名下的邮件服务器指向到自己的mail server上，然后即可自行操控所有的邮箱设置。当有多个MX记录（即有多个邮件服务器）时，则需要设置数值来确定其优先级。通过设置优先级数字来指明首选服务器，数字越小表示优先级越高。（5）NS记录（Name Server）NS（Name Server）记录是域名服务器记录，也称为授权服务器，用来指定该域名由哪个DNS服务器来进行解析。将网站的NS记录指向到目标地址，在设置NS记录的同时还需要设置目标网站的指向，否则NS记录将无法正常解析NS记录优先于A记录。即，如果一个主机地址同时存在NS记录和A记录，则A记录不生效。（6）AAAA记录 IPV6解析记录该记录是将域名解析到一个指定的IPV6的IP上。 3.6、相应的数据配置文件完成后对数据文件的属主进行修改1234[root@localhost named]#chown named node.*[root@node4 named]# ll node.*-rw-r-----. 1 named root 286 Nov 22 16:12 node.conf-rw-r-----. 1 named root 310 Nov 22 16:12 node.txt 3.7、配置完成后检测解析文件是否正确[root@node4 named]# named-checkzone “gfs.com” node.confzone gfs.com/IN: loaded serial 0OK 3.8、检测反向解析库配置有没有错误123[root@node4 named]# named-checkzone &quot;30.0.10.in-addr.arpa&quot; node.txtzone 30.0.10.in-addr.arpa/IN: loaded serial 0OK 4、修改完成后启动dns服务1systemctl start named.service 5、在客户端配置dns信息5.1、修改 /etc/NetworkManager/NetworkManager.conf 文件，防止重启网卡后dns被重置，在main部分添加 “dns=none” 选项123[main]plugins=ifcfg-rhdns=none 5.2、NetworkManager重新装载上面修改的配置1# systemctl restart NetworkManager.service 5.3、之后修改/etc/resolv.conf配置文件：12345678910111213141516171819202122232425[root@node2 ~]# more /etc/resolv.conf# Generated by NetworkManagernameserver 10.0.30.95[root@node2 ~]# /etc/init.d/network restartRestarting network (via systemctl): [ OK ][root@node2 ~]# more /etc/resolv.conf# Generated by NetworkManagernameserver 202.106.0.20nameserver 8.8.8.8[root@node2 ~]# [root@node2 ~]# lsanaconda-ks.cfg Desktop Documents Downloads initial-setup-ks.cfg Music Pictures Public Templates Videos[root@node2 ~]# vim /etc/NetworkManager/NetworkManager.conf [root@node2 ~]# [root@node2 ~]# systemctl restart NetworkManager.service[root@node2 ~]# vim /etc/resolv.conf[root@node2 ~]# [root@node2 ~]# more /etc/resolv.conf# Generated by NetworkManagernameserver 10.0.30.95[root@node2 ~]# /etc/init.d/network restartRestarting network (via systemctl): [ OK ][root@node2 ~]# more /etc/resolv.conf# Generated by NetworkManagernameserver 10.0.30.95 5.4、测试dns是否能正常解析：正向解析：12345678910111213141516171819202122232425262728[root@node2 ~]# dig -t A node1.gfs.com; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7 &lt;&lt;&gt;&gt; -t A node1.gfs.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 30794;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 2, ADDITIONAL: 3;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;node1.gfs.com. IN A;; ANSWER SECTION:node1.gfs.com. 10800 IN A 10.0.30.51;; AUTHORITY SECTION:gfs.com. 10800 IN NS dns1.gfs.com.gfs.com. 10800 IN NS dns2.gfs.com.;; ADDITIONAL SECTION:dns1.gfs.com. 10800 IN A 10.0.30.95dns2.gfs.com. 10800 IN A 10.0.30.117;; Query time: 2 msec;; SERVER: 10.0.30.95#53(10.0.30.95);; WHEN: Thu Nov 22 17:44:35 CST 2018;; MSG SIZE rcvd: 128 反向解析：12345678910111213141516171819202122232425262728[root@node2 ~]# dig -x 10.0.30.51 ; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7 &lt;&lt;&gt;&gt; -x 10.0.30.51;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 50154;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 2, ADDITIONAL: 3;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;51.30.0.10.in-addr.arpa. IN PTR;; ANSWER SECTION:51.30.0.10.in-addr.arpa. 10800 IN PTR node1.gfs.com.;; AUTHORITY SECTION:30.0.10.in-addr.arpa. 10800 IN NS dns2.gfs.com.30.0.10.in-addr.arpa. 10800 IN NS dns1.gfs.com.;; ADDITIONAL SECTION:dns1.gfs.com. 10800 IN A 10.0.30.95dns2.gfs.com. 10800 IN A 10.0.30.117;; Query time: 2 msec;; SERVER: 10.0.30.95#53(10.0.30.95);; WHEN: Thu Nov 22 17:44:47 CST 2018;; MSG SIZE rcvd: 149 上面单节点的dns配置完成，下面配置dns主从，从节点实时同步主节点更新的数据。 6、主节点修改/etc/named.rfc1912.zonesallow-transfer { 10.0.30.117; }; #指定允许转发的目标主机，即从服务器also-notify:主动通知从域名服务器进行更新，在主域名服务器进行更新后，而不需要在等规定的时间后才通知从域名服务器进行更新12345678910111213zone &quot;gfs.com&quot; IN &#123; type master; file &quot;node.conf&quot;; allow-transfer &#123; 10.0.30.117; &#125;; also-notify &#123; 10.0.30.117; &#125;;&#125;;zone &quot;30.0.10.in-addr.arpa&quot; IN&#123; type master; file &quot;node.txt&quot;; also-notify &#123; 10.0.30.117; &#125;; allow-transfer &#123; 10.0.30.117; &#125;;&#125;; 7、修改主节点正向解析文件[root@node4 named]# more node.conf123456789101112131415$TTL 3H@ IN SOA gfs.com. rname.invalid. ( 2 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns1.gfs.com. NS dns2.gfs.com.dns1 A 10.0.30.95dns2 A 10.0.30.117node1 A 10.0.30.51node2 A 10.0.30.35node3 A 10.0.30.117 AAAA ::1 8、修改主节点反向解析文件[root@node4 named]# more node.txt1234567891011121314$TTL 3H@ IN SOA gfs.com. rname.invalid. ( 2 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns1.gfs.com. NS dns2.gfs.com.95 PTR dns1.gfs.com.117 PTR dns2.gfs.com.51 PTR node1.gfs.com.35 PTR node2.gfs.com.117 PTR node3.gfs.com. 9、重新启动dns服务器1systemctl restart named.service 10、在从服务器上安装dns服务1yum -y install bind bind-chroot bind-utils 11、修改从节点/etc/named.conf配置文件123456789options &#123; listen-on port 53 &#123; 10.0.30.117;127.0.0.1; &#125;; listen-on-v6 port 53 &#123; ::1; &#125;; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; allow-query &#123; any; &#125;; forwarders &#123; 8.8.8.8; &#125;; 12、修改/etc/named.rfc1912.zonesallow-notify 接受这个网段内的主机发送来的通知12345678910111213zone &quot;gfs.com&quot; IN &#123; type slave; masters &#123; 10.0.30.95; &#125;; allow-notify &#123; 10.0.30.95; &#125;; file &quot;slaves/node.conf&quot;;&#125;;zone &quot;30.0.10.in-addr.arpa&quot; IN&#123; type slave; masters &#123; 10.0.30.95; &#125;; allow-notify &#123; 10.0.30.95; &#125;; file &quot;slaves/node.txt&quot;;&#125;; 13、从服务器启动dns服务1systemctl start named.service 14、查看/var/named/slaves目录下已经多出了node.*的两个文件123456[root@node3 slaves]# pwd/var/named/slaves[root@node3 slaves]# lltotal 8-rw-r--r--. 1 named named 416 Nov 22 16:12 node.conf-rw-r--r--. 1 named named 514 Nov 22 16:12 node.txt 15、测试主从文件能否正常同步，修改主节点node.*配置文件123456[root@node4 named]# vim node.conf [root@node4 named]# vim node.txt [root@node4 named]# [root@node4 named]# ll node.*-rw-r-----. 1 named root 265 Nov 22 18:48 node.conf-rw-r-----. 1 named root 272 Nov 22 18:48 node.txt ***注意修改配置文件的时候一定要修改配置文件中serial那一行，只有该行的数值大于前一个值，才会同步到从节点。 16、主节点重新加载dns1systemctl reload named.service 17、查看从节点日志信息123456789101112Nov 22 18:48:55 node3 named[98442]: client 10.0.30.95#52505: received notify for zone &apos;gfs.com&apos;Nov 22 18:48:55 node3 named[98442]: zone gfs.com/IN: Transfer started.Nov 22 18:48:55 node3 named[98442]: transfer of &apos;gfs.com/IN&apos; from 10.0.30.95#53: connected using 10.0.30.117#58148Nov 22 18:48:55 node3 named[98442]: zone gfs.com/IN: transferred serial 3Nov 22 18:48:55 node3 named[98442]: transfer of &apos;gfs.com/IN&apos; from 10.0.30.95#53: Transfer completed: 1 messages, 9 records, 252 bytes, 0.008 secs (31500 bytes/sec)Nov 22 18:48:55 node3 named[98442]: zone gfs.com/IN: sending notifies (serial 3)Nov 22 18:48:55 node3 named[98442]: client 10.0.30.95#54023: received notify for zone &apos;30.0.10.in-addr.arpa&apos;Nov 22 18:48:55 node3 named[98442]: zone 30.0.10.in-addr.arpa/IN: Transfer started.Nov 22 18:48:55 node3 named[98442]: transfer of &apos;30.0.10.in-addr.arpa/IN&apos; from 10.0.30.95#53: connected using 10.0.30.117#51530Nov 22 18:48:55 node3 named[98442]: zone 30.0.10.in-addr.arpa/IN: transferred serial 3Nov 22 18:48:55 node3 named[98442]: transfer of &apos;30.0.10.in-addr.arpa/IN&apos; from 10.0.30.95#53: Transfer completed: 1 messages, 8 records, 249 bytes, 0.002 secs (124500 bytes/sec)Nov 22 18:48:55 node3 named[98442]: zone 30.0.10.in-addr.arpa/IN: sending notifies (serial 3) 18、查看从节点配置文件信息1234[root@node3 slaves]# lltotal 8-rw-r--r--. 1 named named 375 Nov 22 18:48 node.conf-rw-r--r--. 1 named named 433 Nov 22 18:48 node.txt 从节点文件已和主节点正常同步 19、客户端测试主节点dns服务器挂掉从节点能否正常使用1234[root@node2 ~]# more /etc/resolv.conf# Generated by NetworkManagernameserver 10.0.30.95nameserver 10.0.30.117 20、测试正向解析是否正常12345678910111213141516171819202122232425262728[root@node2 ~]# dig -t A node1.gfs.com; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7 &lt;&lt;&gt;&gt; -t A node1.gfs.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 55761;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 2, ADDITIONAL: 3;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;node1.gfs.com. IN A;; ANSWER SECTION:node1.gfs.com. 10800 IN A 10.0.30.51;; AUTHORITY SECTION:gfs.com. 10800 IN NS dns1.gfs.com.gfs.com. 10800 IN NS dns2.gfs.com.;; ADDITIONAL SECTION:dns1.gfs.com. 10800 IN A 10.0.30.95dns2.gfs.com. 10800 IN A 10.0.30.117;; Query time: 0 msec;; SERVER: 10.0.30.95#53(10.0.30.95);; WHEN: Thu Nov 22 18:54:03 CST 2018;; MSG SIZE rcvd: 128 21、主节点停掉dns服务：12345678910111213141516[root@node4 named]# systemctl stop named.service[root@node4 named]# systemctl status named.service● named.service - Berkeley Internet Name Domain (DNS) Loaded: loaded (/usr/lib/systemd/system/named.service; disabled; vendor preset: disabled) Active: inactive (dead)Nov 22 18:48:55 node4 named[10366]: zone 30.0.10.in-addr.arpa/IN: sending notifies (serial 3)Nov 22 18:48:55 node4 named[10366]: client 10.0.30.117#58148 (gfs.com): transfer of &apos;gfs.com/IN&apos;: AXFR-style IXFR startedNov 22 18:48:55 node4 named[10366]: client 10.0.30.117#58148 (gfs.com): transfer of &apos;gfs.com/IN&apos;: AXFR-style IXFR endedNov 22 18:48:55 node4 named[10366]: client 10.0.30.117#28664: received notify for zone &apos;gfs.com&apos;Nov 22 18:48:55 node4 named[10366]: client 10.0.30.117#51530 (30.0.10.in-addr.arpa): transfer of &apos;30.0.10.in-addr.arpa/IN&apos;: AXFR-style IXFR startedNov 22 18:48:55 node4 named[10366]: client 10.0.30.117#51530 (30.0.10.in-addr.arpa): transfer of &apos;30.0.10.in-addr.arpa/IN&apos;: AXFR-style IXFR endedNov 22 18:48:56 node4 named[10366]: client 10.0.30.117#41288: received notify for zone &apos;30.0.10.in-addr.arpa&apos;Nov 22 18:59:26 node4 systemd[1]: Stopping Berkeley Internet Name Domain (DNS)...Nov 22 18:59:26 node4 named[10366]: received control channel command &apos;stop&apos;Nov 22 18:59:26 node4 systemd[1]: Stopped Berkeley Internet Name Domain (DNS). 22、客户端解析仍然正常12345678910111213141516171819202122232425262728[root@node2 ~]# dig -t A node1.gfs.com; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7 &lt;&lt;&gt;&gt; -t A node1.gfs.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 39672;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 2, ADDITIONAL: 3;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;node1.gfs.com. IN A;; ANSWER SECTION:node1.gfs.com. 10800 IN A 10.0.30.51;; AUTHORITY SECTION:gfs.com. 10800 IN NS dns2.gfs.com.gfs.com. 10800 IN NS dns1.gfs.com.;; ADDITIONAL SECTION:dns1.gfs.com. 10800 IN A 10.0.30.95dns2.gfs.com. 10800 IN A 10.0.30.117;; Query time: 2 msec;; SERVER: 10.0.30.117#53(10.0.30.117);; WHEN: Thu Nov 22 18:59:48 CST 2018;; MSG SIZE rcvd: 128 23、指定dns为主节点，解析失败12345[root@node2 ~]# dig -t A node1.gfs.com @10.0.30.95; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7 &lt;&lt;&gt;&gt; -t A node1.gfs.com @10.0.30.95;; global options: +cmd;; connection timed out; no servers could be reached 24、指定为从节点，解析正常12345678910111213141516171819202122232425262728[root@node2 ~]# dig -t A node1.gfs.com @10.0.30.117; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7 &lt;&lt;&gt;&gt; -t A node1.gfs.com @10.0.30.117;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 53270;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 2, ADDITIONAL: 3;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;node1.gfs.com. IN A;; ANSWER SECTION:node1.gfs.com. 10800 IN A 10.0.30.51;; AUTHORITY SECTION:gfs.com. 10800 IN NS dns1.gfs.com.gfs.com. 10800 IN NS dns2.gfs.com.;; ADDITIONAL SECTION:dns1.gfs.com. 10800 IN A 10.0.30.95dns2.gfs.com. 10800 IN A 10.0.30.117;; Query time: 2 msec;; SERVER: 10.0.30.117#53(10.0.30.117);; WHEN: Thu Nov 22 19:01:30 CST 2018;; MSG SIZE rcvd: 128 25、把从节点dns服务也关掉1[root@node3 slaves]# systemctl stop named.service 26、解析域名报错12345[root@node2 ~]# dig -t A node1.gfs.com; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-61.el7 &lt;&lt;&gt;&gt; -t A node1.gfs.com;; global options: +cmd;; connection timed out; no servers could be reached 27、额外参考信息123DNS服务器监听端口:PORT: udp/tcp 53 ---&gt; 服务端口PORT: udp/tcp 953 ---&gt; 主从连接端口 1dns各参数详解，点击[此处](https://blog.csdn.net/bpingchang/article/details/38427113)跳转]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql-show table status语法]]></title>
    <url>%2F2018%2F11%2F14%2Fmysql-show%20table%20status%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1、使用方法：123show table status from aaaa like &apos;test_order&apos;;或show table status in aaaa like &apos;test_order&apos;; 结果显示如下：1234567891011121314151617181920*************************** 1. row *************************** Name: test_order Engine: InnoDB Version: 10 Row_format: Dynamic Rows: 84052104 Avg_row_length: 372 Data_length: 31321817088Max_data_length: 0 Index_length: 25528303616 Data_free: 7340032 Auto_increment: 86709693 Create_time: 2018-11-02 16:20:25 Update_time: NULL Check_time: NULL Collation: utf8mb4_general_ci Checksum: NULL Create_options: Comment: 1 row in set (0.00 sec) 2、具体参数详解：1Name:表名 1Engine:表使用的存储引擎 1Version：该列在8.0版本已经废弃，在5.7版本下显示硬编码10 1Row_format：行存储格式（Fixed, Dynamic, Compressed, Redundant, Compact）。对于myisam表，Dynamic值与 myisamchk -dvv查询出来的Packed值相对应。 1Rows:对于myisam存储引擎，显示实际行数。对于其他引擎，比如innodb，值是近似的(粗略估计），和实际值可能相差40%至50%之间，想要获取准确的值使用select count(*)。 1Avg_row_length：平均行长度。 1Data_length：对于Myisam表，该值以字节为单位显示数据文件的长度；对于Innodb,该值以字节为单位显示为聚簇索引分配内存的值（近似）。 1Max_data_length：对于Myisam表，该值显示数据文件的最大长度。这是可以存储在表中的数据的总字节数，该表给定使用的数据指针大小。对于Innodb，该值已弃用。 12Index_length：对于Myisam，该值为索引文件的长度（以字节为单位）。对于Innodb，该值显示内存为非聚簇索引分配的数量（以字节为单位），它是内存页中非聚簇索引大小的总和与Innodb内存页大小的乘积。 1234Data_free：已分配但未使用的字节数。Innodb-tables代表当前表所属表空间的可用空间。如果该表位于共享表空间，该值代表共享表空间的剩余空间。如果使用多个表空间，每个表拥有自己的表空间，该值仅代表当前表的剩余空间。剩余空间的意思就是当前完全空闲的区字节数减去一个安全的边界值。即使该值为0，只要新的区可以被分配插入数据也是正常的。对于NDB集群，该值显示磁盘分配的空间，但是还没有被磁盘数据表或者磁盘数据碎片使用的空间。对于分区表，该值只是估计值并不准确。想要获取更加准确的方法使用下面方法查询：SELECT SUM(DATA_FREE) FROM INFORMATION_SCHEMA.PARTITIONS WHERE TABLE_SCHEMA = &apos;mydb&apos; AND TABLE_NAME = &apos;mytable&apos;; 1Auto_increment：下一个自动增长值。 1Create_time：表创建时间。 12Update_time：数据文件最后更新时间。对于一些存储引擎，该是为null。例如，Innodb存储多个表在系统表空间而且数据文件时间戳没有应用。Innodb表是使用file-per-table模式，每个表都有他自己的.ibd文件，内存buffer写入数据文件存在延迟，因而文件修改时间与最后的插入、更新、删除修改时间是不同的。对于Myisam表，数据文件时间戳是可用的，但是在window系统上，更新并不会更新时间戳，因此该值是不准确的。对于非分区的Innodb表，该值显示最后update、insert、delete的时间戳的值。对于MVCC，时间戳的值代表commit的时间。如果服务器重启或者该表被数据字典缓存清除，时间戳不会保留。 1Check_time：表最后一次检查时间。 1Collation：表默认的排序规则。 1Checksum：校验值。 12Create_options：创建表时的额外选项。当执行create table时被保留的原始选项，这些选项与活动表的设置和选项不同。对于Innodb表，该值显示row_format和key_block_size选项。如果表示分区表，该值会显示partitioned。当创建或者修改一个file-per-table表空间为加密模式时，该值会显示为ENCRYPTIONG(general tablespace除外)。 1Comment：创建表空间添加的注释。 3、测试对innodb表添加encryption功能：3.1、安装插件1INSTALL PLUGIN keyring_file soname &apos;keyring_file.so&apos;; 3.2、创建密钥文件目录12mkdir -p /data/mysql/keyfilechown mysql.mysql -R /data/mysql/keyfile 3.3、设置key文件：123456789set global keyring_file_data=&apos;/data/mysql/keyfile/key01&apos;;show global variables like &apos;%keyring_file_data%&apos;;root@db 16:28: [aaaa]&gt; show global variables like &apos;%keyring_file_data%&apos;;+-------------------+---------------------------+| Variable_name | Value |+-------------------+---------------------------+| keyring_file_data | /data/mysql/keyfile/key01 |+-------------------+---------------------------+1 row in set (0.01 sec) 3.4、查看插件状态：123456789root@db 16:27: [aaaa]&gt; SELECT PLUGIN_NAME, PLUGIN_STATUS -&gt; FROM INFORMATION_SCHEMA.PLUGINS -&gt; WHERE PLUGIN_NAME LIKE &apos;keyring%&apos;;+--------------+---------------+| PLUGIN_NAME | PLUGIN_STATUS |+--------------+---------------+| keyring_file | ACTIVE |+--------------+---------------+1 row in set (0.01 sec) 3.5、表aa未加密时，查看表状态：1234567root@db 16:29: [aaaa]&gt; show table status from aaaa like &apos;aa&apos;;+------+--------+---------+------------+------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+-------------+------------+--------------------+----------+----------------+---------+| Name | Engine | Version | Row_format | Rows | Avg_row_length | Data_length | Max_data_length | Index_length | Data_free | Auto_increment | Create_time | Update_time | Check_time | Collation | Checksum | Create_options | Comment |+------+--------+---------+------------+------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+-------------+------------+--------------------+----------+----------------+---------+| aa | InnoDB | 10 | Dynamic | 12 | 1365 | 16384 | 0 | 0 | 0 | NULL | 2018-10-29 17:30:47 | NULL | NULL | utf8mb4_general_ci | NULL | | |+------+--------+---------+------------+------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+-------------+------------+--------------------+----------+----------------+---------+1 row in set (0.00 sec) 3.6、对表aa进行加密：123root@db 16:29: [aaaa]&gt; alter table aa encryption=&apos;Y&apos;;Query OK, 12 rows affected (5.13 sec)Records: 12 Duplicates: 0 Warnings: 0 3.7、再次查看表aa状态：1234567root@db 16:30: [aaaa]&gt; show table status from aaaa like &apos;aa&apos;;+------+--------+---------+------------+------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+---------------------+------------+--------------------+----------+----------------+---------+| Name | Engine | Version | Row_format | Rows | Avg_row_length | Data_length | Max_data_length | Index_length | Data_free | Auto_increment | Create_time | Update_time | Check_time | Collation | Checksum | Create_options | Comment |+------+--------+---------+------------+------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+---------------------+------------+--------------------+----------+----------------+---------+| aa | InnoDB | 10 | Dynamic | 12 | 1365 | 16384 | 0 | 0 | 0 | NULL | 2018-11-14 16:30:16 | 2018-11-14 16:30:14 | NULL | utf8mb4_general_ci | NULL | ENCRYPTION=&quot;Y&quot; | |+------+--------+---------+------------+------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+---------------------+------------+--------------------+----------+----------------+---------+1 row in set (0.01 sec) 4、下面为大佬提供的方便查询数据库下所有表所占行数（估计）和占用空间的存储过程，点此跳转至原文，具体内容如下：4.1、存储过程所在的aaaa数据库，可根据自己数据库情况自行修改123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051DELIMITER $$DROP PROCEDURE IF EXISTS `aaaa`.`sp_status` $$CREATE PROCEDURE `aaaa`.`sp_status`(dbname VARCHAR(50))BEGIN -- Obtaining tables and views( SELECT TABLE_NAME AS `Table Name`, ENGINE AS `Engine`, TABLE_ROWS AS `Rows`, CONCAT( (FORMAT((DATA_LENGTH + INDEX_LENGTH) / POWER(1024,2),2)) , &apos; Mb&apos;) AS `Size`, TABLE_COLLATION AS `Collation` FROM information_schema.TABLES WHERE TABLES.TABLE_SCHEMA = dbname AND TABLES.TABLE_TYPE = &apos;BASE TABLE&apos;)UNION( SELECT TABLE_NAME AS `Table Name`, &apos;[VIEW]&apos; AS `Engine`, &apos;-&apos; AS `Rows`, &apos;-&apos; `Size`, &apos;-&apos; AS `Collation` FROM information_schema.TABLES WHERE TABLES.TABLE_SCHEMA = dbname AND TABLES.TABLE_TYPE = &apos;VIEW&apos;)ORDER BY 1;-- Obtaining functions, procedures and triggers( SELECT ROUTINE_NAME AS `Routine Name`, ROUTINE_TYPE AS `Type`, &apos;&apos; AS `Comment` FROM information_schema.ROUTINES WHERE ROUTINE_SCHEMA = dbname ORDER BY ROUTINES.ROUTINE_TYPE, ROUTINES.ROUTINE_NAME)UNION( SELECT TRIGGER_NAME,&apos;TRIGGER&apos; AS `Type`, concat(&apos;On &apos;,EVENT_MANIPULATION,&apos;: &apos;,EVENT_OBJECT_TABLE) AS `Comment` FROM information_schema.TRIGGERS WHERE EVENT_OBJECT_SCHEMA = dbname)ORDER BY 2,1;END$$DELIMITER ; 4.2、执行存储过程：1call aaaa.sp_status(&apos;aaaa&apos;); 4.3、结果如下：12345678910root@db 16:58: [aaaa]&gt; call aaaa.sp_status(&apos;aaaa&apos;);+--------------+--------+----------+--------------+--------------------+| Table Name | Engine | Rows | Size | Collation |+--------------+--------+----------+--------------+--------------------+| aa | InnoDB | 12 | 0.02 Mb | utf8mb4_general_ci || sequence | InnoDB | 12292293 | 515.98 Mb | utf8mb4_general_ci || test | InnoDB | 0 | 0.02 Mb | utf8mb4_general_ci || test1 | InnoDB | 6 | 0.02 Mb | utf8mb4_general_ci || test_order | InnoDB | 84052104 | 54,216.50 Mb | utf8mb4_general_ci |+--------------+--------+----------+--------------+--------------------+]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Linux] vsftpd配置ssl]]></title>
    <url>%2F2018%2F11%2F12%2Fvsftpd%E9%85%8D%E7%BD%AEssl%2F</url>
    <content type="text"><![CDATA[0、如果还没有部署vsftp-server，请先跳转到此处1、查询vsftpd软件是否支持SSLldd /usr/sbin/vsftpd | grep ssl1libssl.so.10 =&gt; /lib64/libssl.so.10 (0x00007f6cfc4a6000) 2、生成vsftpd.pem 证书[root@localhost vsftpd]# openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout /etc/vsftpd/vsftpd.pem -out /etc/vsftpd/vsftpd.pem12345678910111213141516171819Generating a 1024 bit RSA private key..................................++++++.........++++++writing new private key to &apos;/etc/vsftpd/vsftpd.pem&apos;-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &apos;.&apos;, the field will be left blank.-----Country Name (2 letter code) [XX]:cnState or Province Name (full name) []:beijingLocality Name (eg, city) [Default City]:beijingOrganization Name (eg, company) [Default Company Ltd]:goopalOrganizational Unit Name (eg, section) []:goopalCommon Name (eg, your name or your server&apos;s hostname) []:zevenEmail Address []:test@goopal.com 查看生成vsftpd.pem是否成功ls -l /etc/vsftpd/|grep vsftpd.pem1-rw-r--r-- 1 root root 1982 11月 9 14:20 vsftpd.pem 3、修改主配置文件/etc/vsftpd/vsftpd.conf下面是ssl参数一些定义，根据自己需求去修改1234567891011121314151617ssl_enable=yes/no 是否启用 SSL,默认为noallow_anon_ssl=yes/no 是否允许匿名用户使用SSL,默认为norsa_cert_file=/path/to/file rsa证书的位置dsa_cert_file=/path/to/file dsa证书的位置force_local_logins_ssl=yes/no 非匿名用户登陆时是否加密,默认为yesforce_local_data_ssl=yes/no 非匿名用户传输数据时是否加密,默认为yesforce_anon_logins_ssl=yes/no 匿名用户登录时是否加密,默认为noforce_anon_data_ssl=yes/no 匿名用户数据传输时是否加密,默认为nossl_sslv2=yes/no 是否激活sslv2加密,默认nossl_sslv3=yes/no 是否激活sslv3加密,默认nossl_tlsv1=yes/no 是否激活tls v1加密,默认yesssl_ciphers=加密方法 默认是DES-CBC3-SHArequire_ssl_reuse=no 需要数据与控制流使用相同的ssl通道，程序是另起一个ssl通道，选no#使用ftps的情况下，主动模式是不能用的，必须使用别动模式，为考虑安全问题，可以指定被动端口范围：pasv_enable=YESpasv_min_port=20000pasv_max_port=20001 修改vsftpd的主配置文件vsftpd.conf添加如下内容1234567891011121314ssl_enable=YESallow_anon_ssl=YESforce_anon_logins_ssl=YESforce_anon_data_ssl=YESforce_local_logins_ssl=YESforce_local_data_ssl=YESssl_tlsv1=YESssl_sslv2=NOssl_sslv3=NOrsa_cert_file=/etc/vsftpd/vsftpd.pemrequire_ssl_reuse=nopasv_enable=YESpasv_min_port=20000pasv_max_port=20001 4、重启vsftpd服务1systemctl restart vsftpd 5、默认linux自带的ftp client不支持ssl协议，如果配置了ssl，使用ftp client连接会报错可以使用第三方的surgeftp测试，下载方式：1wget ftp://ftp.netwinsite.com/pub/surgeftp/surgeftp_23f2_linux64.tar.gz 解压之后，测试连接是否正常：也可以使用windows下的ftp软件测试，如FileZilla，配置方法如下： 6、配置过程终于到的问题：1234522 SSL connection failed; session reuse required: see require_ssl_reuse option in vsftpd.conf man page解决方法：查看vsftpd.conf配置文件是否配置了下面参数require_ssl_reuse=no]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql-mgr解散集群并重新加入集群]]></title>
    <url>%2F2018%2F11%2F09%2Fmysql-mgr%E8%A7%A3%E6%95%A3%E9%9B%86%E7%BE%A4%E5%B9%B6%E9%87%8D%E6%96%B0%E6%B7%BB%E5%8A%A0%2F</url>
    <content type="text"><![CDATA[1、查看集群状态123456789101112131415161718192021222324252627282930 MySQL dax-mysql-master:3306 JS &gt; cluster = dba.getCluster(&quot;prodCluster&quot;)&lt;Cluster:prodCluster&gt; MySQL dax-mysql-master:3306 JS &gt; cluster.status()&#123; &quot;clusterName&quot;: &quot;prodCluster&quot;, &quot;defaultReplicaSet&quot;: &#123; &quot;name&quot;: &quot;default&quot;, &quot;primary&quot;: &quot;dax-mysql-master:3306&quot;, &quot;ssl&quot;: &quot;DISABLED&quot;, &quot;status&quot;: &quot;OK_NO_TOLERANCE&quot;, &quot;statusText&quot;: &quot;Cluster is NOT tolerant to any failures.&quot;, &quot;topology&quot;: &#123; &quot;dax-mysql-master:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-master:3306&quot;, &quot;mode&quot;: &quot;R/W&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125;, &quot;dax-mysql-slave:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-slave:3306&quot;, &quot;mode&quot;: &quot;R/O&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125; &#125; &#125;, &quot;groupInformationSourceMember&quot;: &quot;mysql://repl@dax-mysql-master:3306&quot;&#125; 2、解散集群：1234567891011121314151617181920212223242526272829 MySQL dax-mysql-master:3306 JS &gt; cluster.dissolve()The cluster still has active ReplicaSets.Please use cluster.dissolve(&#123;force: true&#125;) to deactivate replicationand unregister the ReplicaSets from the cluster.The following replicasets are currently registered:&#123; &quot;clusterName&quot;: &quot;prodCluster&quot;, &quot;defaultReplicaSet&quot;: &#123; &quot;name&quot;: &quot;default&quot;, &quot;topology&quot;: [ &#123; &quot;address&quot;: &quot;dax-mysql-slave:3306&quot;, &quot;label&quot;: &quot;dax-mysql-slave:3306&quot;, &quot;role&quot;: &quot;HA&quot; &#125;, &#123; &quot;address&quot;: &quot;dax-mysql-master:3306&quot;, &quot;label&quot;: &quot;dax-mysql-master:3306&quot;, &quot;role&quot;: &quot;HA&quot; &#125; ] &#125;&#125; MySQL dax-mysql-master:3306 JS &gt; cluster.dissolve(&#123;force: true&#125;)WARNING: On instance &apos;dax-mysql-slave:3306&apos; configuration cannot be persisted since MySQL version 5.7.22 does not support the SET PERSIST command (MySQL version &gt;= 8.0.5 required). Please set the &apos;group_replication_start_on_boot&apos; variable to &apos;OFF&apos; in the server configuration file, otherwise it might rejoin the cluster upon restart.WARNING: On instance &apos;dax-mysql-master:3306&apos; configuration cannot be persisted since MySQL version 5.7.22 does not support the SET PERSIST command (MySQL version &gt;= 8.0.5 required). Please set the &apos;group_replication_start_on_boot&apos; variable to &apos;OFF&apos; in the server configuration file, otherwise it might rejoin the cluster upon restart.The cluster was successfully dissolved.Replication was disabled but user data was left intact. 3、查看集群信息已经被删掉了：12root@db 16:54: [mysql_innodb_cluster_metadata]&gt; select * from clusters;Empty set (0.00 sec) 4、重新建立集群：12345678910111213141516171819MySQL dax-mysql-master:3306 JS &gt; var cluster = dba.createCluster(&apos;prodCluster&apos;);A new InnoDB cluster will be created on instance &apos;repl@dax-mysql-master:3306&apos;.Validating instance at dax-mysql-master:3306...This instance reports its own address as dax-mysql-masterWARNING: The following tables do not have a Primary Key or equivalent column: aaaa.testGroup Replication requires tables to use InnoDB and have a PRIMARY KEY or PRIMARY KEY Equivalent (non-null unique key). Tables that do not follow these requirements will be readable but not updateable when used with Group Replication. If your applications make updates (INSERT, UPDATE or DELETE) to these tables, ensure they use the InnoDB storage engine and have a PRIMARY KEY or PRIMARY KEY Equivalent.Instance configuration is suitable.Creating InnoDB cluster &apos;prodCluster&apos; on &apos;repl@dax-mysql-master:3306&apos;...WARNING: On instance &apos;dax-mysql-master:3306&apos; membership change cannot be persisted since MySQL version 5.7.22 does not support the SET PERSIST command (MySQL version &gt;= 8.0.5 required). Please use the &lt;Dba&gt;.configureLocalInstance command locally to persist the changes.Adding Seed Instance...Cluster successfully created. Use Cluster.addInstance() to add MySQL instances.At least 3 instances are needed for the cluster to be able to withstand up toone server failure. 5、 查看集群状态，现在只有一个节点信息123456789101112131415161718192021MySQL dax-mysql-master:3306 JS &gt; cluster.status();&#123; &quot;clusterName&quot;: &quot;prodCluster&quot;, &quot;defaultReplicaSet&quot;: &#123; &quot;name&quot;: &quot;default&quot;, &quot;primary&quot;: &quot;dax-mysql-master:3306&quot;, &quot;ssl&quot;: &quot;DISABLED&quot;, &quot;status&quot;: &quot;OK_NO_TOLERANCE&quot;, &quot;statusText&quot;: &quot;Cluster is NOT tolerant to any failures.&quot;, &quot;topology&quot;: &#123; &quot;dax-mysql-master:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-master:3306&quot;, &quot;mode&quot;: &quot;R/W&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125; &#125; &#125;, &quot;groupInformationSourceMember&quot;: &quot;mysql://repl@dax-mysql-master:3306&quot;&#125; 6、添加新的节点：123456789101112131415161718192021cluster.addInstance(&apos;dax-mysql-slave:3306&apos;) MySQL dax-mysql-master:3306 JS &gt; cluster.addInstance(&apos;dax-mysql-slave:3306&apos;)A new instance will be added to the InnoDB cluster. Depending on the amount ofdata on the cluster this might take from a few seconds to several hours.Please provide the password for &apos;root@dax-mysql-slave:3306&apos;: ********Adding instance to the cluster ...Validating instance at dax-mysql-slave:3306...This instance reports its own address as dax-mysql-slaveWARNING: The following tables do not have a Primary Key or equivalent column: aaaa.testGroup Replication requires tables to use InnoDB and have a PRIMARY KEY or PRIMARY KEY Equivalent (non-null unique key). Tables that do not follow these requirements will be readable but not updateable when used with Group Replication. If your applications make updates (INSERT, UPDATE or DELETE) to these tables, ensure they use the InnoDB storage engine and have a PRIMARY KEY or PRIMARY KEY Equivalent.Instance configuration is suitable.WARNING: On instance &apos;dax-mysql-slave:3306&apos; membership change cannot be persisted since MySQL version 5.7.22 does not support the SET PERSIST command (MySQL version &gt;= 8.0.5 required). Please use the &lt;Dba&gt;.configureLocalInstance command locally to persist the changes.WARNING: On instance &apos;dax-mysql-master:3306&apos; membership change cannot be persisted since MySQL version 5.7.22 does not support the SET PERSIST command (MySQL version &gt;= 8.0.5 required). Please use the &lt;Dba&gt;.configureLocalInstance command locally to persist the changes.The instance &apos;root@dax-mysql-slave:3306&apos; was successfully added to the cluster. 7、查看集群状态，可以看到两个节点了：123456789101112131415161718192021222324252627282930313233343536373839404142434445 MySQL dax-mysql-master:3306 JS &gt; cluster.status();&#123; &quot;clusterName&quot;: &quot;prodCluster&quot;, &quot;defaultReplicaSet&quot;: &#123; &quot;name&quot;: &quot;default&quot;, &quot;primary&quot;: &quot;dax-mysql-master:3306&quot;, &quot;ssl&quot;: &quot;DISABLED&quot;, &quot;status&quot;: &quot;OK_NO_TOLERANCE&quot;, &quot;statusText&quot;: &quot;Cluster is NOT tolerant to any failures.&quot;, &quot;topology&quot;: &#123; &quot;dax-mysql-master:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-master:3306&quot;, &quot;mode&quot;: &quot;R/W&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125;, &quot;dax-mysql-slave:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-slave:3306&quot;, &quot;mode&quot;: &quot;R/O&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125; &#125; &#125;, &quot;groupInformationSourceMember&quot;: &quot;mysql://repl@dax-mysql-master:3306&quot;&#125;root@db 17:14: [mysql_innodb_cluster_metadata]&gt; SELECT * FROM performance_schema.replication_group_members;+---------------------------+--------------------------------------+------------------+-------------+--------------+| CHANNEL_NAME | MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE |+---------------------------+--------------------------------------+------------------+-------------+--------------+| group_replication_applier | bddd9c32-8fee-11e8-ac79-525400edbe8d | dax-mysql-slave | 3306 | ONLINE || group_replication_applier | d5bd8edd-9a1d-11e8-993e-525400578639 | dax-mysql-master | 3306 | ONLINE |+---------------------------+--------------------------------------+------------------+-------------+--------------+2 rows in set (0.00 sec)root@db 17:14: [mysql_innodb_cluster_metadata]&gt; show status like &apos;%primary%&apos;;+----------------------------------+--------------------------------------+| Variable_name | Value |+----------------------------------+--------------------------------------+| group_replication_primary_member | d5bd8edd-9a1d-11e8-993e-525400578639 |+----------------------------------+--------------------------------------+1 row in set (0.01 sec)]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql-mgr单主模式切换为多主，多主切换为单主]]></title>
    <url>%2F2018%2F11%2F09%2Fmysql-mgr%E5%8D%95%E4%B8%BB%E6%A8%A1%E5%BC%8F%E5%88%87%E6%8D%A2%E4%B8%BA%E5%A4%9A%E4%B8%BB%2F</url>
    <content type="text"><![CDATA[1、停止组复制(所有节点执行)：123mysql&gt; stop group_replication; mysql&gt; set global group_replication_single_primary_mode=OFF; mysql&gt; set global group_replication_enforce_update_everywhere_checks=ON; 2、随便选择某个节点执行123mysql&gt; SET GLOBAL group_replication_bootstrap_group=ON; mysql&gt; START GROUP_REPLICATION; mysql&gt; SET GLOBAL group_replication_bootstrap_group=OFF; 3、其他节点执行1mysql&gt; START GROUP_REPLICATION; 查看节点启动报错：122018-11-08T14:00:52.098660Z 56 [ERROR] Plugin group_replication reported: &apos;There was an error when connecting to the donor server. Please check that group_replication_recovery channel credentials and all MEMBER_HOST column values of performance_schema.replication_group_members table are correct and DNS resolvable.&apos;2018-11-08T14:00:52.098689Z 56 [ERROR] Plugin group_replication reported: &apos;For details please check performance_schema.replication_connection_status table and error log messages of Slave I/O for channel group_replication_recovery.&apos; 手动配置复制信息：1change master to master_user=&apos;repl&apos;,master_password=&apos;12345678&apos; for channel &apos;group_replication_recovery&apos;; 再次启动：1mysql&gt; START GROUP_REPLICATION; 4、查看组信息，所有节点的信息正常，所有节点均为主：1234567SELECT * FROM performance_schema.replication_group_members;+---------------------------+--------------------------------------+------------------+-------------+--------------+| CHANNEL_NAME | MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE |+---------------------------+--------------------------------------+------------------+-------------+--------------+| group_replication_applier | bddd9c32-8fee-11e8-ac79-525400edbe8d | dax-mysql-slave | 3306 | ONLINE || group_replication_applier | d5bd8edd-9a1d-11e8-993e-525400578639 | dax-mysql-master | 3306 | ONLINE |+---------------------------+--------------------------------------+------------------+-------------+--------------+ 5、查看集群状态12345 MySQL dax-mysql-master:3306 JS &gt; cluster = dba.getCluster(&quot;prodCluster&quot;)&lt;Cluster:prodCluster&gt; MySQL dax-mysql-master:3306 JS &gt; cluster.status();Cluster.status: The InnoDB Cluster topology type (Single-Master) does not match the current Group Replication configuration (Multi-Master). Please use &lt;cluster&gt;.rescan() or change the Group Replication configuration accordingly. (RuntimeError) 提示cluster 拓扑类型为单主与当前的mgr(多主)不匹配，使用cluster.rescan()参数重新扫描：1234567891011 MySQL dax-mysql-master:3306 JS &gt; cluster.rescan()Rescanning the cluster...Result of the rescanning operation:&#123; &quot;defaultReplicaSet&quot;: &#123; &quot;name&quot;: &quot;default&quot;, &quot;newlyDiscoveredInstances&quot;: [], &quot;unavailableInstances&quot;: [] &#125;&#125; 再次查看集群状态仍然报错： 12 MySQL dax-mysql-master:3306 JS &gt; cluster.status();Cluster.status: The InnoDB Cluster topology type (Single-Master) does not match the current Group Replication configuration (Multi-Master). Please use &lt;cluster&gt;.rescan() or change the Group Replication configuration accordingly. (RuntimeError) 尝试删除集群元数据：123dba.dropMetadataSchema();Are you sure you want to remove the Metadata? [y/N]: yMetadata Schema successfully removed. 重新创建集群1var cluster = dba.createCluster(&apos;prodCluster&apos;, &#123;adoptFromGR: true ,force: true&#125;); 报错提示：1[ERROR] Plugin group_replication reported: &apos;Table instances has a foreign key with &apos;CASCADE&apos; clause. This is not compatible with Group Replication&apos; 创建集群会在mysql_innodb_cluster_metadata库下建立一个instances的表，在单主模式下创建集群没有外键级联的报错问题，在多主模式下提示有外键级联的问题，表内容如下：1234567891011121314151617181920CREATE TABLE `instances` ( `instance_id` int(10) unsigned NOT NULL AUTO_INCREMENT, `host_id` int(10) unsigned NOT NULL, `replicaset_id` int(10) unsigned DEFAULT NULL, `mysql_server_uuid` varchar(40) NOT NULL, `instance_name` varchar(256) NOT NULL, `role` enum(&apos;HA&apos;,&apos;readScaleOut&apos;) NOT NULL, `weight` float DEFAULT NULL, `addresses` json NOT NULL, `attributes` json DEFAULT NULL, `version_token` int(10) unsigned DEFAULT NULL, `description` text, PRIMARY KEY (`instance_id`), UNIQUE KEY `mysql_server_uuid` (`mysql_server_uuid`), UNIQUE KEY `instance_name` (`instance_name`), KEY `host_id` (`host_id`), KEY `instances_ibfk_2` (`replicaset_id`), CONSTRAINT `instances_ibfk_1` FOREIGN KEY (`host_id`) REFERENCES `hosts` (`host_id`), CONSTRAINT `instances_ibfk_2` FOREIGN KEY (`replicaset_id`) REFERENCES `replicasets` (`replicaset_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 查阅官方文档有提示说多主模式下存在这个bug，将该表下的外键约束去掉：12ALTER TABLE `mysql_innodb_cluster_metadata`.`instances` DROP FOREIGN KEY `instances_ibfk_1`;ALTER TABLE `mysql_innodb_cluster_metadata`.`instances` DROP FOREIGN KEY `instances_ibfk_2`; 重建添加外键使用下面的命令：123ALTER TABLE `mysql_innodb_cluster_metadata`.`instances` ADD CONSTRAINT `instances_ibfk_1` FOREIGN KEY (`host_id`) REFERENCES `hosts` (`host_id`);ALTER TABLE `mysql_innodb_cluster_metadata`.`instances` ADD CONSTRAINT `instances_ibfk_2` FOREIGN KEY (`replicaset_id`) REFERENCES `replicasets` (`replicaset_id`); 将外键约束删除之后，重新创建集群：12345678 MySQL dax-mysql-master:3306 JS &gt; var cluster = dba.createCluster(&apos;prodCluster&apos;, &#123;adoptFromGR: true ,force: true&#125;);A new InnoDB cluster will be created based on the existing replication group on instance &apos;repl@dax-mysql-master:3306&apos;.Creating InnoDB cluster &apos;prodCluster&apos; on &apos;repl@dax-mysql-master:3306&apos;...Adding Instance &apos;dax-mysql-slave:3306&apos;...Adding Instance &apos;dax-mysql-master:3306&apos;...Cluster successfully created based on existing replication group. 集群创建成功，查看集群状态,两个节点均为读写模式：123456789101112131415161718192021222324252627 MySQL dax-mysql-master:3306 JS &gt; cluster.status()&#123; &quot;clusterName&quot;: &quot;prodCluster&quot;, &quot;defaultReplicaSet&quot;: &#123; &quot;name&quot;: &quot;default&quot;, &quot;ssl&quot;: &quot;DISABLED&quot;, &quot;status&quot;: &quot;OK_NO_TOLERANCE&quot;, &quot;statusText&quot;: &quot;Cluster is NOT tolerant to any failures.&quot;, &quot;topology&quot;: &#123; &quot;dax-mysql-master:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-master:3306&quot;, &quot;mode&quot;: &quot;R/W&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125;, &quot;dax-mysql-slave:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-slave:3306&quot;, &quot;mode&quot;: &quot;R/W&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125; &#125; &#125;, &quot;groupInformationSourceMember&quot;: &quot;mysql://repl@dax-mysql-master:3306&quot;&#125; 6、mysql-mgr多主模式切换成功，现在将模式从多主切换为单主：7、所有节点执行123mysql&gt; stop group_replication; mysql&gt; set global group_replication_enforce_update_everywhere_checks=OFF; mysql&gt; set global group_replication_single_primary_mode=ON; 8、主节点（dax-mysql-master）执行123SET GLOBAL group_replication_bootstrap_group=ON; START GROUP_REPLICATION; SET GLOBAL group_replication_bootstrap_group=OFF; 9、从节点执行：1START GROUP_REPLICATION; 10、查看MGR组信息,所有节点同步正常：1234567mysql&gt; SELECT * FROM performance_schema.replication_group_members;+---------------------------+--------------------------------------+------------------+-------------+--------------+| CHANNEL_NAME | MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE |+---------------------------+--------------------------------------+------------------+-------------+--------------+| group_replication_applier | bddd9c32-8fee-11e8-ac79-525400edbe8d | dax-mysql-slave | 3306 | ONLINE || group_replication_applier | d5bd8edd-9a1d-11e8-993e-525400578639 | dax-mysql-master | 3306 | ONLINE |+---------------------------+--------------------------------------+------------------+-------------+--------------+ 查看当前主节点信息：1234567show status like &apos;%primary%&apos;;+----------------------------------+--------------------------------------+| Variable_name | Value |+----------------------------------+--------------------------------------+| group_replication_primary_member | d5bd8edd-9a1d-11e8-993e-525400578639 |+----------------------------------+--------------------------------------+1 row in set (0.01 sec) 11、登录mysql-shell查看集群信息：1234567891011121314151617181920MySQL dax-mysql-master:3306 JS &gt; cluster = dba.getCluster(&quot;prodCluster&quot;)&lt;Cluster:prodCluster&gt; MySQL dax-mysql-master:3306 JS &gt; cluster.status();Cluster.status: The InnoDB Cluster topology type (Multi-Master) does not match the current Group Replication configuration (Single-Master). Please use &lt;cluster&gt;.rescan() or change the Group Replication configuration accordingly. (RuntimeError) MySQL dax-mysql-master:3306 JS &gt; cluster.rescan()Rescanning the cluster...Result of the rescanning operation:&#123; &quot;defaultReplicaSet&quot;: &#123; &quot;name&quot;: &quot;default&quot;, &quot;newlyDiscoveredInstances&quot;: [], &quot;unavailableInstances&quot;: [] &#125;&#125; MySQL dax-mysql-master:3306 JS &gt; cluster.status();Cluster.status: The InnoDB Cluster topology type (Multi-Master) does not match the current Group Replication configuration (Single-Master). Please use &lt;cluster&gt;.rescan() or change the Group Replication configuration accordingly. (RuntimeError) 跟多主切换为单主报错相同，尝试删除集群元数据：123dba.dropMetadataSchema();Are you sure you want to remove the Metadata? [y/N]: yMetadata Schema successfully removed. 重新创建集群：12345678 MySQL dax-mysql-master:3306 JS &gt; var cluster = dba.createCluster(&apos;prodCluster&apos;, &#123;adoptFromGR: true&#125;);A new InnoDB cluster will be created based on the existing replication group on instance &apos;repl@dax-mysql-master:3306&apos;.Creating InnoDB cluster &apos;prodCluster&apos; on &apos;repl@dax-mysql-master:3306&apos;...Adding Seed Instance...Adding Instance &apos;dax-mysql-slave:3306&apos;...Cluster successfully created based on existing replication group. 集群创建成功，查看集群状态为一主一从模式：12345678910111213141516171819202122232425262728 MySQL dax-mysql-master:3306 JS &gt; cluster.status();&#123; &quot;clusterName&quot;: &quot;prodCluster&quot;, &quot;defaultReplicaSet&quot;: &#123; &quot;name&quot;: &quot;default&quot;, &quot;primary&quot;: &quot;dax-mysql-master:3306&quot;, &quot;ssl&quot;: &quot;DISABLED&quot;, &quot;status&quot;: &quot;OK_NO_TOLERANCE&quot;, &quot;statusText&quot;: &quot;Cluster is NOT tolerant to any failures.&quot;, &quot;topology&quot;: &#123; &quot;dax-mysql-master:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-master:3306&quot;, &quot;mode&quot;: &quot;R/W&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125;, &quot;dax-mysql-slave:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-slave:3306&quot;, &quot;mode&quot;: &quot;R/O&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125; &#125; &#125;, &quot;groupInformationSourceMember&quot;: &quot;mysql://repl@dax-mysql-master:3306&quot;&#125;]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql-mgr（双节点)依次修改server-id]]></title>
    <url>%2F2018%2F11%2F09%2Fmysql-mgr%EF%BC%88%E5%8F%8C%E8%8A%82%E7%82%B9%E4%BE%9D%E6%AC%A1%E4%BF%AE%E6%94%B9server-id)%2F</url>
    <content type="text"><![CDATA[1、查看从节点server-id信息：123456789101112root@db 16:55: [(none)]&gt; show variables like &quot;%server%&quot;;+---------------------------------------------------+--------------------------------------+| Variable_name | Value |+---------------------------------------------------+--------------------------------------+| character_set_server | utf8mb4 || collation_server | utf8mb4_general_ci || group_replication_recovery_ssl_verify_server_cert | OFF || innodb_ft_server_stopword_table | || server_id | 3306102 || server_id_bits | 32 || server_uuid | bddd9c32-8fee-11e8-ac79-525400edbe8d |+---------------------------------------------------+--------------------------------------+ 2、登录mysql-shell查看集群信息：1234567891011121314151617181920212223242526272829MySQL dax-mysql-master:3306 JS &gt; cluster = dba.getCluster(&quot;prodCluster&quot;)MySQL dax-mysql-master:3306 JS &gt; cluster.status();&#123; &quot;clusterName&quot;: &quot;prodCluster&quot;, &quot;defaultReplicaSet&quot;: &#123; &quot;name&quot;: &quot;default&quot;, &quot;primary&quot;: &quot;dax-mysql-master:3306&quot;, &quot;ssl&quot;: &quot;DISABLED&quot;, &quot;status&quot;: &quot;OK_NO_TOLERANCE&quot;, &quot;statusText&quot;: &quot;Cluster is NOT tolerant to any failures.&quot;, &quot;topology&quot;: &#123; &quot;dax-mysql-master:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-master:3306&quot;, &quot;mode&quot;: &quot;R/W&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125;, &quot;dax-mysql-slave:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-slave:3306&quot;, &quot;mode&quot;: &quot;R/O&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125; &#125; &#125;, &quot;groupInformationSourceMember&quot;: &quot;mysql://repl@dax-mysql-master:3306&quot;&#125; 将从节点移除集群：MySQL dax-mysql-master:3306 JS &gt; cluster.removeInstance(‘dax-mysql-slave:3306’)123456789The instance will be removed from the InnoDB cluster. Depending on the instance being the Seed or not, the Metadata session might become invalid. If so, please start a new session to the Metadata Storage R/W instance.WARNING: On instance &apos;dax-mysql-master:3306&apos; membership change cannot be persisted since MySQL version 5.7.22 does not support the SET PERSIST command (MySQL version &gt;= 8.0.5 required). Please use the &lt;Dba&gt;.configureLocalInstance command locally to persist the changes.WARNING: On instance &apos;dax-mysql-slave:3306&apos; configuration cannot be persisted since MySQL version 5.7.22 does not support the SET PERSIST command (MySQL version &gt;= 8.0.5 required). Please set the &apos;group_replication_start_on_boot&apos; variable to &apos;OFF&apos; in the server configuration file, otherwise it might rejoin the cluster upon restart.The instance &apos;dax-mysql-slave:3306&apos; was successfully removed from the cluster.WARNING: The &apos;group_replication_start_on_boot&apos; variable must be set to &apos;OFF&apos; in the server configuration file, otherwise it might silently rejoin the cluster upon restart. 重新扫描集群信息：1234567891011 MySQL dax-mysql-master:3306 JS &gt; cluster.rescan();Rescanning the cluster...Result of the rescanning operation:&#123; &quot;defaultReplicaSet&quot;: &#123; &quot;name&quot;: &quot;default&quot;, &quot;newlyDiscoveredInstances&quot;: [], &quot;unavailableInstances&quot;: [] &#125;&#125; 查看集群信息：123456789101112131415161718192021 MySQL dax-mysql-master:3306 JS &gt; cluster.status();&#123; &quot;clusterName&quot;: &quot;prodCluster&quot;, &quot;defaultReplicaSet&quot;: &#123; &quot;name&quot;: &quot;default&quot;, &quot;primary&quot;: &quot;dax-mysql-master:3306&quot;, &quot;ssl&quot;: &quot;DISABLED&quot;, &quot;status&quot;: &quot;OK_NO_TOLERANCE&quot;, &quot;statusText&quot;: &quot;Cluster is NOT tolerant to any failures.&quot;, &quot;topology&quot;: &#123; &quot;dax-mysql-master:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-master:3306&quot;, &quot;mode&quot;: &quot;R/W&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125; &#125; &#125;, &quot;groupInformationSourceMember&quot;: &quot;mysql://repl@dax-mysql-master:3306&quot;&#125; 3、从节点修改server-id，并重启库，之后将从节点加入集群：12345678910111213141516171819mysql-shell &gt; cluster.addInstance(&apos;dax-mysql-slave:3306&apos;)A new instance will be added to the InnoDB cluster. Depending on the amount ofdata on the cluster this might take from a few seconds to several hours.Please provide the password for &apos;root@dax-mysql-slave:3306&apos;: ********Adding instance to the cluster ...Validating instance at dax-mysql-slave:3306...This instance reports its own address as dax-mysql-slaveWARNING: The following tables do not have a Primary Key or equivalent column: aaaa.testGroup Replication requires tables to use InnoDB and have a PRIMARY KEY or PRIMARY KEY Equivalent (non-null unique key). Tables that do not follow these requirements will be readable but not updateable when used with Group Replication. If your applications make updates (INSERT, UPDATE or DELETE) to these tables, ensure they use the InnoDB storage engine and have a PRIMARY KEY or PRIMARY KEY Equivalent.Instance configuration is suitable.WARNING: On instance &apos;dax-mysql-slave:3306&apos; membership change cannot be persisted since MySQL version 5.7.22 does not support the SET PERSIST command (MySQL version &gt;= 8.0.5 required). Please use the &lt;Dba&gt;.configureLocalInstance command locally to persist the changes.WARNING: On instance &apos;dax-mysql-master:3306&apos; membership change cannot be persisted since MySQL version 5.7.22 does not support the SET PERSIST command (MySQL version &gt;= 8.0.5 required). Please use the &lt;Dba&gt;.configureLocalInstance command locally to persist the changes.The instance &apos;root@dax-mysql-slave:3306&apos; was successfully added to the cluster. 查看集群信息：12345678910111213141516171819202122232425262728 MySQL dax-mysql-master:3306 JS &gt; cluster.status();&#123; &quot;clusterName&quot;: &quot;prodCluster&quot;, &quot;defaultReplicaSet&quot;: &#123; &quot;name&quot;: &quot;default&quot;, &quot;primary&quot;: &quot;dax-mysql-master:3306&quot;, &quot;ssl&quot;: &quot;DISABLED&quot;, &quot;status&quot;: &quot;OK_NO_TOLERANCE&quot;, &quot;statusText&quot;: &quot;Cluster is NOT tolerant to any failures.&quot;, &quot;topology&quot;: &#123; &quot;dax-mysql-master:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-master:3306&quot;, &quot;mode&quot;: &quot;R/W&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125;, &quot;dax-mysql-slave:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-slave:3306&quot;, &quot;mode&quot;: &quot;R/O&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125; &#125; &#125;, &quot;groupInformationSourceMember&quot;: &quot;mysql://repl@dax-mysql-master:3306&quot;&#125; 其他节点修改方法如上。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Oracle] oracle本地表空间在uniform、system等不同模式下分配extent方式]]></title>
    <url>%2F2018%2F11%2F07%2Foracle%E6%9C%AC%E5%9C%B0%E8%A1%A8%E7%A9%BA%E9%97%B4%E5%9C%A8uniform%E3%80%81system%E7%AD%89%E4%B8%8D%E5%90%8C%E6%A8%A1%E5%BC%8F%E4%B8%8B%E5%88%86%E9%85%8Dextent%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1、表空间和表都使用oracle默认参数查看scott用户默认表空间：1234select username,default_tablespace from dba_users where username=&apos;SCOTT&apos;;USERNAME DEFAULT_TABLESPACE------------------------------ ------------------------------SCOTT USERS 查看users表空间初始化extent大小：12345select TABLESPACE_NAME,BLOCK_SIZE,INITIAL_EXTENT,NEXT_EXTENT,MIN_EXTENTS,MAX_EXTENTS,EXTENT_MANAGEMENT,ALLOCATION_TYPE from user_tablespaces where tablespace_name=&apos;USERS&apos;;TABLESPACE_NAME BLOCK_SIZE INITIAL_EXTENT NEXT_EXTENT MIN_EXTENTS MAX_EXTENTS EXTENT_MAN ALLOCATIO------------------------------ ---------- -------------- ----------- ----------- ----------- ---------- ---------USERS 8192 65536 1 2147483645 LOCAL SYSTEM 查看当前数据库块大小：12345show parameter db_block_size;NAME TYPE VALUE------------------------------------ ----------- ------------------------------db_block_size integer 8192 创建测试表：1234511:43:49 SCOTT@ boston&gt; create table t1 as select * from emp where 0=1;Table created.表数据为空，查看是否有extent区在：11:43:54 SYS@ boston&gt; select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T1&apos;;no rows selected 向t1表内插入数据：1211:43:50 SCOTT@ boston&gt; insert into t1 select * from emp;14 rows created. 查看已分配出一个8个block的extent：12345611:43:55 SYS@ boston&gt; select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T1&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T1 0 4 176 8Elapsed: 00:00:00.50 因数据量太小，只分配了一个区，需插入大量数据：12345612:00:40 SCOTT@ boston&gt; insert into t1 select * from t1;42 rows created....12:00:57 SCOTT@ boston&gt; /2688 rows created.Elapsed: 00:00:00.02 再次查看extent，当分配16个extent之后，数据量达到了1688/1024=1M，当达到1m之后，在分配下一个extent会直接分配128个块，大小分为128*8/1024=1M：1234567812:01:01 SYS@ boston&gt; select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T1&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T1 0 4 176 8...T1 15 4 296 8T1 16 4 384 128 再次插入数据：12345612:28:36 SCOTT@ boston&gt; /5376 rows created.Elapsed: 00:00:00.02....12:30:11 SCOTT@ boston&gt; /688128 rows created. 查看extent情况：123456789101112:30:14 SYS@ boston&gt;select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T1&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T1 0 4 176 8....T1 15 4 296 8T1 16 4 384 128...T1 78 4 8320 128T1 79 4 8448 102480 rows selected. 当表数据量达到64M的时候，再次分配下一个extent会直接分配1024个block，大小为64M。 手动对表t1分配extent12345678913:35:52 SCOTT@ boston&gt; alter table t1 allocate extent;Table altered.Elapsed: 00:00:00.4914:18:45 SCOTT@ boston&gt; /Table altered.Elapsed: 00:00:00.0714:19:28 SCOTT@ boston&gt; /Table altered.Elapsed: 00:00:00.06 查看extent情况：12345678910111213141514:19:06 SYS@ boston&gt; select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T1&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T1 0 4 176 8...T1 15 4 296 8T1 16 4 384 128...T1 78 4 8320 128T1 79 4 8448 1024T1 80 4 9472 128T1 81 4 9600 128T1 82 4 9728 12883 rows selected.Elapsed: 00:00:00.45 使用alter table ** allocate extent手动分配的区大小为1M。 再次对表插入数据：12314:19:30 SCOTT@ boston&gt; insert into t1 select * from t1;1376256 rows created.Elapsed: 00:00:12.03 查看extent情况123456789101112131415161714:20:34 SYS@ boston&gt; select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T1&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T1 0 4 176 8...T1 15 4 296 8T1 16 4 384 128...T1 78 4 8320 128T1 79 4 8448 1024T1 80 4 9472 128T1 81 4 9600 128T1 82 4 9728 128T1 83 4 9856 1024...T1 88 4 14976 102489 rows selected. 插入数据分配的extent大小均为64M。 2、表空间空间使用默认参数，表参数自定义（INITIAL参数小于65536）：创建测试表：123456create table t2 as select * from emp where 0=1;Table created.ALTER TABLE t2 MOVE STORAGE ( INITIAL 20k NEXT 40k) TABLESPACE users;Table altered.Elapsed: 00:00:00.23 查看表参数：1234SELECT initial_extent, next_extent, min_extents, max_extents, pct_increase, blocks, sample_size FROM user_tables WHERE table_name = &apos;T2&apos;;INITIAL_EXTENT NEXT_EXTENT MIN_EXTENTS MAX_EXTENTS PCT_INCREASE BLOCKS SAMPLE_SIZE-------------- ----------- ----------- ----------- ------------ ---------- ----------- 24576 40960 表数据为空，查看是否有extent区在：12select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T2&apos;;no rows selected 向表空插入数据12insert into t2 select * from emp;insert into t2 select * from t2; 查看extent情况:1234select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T2&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T2 0 4 168 8 分配出来的extent最少8个块。多次插入数据查询：1234567891011121314select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T2&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T2 0 4 168 8...T2 15 4 17184 8T2 16 4 17280 128...T2 78 4 7936 128T2 79 4 8064 1024T2 80 4 9088 1024T2 81 4 10112 1024T2 82 4 11136 1024 extent分配增长情况为（64k-1m-64m-…)。 3、表空间空间使用默认参数，表参数自定义（INITIAL参数大于64k，next小于64k）：创建测试表：123456 create table t3 as select * from emp where 0=1;Table created.ALTER TABLE t3 MOVE STORAGE ( INITIAL 100k NEXT 40k) TABLESPACE users;Table altered.Elapsed: 00:00:00.23 查看表参数：12345SELECT initial_extent, next_extent, min_extents, max_extents, pct_increase, blocks, sample_size FROM user_tables WHERE table_name = &apos;T3&apos;;INITIAL_EXTENT NEXT_EXTENT MIN_EXTENTS MAX_EXTENTS PCT_INCREASE BLOCKS SAMPLE_SIZE-------------- ----------- ----------- ----------- ------------ ---------- ----------- 106496 40960 表数据为空，查看是否有extent区在：12select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T3&apos;;no rows selected 向表空插入数据1insert into t3 select * from emp; 一次分配出来两个extent，每个extent8个block：123456 select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T3&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T3 0 4 176 8T3 1 4 184 8 再次插入数据：1insert into t3 select * from t3; 不要插入太多数据，查看下一次分配的extent为1个，占8个块1234567select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T3&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T3 0 4 208 8T3 1 4 216 8T3 2 4 224 8 再次插入数据，extent增长情况（64k-1m-64m） 4、表空间空间使用默认参数，表参数自定义（INITIAL参数大于64k，next大于64k）：创建测试表：123456 create table t4 as select * from emp where 0=1;Table created.ALTER TABLE t4 MOVE STORAGE ( INITIAL 100k NEXT 140k) TABLESPACE users;Table altered.Elapsed: 00:00:00.23 查看表参数：1234514:31:43 SCOTT@ boston&gt; SELECT initial_extent, next_extent, min_extents, max_extents, pct_increase, blocks, sample_size FROM user_tables WHERE table_name = &apos;T4&apos;;INITIAL_EXTENT NEXT_EXTENT MIN_EXTENTS MAX_EXTENTS PCT_INCREASE BLOCKS SAMPLE_SIZE-------------- ----------- ----------- ----------- ------------ ---------- ----------- 106496 147456 表数据为空，查看是否有extent区在：12select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T4&apos;;no rows selected 向表空插入数据1insert into t4 select * from emp; 一次分配出来两个extent，每个extent8个block：123456select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T4&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T4 0 4 176 8T4 1 4 184 8 再次插入数据：1insert into t4 select * from t4; 不要插入太多数据，查看下一次分配的extent为1个，占8个块1234567select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T4&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T4 0 4 176 8T4 1 4 184 8T4 2 4 192 8 再次插入数据，extent增长情况（64k-1m-64m） 5、创建表空间使用并设置extent大小，统一为2m：创建测试表空间1create tablespace test datafile &apos;/data/u01/app/oracle/oradata/boston/test01.dbf&apos; size 100M extent management local uniform size 2m; 查看表空间参数1234515:28:39 SYS@ boston&gt; select TABLESPACE_NAME,BLOCK_SIZE,INITIAL_EXTENT,NEXT_EXTENT,MIN_EXTENTS,MAX_EXTENTS,EXTENT_MANAGEMENT,ALLOCATION_TYPE from user_tablespaces where tablespace_name=&apos;TEST&apos;;TABLESPACE_NAME BLOCK_SIZE INITIAL_EXTENT NEXT_EXTENT MIN_EXTENTS MAX_EXTENTS EXTENT_MAN ALLOCATIO------------------------------ ---------- -------------- ----------- ----------- ----------- ---------- ---------TEST 8192 2097152 2097152 1 2147483645 LOCAL UNIFORM 6、表空间使用上面创建的test表空间，表参数默认：创建测试表：123drop table t1;create table t1 as select * from emp where 0=1;alter table t1 move tablespace test; 查看表t1参数12345SELECT initial_extent,next_extent,min_extents,max_extents,pct_increase,blocks,sample_size FROM user_tables WHERE table_name = &apos;T1&apos;;INITIAL_EXTENT NEXT_EXTENT MIN_EXTENTS MAX_EXTENTS PCT_INCREASE BLOCKS SAMPLE_SIZE-------------- ----------- ----------- ----------- ------------ ---------- ----------- 2097152 2097152 1 2147483645 0 表数据为空，查看是否有extent区在：1234select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T1&apos;;15:32:51 SYS@ boston&gt; select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T1&apos;;no rows selectedElapsed: 00:00:00.04 插入数据1215:33:16 SCOTT@ boston&gt; insert into t1 select * from emp;14 rows created. 为t1表分配了一个2m大小的extent：123415:32:54 SYS@ boston&gt; select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T1&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T1 0 5 128 256 批量插入数据1234567815:48:07 SYS@ boston&gt; select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T1&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T1 0 5 128 256...T1 80 5 20608 25681 rows selected. 不管插入多少数据，因为设置了统一大小为2m，一个extent永远都是256个块。 7、表空间使用上面创建的test表空间，表参数自定义：创建测试表：123drop table t2;create table t2 as select * from emp where 0=1;ALTER TABLE t2 MOVE STORAGE ( INITIAL 5m NEXT 5m) TABLESPACE test; 查看表t2参数12345SELECT initial_extent,next_extent,min_extents,max_extents,pct_increase,blocks,sample_size FROM user_tables WHERE table_name = &apos;T2&apos;;INITIAL_EXTENT NEXT_EXTENT MIN_EXTENTS MAX_EXTENTS PCT_INCREASE BLOCKS SAMPLE_SIZE-------------- ----------- ----------- ----------- ------------ ---------- ----------- 5242880 5242880 表数据为空，查看是否有extent区在：123select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T2&apos;;no rows selectedElapsed: 00:00:00.11 插入数据：1insert into t2 select * from emp; 因为初始化设置为5m，而每个extent大小统一为2m，因此分配出三个extent：1234567select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T2&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T2 0 5 1920 256T2 1 5 2176 256T2 2 5 2432 256 再次插入数据，一次性不要插入太多：123insert into t2 select * from t2;...insert into t2 (select * from t2 where rownum &lt; 10000); 当前三个extent用满之后再次分配extent时，分配1个extent，大小为2m。1234567select SEGMENT_NAME,EXTENT_ID,FILE_ID,BLOCK_ID,BLOCKS from dba_extents where OWNER=&apos;SCOTT&apos; AND SEGMENT_NAME=&apos;T2&apos;;SEGMENT_NAME EXTENT_ID FILE_ID BLOCK_ID BLOCKS--------------------------------------------------------------------------------- ---------- ---------- ---------- ----------T2 0 5 640 256T2 1 5 896 256T2 2 5 1152 256T2 3 5 1408 256 8、测试总结在本地管理表空间的模式下，oracle分配extent，会根据initial指定的参数分配（按相应的倍数），但是next指定的值会被忽略。官方文档如下所示：]]></content>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Linux] centos安装vsftp]]></title>
    <url>%2F2018%2F11%2F06%2Fcentos%E5%AE%89%E8%A3%85vsftp%2F</url>
    <content type="text"><![CDATA[1、检查vsftp是否安装，如果没有yum安装vsftp：12rpm -qa| grep vsftpdyum install vsftpd -y 2、创建用户：123adduser -s /sbin/nologin -d /var/ftp/ftp1 ftp1 修改用户名：passwd ftp1 3、修改配置文件：cat /etc/vsftpd/vsftpd.conf |grep -v ‘^#’1234567891011121314151617181920212223#禁止匿名用户登录anonymous_enable=NOlocal_enable=YESwrite_enable=YESlocal_umask=022dirmessage_enable=YESxferlog_enable=YESconnect_from_port_20=YESxferlog_std_format=YES#不允许chroot_list下的用户访问其他目录：chroot_list_file=/etc/vsftpd/chroot_listchroot_local_user=YESchroot_list_enable=NOlisten=NOlisten_ipv6=YESpam_service_name=vsftpd#只允许vsftpd.user_list文件下的用户登录：userlist_enable=YESuserlist_deny=NOuserlist_file=/etc/vsftpd/vsftpd.user_listtcp_wrappers=YESallow_writeable_chroot=YES 4、添加访问用户：12345cat /etc/vsftpd/vsftpd.user_listftp1cat /etc/vsftpd/chroot_listftp1 5、启动，查看ftp服务状态1234service vsftpd statusservice vsftpd startservice vsftpd stopservice vsftpd restart 6、客户端安装ftp：1yum -y install ftp 连接到ftp-server1shell&gt; ftp *.*.*.* 7、配置过程遇到的问题：123500 OOPS: vsftpd: refusing to run with writable root inside chroot ()查看配置文件该行参数是否配置：allow_writeable_chroot=YES 12345500 OOPS: chrootLogin failed.421 Service not available, remote server has closed connection检查selinux是否为enforcing状态，并修改：setenforce 0 1234567891011121314151617ftp vsftpd 530 login incorrect解决方法：1.检查密码是否正常。2.检查/etc/vsftpd/vsftpd.conf配置：pam_service_name=vsftpduserlist_enable=YESuserlist_deny=NOuserlist_file=/etc/vsftpd/vsftpd.user_list3.检查/etc/pam.d/vsftpd配置：#%PAM-1.0session optional pam_keyinit.so force revokeauth required pam_listfile.so item=user sense=deny file=/etc/vsftpd/ftpusers onerr=succeedauth required pam_shells.soauth include password-authaccount include password-authsession required pam_loginuid.sosession include password-auth]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql意外drop表之后，使用innobackupex恢复]]></title>
    <url>%2F2018%2F11%2F06%2Fmysql%E6%84%8F%E5%A4%96drop%E8%A1%A8%E4%B9%8B%E5%90%8E%EF%BC%8C%E4%BD%BF%E7%94%A8innobackupex%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[1、使用innodbackupex备份测试删除表：innobackupex备份某以一张表1./innobackupex --defaults-file=/etc/my.cnf --databases=&quot;aaaa.test_order&quot; --user=root --password=12345678 --port=3306 /tmp 如果要备份多个表，使用以下命令：1./innobackupex --defaults-file=/etc/my.cnf --databases=&quot;aaaa.test_order aaaa.test1&quot; --user=root --password=12345678 --port=3306 /tmp 使用innobackupex备份出来的的数据在aaaa文件下面会存在table_name.frm和table_name.ibd两个文件。 2、如果知道表结构重新建表即可，如果表结构也无法获得，可通过该链接，对表结构进行恢复3、恢复完表结构之后，开始恢复数据，丢弃表空间：12345#为防止新的数据写入，对表加锁：root@db 16:20: [aaaa]&gt; lock tables tb1 write;root@db 16:20: [aaaa]&gt; alter table test_order discard tablespace; Query OK, 0 rows affected (0.54 sec) 将对应的ibd文件拷入对应的数据目录,修改数据文件权限12shell &gt; cp test_order.ibd /data/mysql/data/aaaa/shell &gt; chown mysql.mysql test_order.ibd 载入表空间：12root@db 16:24: [aaaa]&gt; alter table test_order import tablespace;Query OK, 0 rows affected, 1 warning (1 min 53.51 sec) 查看有无报错：1show warnings; 4、对备份之后的数据进行恢复，查看备份开始的时间点cat xtrabackup_binlog_info1mysql-binlog.000489 748512183 然后查找drop的pos点：1mysqlbinlog -v --base64-output=DECODE-ROWS /data/mysql/log/mysql-binlog.000489 | grep -C 10 -i &quot;DROP&quot; 找到删除的pos点，如果不在该log文件下，需要根据（show master status)的位置向前依次筛选,找到drop表的记录，内容如下。123server id 1 end_log_pos 748512209 CRC32 0x3fa6b448 Query thread_id=27 DROP TABLE `test_order` /* generated by server *//*!*/; 找到表删除的pos点（748512209），利用binlog2sql生成中间发生sql语句，（binlog2sql方法详解此处）1python binlog2sql.py -uroot -p12345678 -daaaa -ttest_order --start-position=748512183 --stop-position=748512209 --start-file=&apos;mysql-binlog.000489&apos; &gt; /tmp/re_aaaa_test_order.sql 生成的sql内容如下:12345678910111213root@dax-mysql-master binlog2sql]# cat /tmp/re_aaaa_test_order.sqlINSERT INTO `aaaa`.`test_order`(`id`) VALUES (12); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (11); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (10); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (9); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (8); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (7); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (6); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (5); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (4); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (2); #start 2804 end 2949 time 2018-11-06 18:45:45DELETE FROM `aaaa`.`test_order` WHERE `id`=12 LIMIT 1; #start 2571 end 2716 time 2018-11-06 18:45:39DELETE FROM `aaaa`.`test_order` WHERE `id`=11 LIMIT 1; #start 2338 end 2483 time 2018-11-06 18:45:33 生成的sql顺序是倒序的，需要重新调整（github上关于binlog2sql的用法未有关于顺序的说明）：sed -i ‘1!G;h;$!d’ /tmp/re_aaaa_test_order.sql1234567891011121314[root@dax-mysql-master binlog2sql]# sed -i &apos;1!G;h;$!d&apos; /tmp/re_aaaa_test_order.sql[root@dax-mysql-master binlog2sql]# cat /tmp/re_aaaa_test_order.sqlDELETE FROM `aaaa`.`test_order` WHERE `id`=11 LIMIT 1; #start 2338 end 2483 time 2018-11-06 18:45:33DELETE FROM `aaaa`.`test_order` WHERE `id`=12 LIMIT 1; #start 2571 end 2716 time 2018-11-06 18:45:39INSERT INTO `aaaa`.`test_order`(`id`) VALUES (2); #start 2804 end 2949 time 2018-11-06 18:45:45INSERT INTO `aaaa`.`test_order`(`id`) VALUES (4); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (5); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (6); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (7); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (8); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (9); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (10); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (11); #start 3037 end 3222 time 2018-11-06 18:54:21INSERT INTO `aaaa`.`test_order`(`id`) VALUES (12); #start 3037 end 3222 time 2018-11-06 18:54:21 5、把这sql文件进入导入即可进行备份后的数据恢复123[root@dax-mysql-master binlog2sql]# mysql -u root -p12345678 aaaa &lt; /tmp/re_aaaa_test_order.sqlmysql: [Warning] Using a password on the command line interface can be insecure.ERROR 1205 (HY000) at line 1: Lock wait timeout exceeded; try restarting transaction 需要先解锁表：1unlock tables; 解锁完成在重新导入：1[root@dax-mysql-master binlog2sql]# mysql -u root -p12345678 aaaa &lt; /tmp/re_aaaa_test_order.sql 导入成功。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] Can't start group replication on secondary member with single primary-mode while asynchronous replication channels are running]]></title>
    <url>%2F2018%2F11%2F01%2Fmysql-Can't%20start%20group%20replication%20on%20secondary%20member%20with%20single%20primary-mode%20while%20asynchronous%20replication%20channels%20are%20running%2F</url>
    <content type="text"><![CDATA[0、mysql-mgr双节点集群，节点信息如下：123456789101112131415select * from performance_schema.replication_group_members;+---------------------------+--------------------------------------+------------------+-------------+--------------+| CHANNEL_NAME | MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE |+---------------------------+--------------------------------------+------------------+-------------+--------------+| group_replication_applier | bddd9c32-8fee-11e8-ac79-525400edbe8d | dax-mysql-slave | 3306 | ONLINE || group_replication_applier | d5bd8edd-9a1d-11e8-993e-525400578639 | dax-mysql-master | 3306 | ONLINE |+---------------------------+--------------------------------------+------------------+-------------+--------------+2 rows in set (0.00 sec)dax-mysql-master为主节点： SHOW STATUS LIKE &apos;group_replication_primary_member&apos;;+----------------------------------+--------------------------------------+| Variable_name | Value |+----------------------------------+--------------------------------------+| group_replication_primary_member | d5bd8edd-9a1d-11e8-993e-525400578639 |+----------------------------------+--------------------------------------+ 1、因某种需要需要重启数据库，先后停止从节点group_replication;123stop group_replication;停止数据库：/etc/init.d/mysql stop 停止主节点group_replication;12345stop group_replication;重启数据库/etc/init.d/mysql restart启动组复制：start group_replication; 1ERROR 3092 (HY000): The server is not configured properly to be an active member of the group. Please see more details on error log. error.log日志报错：1234567891011122018-11-01T06:57:09.766303Z 6 [Note] Plugin group_replication reported: &apos;Group communication SSL configuration: group_replication_ssl_mode: &quot;DISABLED&quot;&apos;2018-11-01T06:57:09.768075Z 6 [Note] Plugin group_replication reported: &apos;[GCS] Added automatically IP ranges 127.0.0.1/8,192.168.168.178/24 to the whitelist&apos;2018-11-01T06:57:09.769536Z 6 [Note] Plugin group_replication reported: &apos;[GCS] Translated &apos;dax-mysql-master&apos; to 192.168.168.178&apos;2018-11-01T06:57:09.770000Z 6 [Warning] Plugin group_replication reported: &apos;[GCS] Automatically adding IPv4 localhost address to the whitelist. It is mandatory that it is added.&apos;2018-11-01T06:57:09.770200Z 6 [Note] Plugin group_replication reported: &apos;[GCS] SSL was not enabled&apos;2018-11-01T06:57:09.770253Z 6 [Note] Plugin group_replication reported: &apos;Initialized group communication with configuration: group_replication_group_name: &quot;740442c0-cc67-11e8-993e-525400578639&quot;; group_replication_local_address: &quot;dax-mysql-master:24901&quot;; group_replication_group_seeds: &quot;dax-mysql-master:24901,dax-mysql-slave:24901&quot;; group_replication_bootstrap_group: false; group_replication_poll_spin_loops: 0; group_replication_compression_threshold: 1000000; group_replication_ip_whitelist: &quot;AUTOMATIC&quot;&apos;2018-11-01T06:57:09.770341Z 6 [Note] Plugin group_replication reported: &apos;[GCS] Configured number of attempts to join: 0&apos;2018-11-01T06:57:09.770356Z 6 [Note] Plugin group_replication reported: &apos;[GCS] Configured time between attempts to join: 5 seconds&apos;2018-11-01T06:57:09.770494Z 6 [Note] Plugin group_replication reported: &apos;Member configuration: member_id: 3306103; member_uuid: &quot;d5bd8edd-9a1d-11e8-993e-525400578639&quot;; single-primary mode: &quot;true&quot;; group_replication_auto_increment_increment: 7; &apos;2018-11-01T06:57:09.770589Z 6 [ERROR] Plugin group_replication reported: &apos;Can&apos;t start group replication on secondary member with single primary-mode while asynchronous replication channels are running.&apos;2018-11-01T06:57:09.770682Z 6 [Note] Plugin group_replication reported: &apos;Requesting to leave the group despite of not being a member&apos;2018-11-01T06:57:09.770696Z 6 [ERROR] Plugin group_replication reported: &apos;[GCS] The member is leaving a group without being on one.&apos; 尝试重启dax-mysql-slave节点组复制：12 start group_replication;ERROR 3092 (HY000): The server is not configured properly to be an active member of the group. Please see more details on error log. error.log日志报错：123456789101112131415161718192021222324252627282018-11-01T06:59:20.617005Z 114 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_applier&apos; executed&apos;. Previous state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 128090715, master_bind=&apos;&apos;. New state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-11-01T06:59:21.516164Z 111 [Note] Plugin group_replication reported: &apos;Group Replication applier module successfully initialized!&apos;2018-11-01T06:59:21.516264Z 117 [Note] Slave SQL thread for channel &apos;group_replication_applier&apos; initialized, starting replication in log &apos;FIRST&apos; at position 0, relay log &apos;./relay-log-group_replication_applier.000002&apos; position: 42018-11-01T06:59:21.516277Z 111 [Note] Plugin group_replication reported: &apos;auto_increment_increment is set to 7&apos;2018-11-01T06:59:21.516298Z 111 [Note] Plugin group_replication reported: &apos;auto_increment_offset is set to 3306102&apos;2018-11-01T06:59:21.553616Z 0 [Note] Plugin group_replication reported: &apos;XCom protocol version: 3&apos;2018-11-01T06:59:21.553718Z 0 [Note] Plugin group_replication reported: &apos;XCom initialized and ready to accept incoming connections on port 24901&apos;2018-11-01T06:59:21.620998Z 0 [ERROR] Plugin group_replication reported: &apos;[GCS] Error on opening a connection to dax-mysql-master:24901 on local port: 24901.&apos;2018-11-01T06:59:21.621468Z 0 [ERROR] Plugin group_replication reported: &apos;[GCS] Error on opening a connection to dax-mysql-master:24901 on local port: 24901.&apos;2018-11-01T06:59:21.621794Z 0 [ERROR] Plugin group_replication reported: &apos;[GCS] Error on opening a connection to dax-mysql-master:24901 on local port: 24901.&apos;2018-11-01T06:59:21.621988Z 0 [ERROR] Plugin group_replication reported: &apos;[GCS] Error on opening a connection to dax-mysql-master:24901 on local port: 24901.&apos;2018-11-01T06:59:21.622185Z 0 [ERROR] Plugin group_replication reported: &apos;[GCS] Error on opening a connection to dax-mysql-master:24901 on local port: 24901.&apos;2018-11-01T06:59:21.622379Z 0 [ERROR] Plugin group_replication reported: &apos;[GCS] Error on opening a connection to dax-mysql-master:24901 on local port: 24901.&apos;2018-11-01T06:59:21.622658Z 0 [ERROR] Plugin group_replication reported: &apos;[GCS] Error on opening a connection to dax-mysql-master:24901 on local port: 24901.&apos;2018-11-01T06:59:21.622875Z 0 [ERROR] Plugin group_replication reported: &apos;[GCS] Error on opening a connection to dax-mysql-master:24901 on local port: 24901.&apos;2018-11-01T06:59:21.623043Z 0 [ERROR] Plugin group_replication reported: &apos;[GCS] Error on opening a connection to dax-mysql-master:24901 on local port: 24901.&apos;2018-11-01T06:59:21.623201Z 0 [ERROR] Plugin group_replication reported: &apos;[GCS] Error on opening a connection to dax-mysql-master:24901 on local port: 24901.&apos;2018-11-01T06:59:21.623220Z 0 [ERROR] Plugin group_replication reported: &apos;[GCS] Error connecting to all peers. Member join failed. Local port: 24901&apos;2018-11-01T06:59:21.624544Z 0 [Warning] Plugin group_replication reported: &apos;read failed&apos;2018-11-01T06:59:21.656139Z 0 [ERROR] Plugin group_replication reported: &apos;[GCS] The member was unable to join the group. Local port: 24901&apos;2018-11-01T07:00:21.516998Z 111 [ERROR] Plugin group_replication reported: &apos;Timeout on wait for view after joining group&apos;2018-11-01T07:00:21.517630Z 111 [Note] Plugin group_replication reported: &apos;Requesting to leave the group despite of not being a member&apos;2018-11-01T07:00:21.517820Z 111 [ERROR] Plugin group_replication reported: &apos;[GCS] The member is leaving a group without being on one.&apos;2018-11-01T07:00:21.519413Z 111 [Note] Plugin group_replication reported: &apos;auto_increment_increment is reset to 1&apos;2018-11-01T07:00:21.519442Z 111 [Note] Plugin group_replication reported: &apos;auto_increment_offset is reset to 1&apos;2018-11-01T07:00:21.521022Z 117 [Note] Error reading relay log event for channel &apos;group_replication_applier&apos;: slave SQL thread was killed2018-11-01T07:00:21.595775Z 114 [Note] Plugin group_replication reported: &apos;The group replication applier thread was killed&apos;2018-11-01T07:10:22.699429Z 111 [Note] Aborted connection 111 to db: &apos;unconnected&apos; user: &apos;root&apos; host: &apos;localhost&apos; (Got timeout reading communication packets) 使用mysql-shell查看集群信息：1/data/soft/mysql-shell/bin/mysqlsh --uri repl@dax-mysql-master:3306 尝试查看集群信息：12cluster = dba.getCluster(&quot;prodCluster&quot;)Dba.getCluster: This function is not available through a session to a standalone instance (metadata exists, but GR is not active) (RuntimeError) 提示集群系统没有激活，尝试重启集群系统：12345678910111213141516171819202122232425262728293031323334353637dba.rebootClusterFromCompleteOutage(&apos;prodCluster&apos;)Reconfiguring the cluster &apos;prodCluster&apos; from complete outage...The instance &apos;dax-mysql-slave:3306&apos; was part of the cluster configuration.Would you like to rejoin it to the cluster? [y/N]: yWARNING: On instance &apos;dax-mysql-master:3306&apos; membership change cannot be persisted since MySQL version 5.7.22 does not support the SET PERSIST command (MySQL version &gt;= 8.0.5 required). Please use the &lt;Dba&gt;.configureLocalInstance command locally to persist the changes.The cluster was successfully rebooted.&lt;Cluster:prodCluster&gt; MySQL dax-mysql-master:3306 JS &gt; cluster = dba.getCluster(&quot;prodCluster&quot;)&lt;Cluster:prodCluster&gt; MySQL dax-mysql-master:3306 JS &gt; cluster.status()&#123; &quot;clusterName&quot;: &quot;prodCluster&quot;, &quot;defaultReplicaSet&quot;: &#123; &quot;name&quot;: &quot;default&quot;, &quot;primary&quot;: &quot;dax-mysql-master:3306&quot;, &quot;ssl&quot;: &quot;DISABLED&quot;, &quot;status&quot;: &quot;OK_NO_TOLERANCE&quot;, &quot;statusText&quot;: &quot;Cluster is NOT tolerant to any failures.&quot;, &quot;topology&quot;: &#123; &quot;dax-mysql-master:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-master:3306&quot;, &quot;mode&quot;: &quot;R/W&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125;, &quot;dax-mysql-slave:3306&quot;: &#123; &quot;address&quot;: &quot;dax-mysql-slave:3306&quot;, &quot;mode&quot;: &quot;R/O&quot;, &quot;readReplicas&quot;: &#123;&#125;, &quot;role&quot;: &quot;HA&quot;, &quot;status&quot;: &quot;ONLINE&quot; &#125; &#125; &#125;, &quot;groupInformationSourceMember&quot;: &quot;mysql://repl@dax-mysql-master:3306&quot;&#125; 集群状态恢复正常。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql使用备份的.frm文件恢复表结构]]></title>
    <url>%2F2018%2F11%2F01%2Fmysql%E4%BD%BF%E7%94%A8%E5%A4%87%E4%BB%BD%E7%9A%84.frm%E6%96%87%E4%BB%B6%E6%81%A2%E5%A4%8D%E8%A1%A8%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[0、例如备份的表为test_order,则备份出来的.frm文件为test_order.frm1、在只知道表名的情况下，随意创建一个表名为test_order的表：1create table test_order (id1 int(2)); 替换test_order.frm文件，替换完成之后重启mysql数据库,查看表信息1234root@db 11:29: [test]&gt; desc test_order;ERROR 1146 (42S02): Table &apos;test.test_order&apos; doesn&apos;t existroot@db 11:29: [test]&gt; show create table test_order;ERROR 1146 (42S02): Table &apos;test.test_order&apos; doesn&apos;t exist 报错提示表不存在，之后查看error.log：12342018-11-01T11:29:10.359514Z 2 [Warning] InnoDB: Table test/test_order contains 1 user defined columns in InnoDB, but 25 columns in MySQL. Please check INFORMATION_SCHEMA.INNODB_SYS_COLUMNS and http://dev.mysql.com/doc/refman/5.7/en/innodb-troubleshooting.html for how to resolve the issue.2018-11-01T11:29:10.359631Z 2 [Warning] InnoDB: Cannot open table test/test_order from the internal data dictionary of InnoDB though the .frm file for the table exists. Please refer to http://dev.mysql.com/doc/refman/5.7/en/innodb-troubleshooting.html for how to resolve the issue.2018-11-01T11:29:25.514189Z 2 [Note] InnoDB: Table `test`.`test_order` is corrupted. Please drop the table and recreate it2018-11-01T11:29:25.514313Z 2 [Warning] InnoDB: Cannot open table test/test_order from the internal data dictionary of InnoDB though the .frm file for the table exists. Please refer to http://dev.mysql.com/doc/refman/5.7/en/innodb-troubleshooting.html for how to resolve the issue. 报错提示手动创建的表只有1列，但是mysql中记录的表有25列 2、删除test_order表，并创建一个25列的test_order表：123root@db 11:40: [test]&gt; drop table test_order;Query OK, 0 rows affected (0.06 sec)root@db 11:40: [test]&gt; create table test_order (id1 int(2),id2 int(2),id3 int(2),id4 int(2),id5 int(2),id6 int(2),id7 int(2),id8 int(2),id9 int(2),id10 int(2),id11 int(2),id12 int(2),id13 int(2),id14 int(2),id15 int(2),id16 int(2),id17 int(2),id18 int(2),id19 int(2),id20 int(2),id21 int(2),id22 int(2),id23 int(2),id24 int(2),id25 int(2)); 替换test_order.frm文件，替换完成之后重启mysql数据库,查看表信息12345678root@db 11:41: [test]&gt; desc test_order;ERROR 2013 (HY000): Lost connection to MySQL server during queryroot@db 11:42: [test]&gt; show create table test_order;ERROR 2006 (HY000): MySQL server has gone awayNo connection. Trying to reconnect...Connection id: 2Current database: testERROR 2013 (HY000): Lost connection to MySQL server during query 查看error.log：12345678910112018-11-01T11:42:25.566095Z 2 [ERROR] Build InnoDB index translation table for Table ./test/test_order failed2018-11-01T11:42:25.566445Z 2 [ERROR] Table ./test/test_order has no primary key in InnoDB data dictionary, but has one in MySQL! If you created the table with a MySQL version &lt; 3.23.54 and did not define a primary key, but defined a unique key with all non-NULL columns, then MySQL internally treats that key as the primary key. You can fix this error by dump + DROP + CREATE + reimport of the table.2018-11-01T11:42:25.566519Z 2 [Warning] Table ./test/test_order key_used_on_scan is 0 even though there is no primary key inside InnoDB.2018-11-01T11:42:25.566576Z 2 [ERROR] InnoDB could not find key no 0 with name PRIMARY from dict cache for table test/test_order11:42:25 UTC - mysqld got signal 11 ;This could be because you hit a bug. It is also possible that this binaryor one of the libraries it was linked against is corrupt, improperly built,or misconfigured. This error can also be caused by malfunctioning hardware.Attempting to collect some information that could help diagnose the problem.As this is a crash and something is definitely wrong, the informationcollection process might fail. 报错提示test_order表没有主键 3、删除test_order表，并创建一个25列的带有主键的test_order表：123root@db 11:45: [test]&gt; drop table test_order;Query OK, 0 rows affected (0.06 sec)root@db 11:45: [test]&gt; create table test_order (id1 int(2) primary key,id2 int(2),id3 int(2),id4 int(2),id5 int(2),id6 int(2),id7 int(2),id8 int(2),id9 int(2),id10 int(2),id11 int(2),id12 int(2),id13 int(2),id14 int(2),id15 int(2),id16 int(2),id17 int(2),id18 int(2),id19 int(2),id20 int(2),id21 int(2),id22 int(2),id23 int(2),id24 int(2),id25 int(2)); 替换test_order.frm文件，替换完成之后重启mysql数据库,查看表信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667root@db 11:46: [test]&gt; desc test_order;+-----------------+----------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-----------------+----------------+------+-----+---------+----------------+| id | bigint(20) | NO | PRI | NULL | auto_increment || order_no | varchar(24) | NO | UNI | NULL | || broker_id | varchar(24) | NO | MUL | NULL | || broker_uid | bigint(20) | NO | | NULL | || plat_id | varchar(24) | NO | | NULL | || price_asset | varchar(8) | NO | | NULL | || test_type | varchar(16) | NO | | NULL | || order_type | varchar(16) | NO | | NULL | || price | decimal(32,20) | NO | | NULL | || number | decimal(32,20) | NO | | NULL | || test_asset | varchar(8) | NO | MUL | NULL | || testd_number | decimal(32,20) | NO | | NULL | || over_number | decimal(32,20) | NO | | NULL | || testd_money | decimal(32,20) | NO | | NULL | || fee_asset | varchar(8) | NO | | NULL | || fee | decimal(32,20) | NO | | NULL | || broker_fee | decimal(32,20) | NO | | NULL | || cloud_fee | decimal(32,20) | NO | | NULL | || client_order_no | varchar(36) | NO | | NULL | || state | varchar(16) | NO | | NULL | || send_state | varchar(16) | NO | | NULL | || error_code | varchar(24) | YES | | NULL | || error_msg | varchar(64) | YES | | NULL | || create_time | datetime | NO | | NULL | || update_time | datetime | NO | | NULL | |+-----------------+----------------+------+-----+---------+----------------+25 rows in set (0.01 sec)root@db 11:46: [test]&gt; show create table test_order\G*************************** 1. row *************************** Table: test_orderCreate Table: CREATE TABLE `test_order` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `order_no` varchar(24) NOT NULL, `broker_id` varchar(24) NOT NULL, `broker_uid` bigint(20) NOT NULL, `plat_id` varchar(24) NOT NULL, `price_asset` varchar(8) NOT NULL, `test_type` varchar(16) NOT NULL, `order_type` varchar(16) NOT NULL, `price` decimal(32,20) NOT NULL, `number` decimal(32,20) NOT NULL, `test_asset` varchar(8) NOT NULL, `testd_number` decimal(32,20) NOT NULL, `over_number` decimal(32,20) NOT NULL, `testd_money` decimal(32,20) NOT NULL, `fee_asset` varchar(8) NOT NULL, `fee` decimal(32,20) NOT NULL, `broker_fee` decimal(32,20) NOT NULL, `cloud_fee` decimal(32,20) NOT NULL, `client_order_no` varchar(36) NOT NULL, `state` varchar(16) NOT NULL, `send_state` varchar(16) NOT NULL, `error_code` varchar(24) DEFAULT NULL, `error_msg` varchar(64) DEFAULT NULL, `create_time` datetime NOT NULL, `update_time` datetime NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `test_order_order_no_uindex` (`order_no`), UNIQUE KEY `test_order_broker_id_client_order_no_uindex` (`broker_id`,`client_order_no`), KEY `multi_price_test_id_uid_state_index` (`test_asset`,`price_asset`,`broker_uid`,`broker_id`,`state`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb41 row in set (0.01 sec) 查看error.log有无其他报错：1234562018-11-01T11:45:09.272752Z 2 [ERROR] Build InnoDB index translation table for Table ./test/test_order failed2018-11-01T11:45:09.272904Z 2 [ERROR] InnoDB: MySQL and InnoDB data dictionaries are out of sync. Unable to find the AUTOINC column id in the InnoDB table `test`.`test_order`. We set the next AUTOINC column value to 0, in effect disabling the AUTOINC next value generation.2018-11-01T11:45:09.272935Z 2 [Note] InnoDB: You can either set the next AUTOINC value explicitly using ALTER TABLE or fix the data dictionary by recreating the table.2018-11-01T11:45:09.272959Z 2 [ERROR] InnoDB: Table test/test_order contains 1 indexes inside InnoDB, which is different from the number of indexes 4 defined in MySQL2018-11-01T11:45:09.272976Z 2 [ERROR] InnoDB could not find key no 1 with name test_order_order_no_uindex from dict cache for table test/test_order2018-11-01T11:45:09.272988Z 2 [ERROR] Table test/test_order contains fewer indexes inside InnoDB than are defined in the MySQL .frm file. Have you mixed up .frm files from different installations? Please refer to http://dev.mysql.com/doc/refman/5.7/en/innodb-troubleshooting.html for how to resolve the issue. log日志依旧有报错，提示innodb只有一个索引，但mysql下存在4个索引，但是已经通过show create table test_order获取到了建表的语句，因此可以通过上面的建表语句，重新创建test_order表了。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Linux] 使用certbot为域名生成免费证书(nginx版)]]></title>
    <url>%2F2018%2F10%2F31%2Fnginx%20https%E8%AF%81%E4%B9%A6%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1、下载letencrypt,用于生产免费证书工具：123cd /data/softwget https://dl.eff.org/certbot-autochmod a+x certbot-auto 2、修改域名对应的配置文件，添加下面内容12345678910server &#123; listen 80; server_name test.com; ... location ~ /.well-known &#123; root /data/soft; allow all; &#125; ...&#125; 上面配置的 root /data/soft信息目录最好不要与其他location指定的目录相同,且确保各个目录存在目录,如果目录相同的情况下可能会遇到以下问题：1234567891011[quote]Failed authorization procedure. test.com (http-01): urn:ietf:params:acme:error:unauthorized :: The client lacks sufficient authorization :: Invalid response from http://mydomain.fr/.well-known/acme-challenge/Xefe-sxGfexdcdezDEUJZRfexjfeeloekcdsesx [2001:1600:4:1::b]: 404IMPORTANT NOTES:The following errors were reported by the server:Domain: test.comType: unauthorizedDetail: Invalid response fromhttp://mydomain.fr/.well-known/acme-challenge/Xefe-sxGfexdcdezDEUJZRfexjfeeloekcdsesx[2001:1600:4:1::b]: 404To fix these errors, please make sure that your domain name wasentered correctly and the DNS A/AAAA record(s) for that domaincontain(s) the right IP address.[/quote] 如果服务器的80端口被限制，会提示如下错误：12http://demo.broker.masterdax.com/.well-known/acme-challenge/pIV-Rh1355xsh8xJFHhB0llri6HS8S2yOSYRE9N5D5I: Timeout during connect (likely firewall problem) 3、执行生成证书命令：1/data/soft/certbot-auto certonly --email 123456@qq.com --agree-tos --webroot -w /data/soft/ -d test.com --dry-run 第一次尝试生成证书最好加上–dry-run参数，如果le生成证书次数（包括报错的次数）每天有上限，添加–dry-run调试没有问题之后再真正生成证书1/data/soft/certbot-auto certonly --email 123456@qq.com --agree-tos --webroot -w /data/soft/ -d test.com 4、查看生成的证书1/data/soft/certbot-auto certificates 5、修改nginx配置文件，添加https相关配置信息,http相关配置加上跳转到https：1234567891011121314151617181920server &#123; listen 80; server_name test.com; rewrite ^(.*)$ https://$server_name$1 permanent; ... location ~ /.well-known &#123; root /data/soft; allow all; &#125; ...&#125;server &#123; listen 443 ssl; server_name test.com; .... ssl_certificate ssl/fullchain.pem; ssl_certificate_key ssl/privkey.pem; ...&#125; 6、将生成的证书添加软连接到nginx配置文件指定的路径：12ln -s /etc/letsencrypt/live/test.com/fullchain.pem /usr/local/openresty/nginx/conf/ssl/fullchain.pemln -s /etc/letsencrypt/live/test.com/privkey.pem /usr/local/openresty/nginx/conf/ssl/privkey.pem 7、因为le生成的证书有效期为90天，需要添加定时任务，使其证书自动更新：cat /data/soft/cron-cerbot.sh12#!/bin/bash/data/soft/certbot-auto renew 授权文件执行权限：1chmod a+x /data/soft/cron-cerbot.sh 添加到crontab,每周日凌晨定期更新crontab -l10 0 * * 0 /data/soft/cron-cerbot.sh]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Oracle] oracle 静默安装]]></title>
    <url>%2F2018%2F10%2F31%2Foracle%20%E9%9D%99%E9%BB%98%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1、安装数据库所需依赖包12345yum install -y gcc*yum install -y glibc*yum install -y compat-libstdc++-33 elfutils-libelf-devel libaio-devel compat-libcap1yum install -y sysstatyum install -y smartmontools 2、创建用户,并修改用户名密码1234/usr/sbin/groupadd -g 505 oinstall/usr/sbin/groupadd -g 502 dba/usr/sbin/groupadd -g 503 oper/usr/sbin/useradd -u 506 -g oinstall -G dba,oper oracle 配置用户名密码1passwd oracle 3、配置相关数据库目录权限1234567mkdir -p /data/u01/app/oraclechown oracle.oinstall -R /data/u01/app/oraclechmod 775 /data/u01/app/oraclemkdir -p /data/u01/app/oracle/product/11.2.0.4/dbhome_1/chown oracle.oinstall -R /data/u01/app/oracle/product/11.2.0.4/dbhome_1/chmod 775 /data/u01/app/oracle/product/11.2.0.4/dbhome_1/chown oracle.oinstall -R /data/u01 4、修改系统参数：4.1、关闭selinux1setenforce 0 4.2、关闭防火墙1iptables -F 4.3、设置用户连接限制cat /etc/security/limits.conf1234567891011121314oracle soft nproc 16384oracle hard nproc 16384oracle soft nofile 65536oracle hard nofile 65536oracle soft memlock 8088608oracle hard memlock 8088608oracle hard stack 10240grid soft nproc 16384grid hard nproc 16384grid soft nofile 65536grid hard nofile 65536grid soft memlock 8088608grid hard memlock 8088608grid hard stack 10240 oracle用户最大能开启的进程数不超过16384，最大能打开的文件数不超过65536。至于soft和hard的区别，不同于磁盘配额中的软限制和硬限制。普通用户可以调整自己的softlimit但最高不能超过hardlimit，而且除了root以外的普通用户也不能够随意更改hard limit。该调整完成之后一般可以使用ulimit命令查看。针对nofile，这个只是基于用户层面的限制和调整方法。基于系统层面的限制和调整方法是修改/etc/sysctl.conf文件，直接改fs.file-max参数，调整之后sysctl –p生效memlock参数随着服务器内存不同，进行调整，该参数要偏小于实际的物理内存，本文以（8g内存为例） 4.4、为使/etc/security/limits.conf文件配置生效，需要确保pam模块pam_limits.so被加入到启动文件中：grep ‘pam_limits.so’ /etc/pam.d/login1session required pam_limits.so pam_limits.so模块对用户使用系统资源的情况进行限制,也可以使用在对一般应用程序使用的资源限制方面。举例来说，如果需要在SSH服务器上对来自不同用户的ssh访问进行限制，就可以调用该模块来实现相关功能。例如，当需要限制用户admin登录到SSH服务器时的最大连接数（防止同一个用户开启过多的登录进程），就可以在/etc/pam.d/sshd 文件中增加一行对 pam_limits.so 模块的调用:然后在/etc/security/limits.conf文件中增加一行对admin用户产生的连接数进行限定:admin hard maxlogins 2完成之后重启服务器端的sshd服务。之后我们可以看到，从客户端以admin身份登录SSH服务器时，在客户端上可以打开两个控制台登录。但当客户端开启第三个登录窗口的时候会被服务器拒绝，但其它用户不会受到限制。 4.5、设置用户最大进程数和进程可以打开的最大文件描述符的数量：cat /etc/profile12345678if [ $USER = &quot;oracle&quot; -o $USER = &quot;grid&quot; ]; then if [ $SHELL = &quot;/bin/ksh&quot; ]; then ulimit -p 16384 ulimit -n 65536 else ulimit -u 16384 -n 65536 fifi 4.6、设置内核参数cat /etc/sysctl.conf12345678910111213kernel.shmmax = 7730941132kernel.shmall = 15586574kernel.shmmni = 4096kernel.sem = 250 32000 100 128fs.file-max = 6815744fs.aio-max-nr = 1048576net.ipv4.ip_local_port_range = 9000 65500net.core.rmem_default = 1048576net.core.rmem_max = 4194304net.core.wmem_default = 262144net.core.wmem_max = 1048576vm.min_free_kbytes = 524288vm.swappiness = 60 使配置生效1sysctl -p 安装过程中shmmax,shmall参数设置过小，导致dbca创建数据库的时候报错ORA-12547：TNS：lost contact图形化安装，oracle有一个runfix脚本，调整这些参数，参考response下的文件，如果文件里面的参数小于参考文件的参数，会进行调整，如果文件参数大于参考文件参数，则不会进行调整。 SHMMAX应该比SGA区大,否则会引发性能的下降,shmmax 指的是单个共享内存段的最大尺寸， 设置shmmax=1G，sga分配了1.2G，当启动实例的时候就分配 2 块共享内存给Oracle kernel.shmall=（SHMMAX/PAGE_SIZE）：共享内存总量，以页为单位。Linux 共享内存页大小为4KB, 共享内存段的大小都是共享内存页大小的整数倍。一个共享内存段的最大大小是16G，那么需要共享内存页数是 16GB/4KB=16777216KB/4KB=4194304 （页），也就是64Bit 系统下16GB 物理内存，设置 kernel.shmall = 4194304 才符合要求 SHMMIN= 最小的内存段的大小 kernel.shmmni：共享内存段的最大数量，shmmni 缺省值 4096 ，一般肯定是够用了 kernel.sem(SEMMSL SEMMNS SEMOPM SEMMNI)：SEMMSL，每个信号量集中的最大信号量数，应该设置为服务器中各个实例中PROCESSES参数的和+10；SEMMNS，系统中信号量的最大数，参数应设置为SEMMSL*SEMMNI。SEMOPM，每个信号量调用所包含的最大操作数。SEMMNI：系统中信号量集的最大数。swappiness的值的大小对如何使用swap分区是有着很大的联系的。swappiness=0的时候表示最大限度使用物理内存，然后才是 swap空间，swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。linux的基本默认设置为60 5、修改oracle用户环境变量su - oraclevim ~/.bash_profile123456789101112131415161718192021# .bash_profile# Get the aliases and functionsif [ -f ~/.bashrc ]; then . ~/.bashrcfi# User specific environment and startup programsPATH=$PATH:$HOME/binexport PATHexport THREADS_FLAG=nativeexport ORACLE_BASE=/data/u01/app/oracleexport ORACLE_HOME=$ORACLE_BASE/product/11.2.0.4/dbhome_1export PATH=$ORACLE_HOME/bin:$ORACLE_HOME/OPatch:$PATH:/usr/sbinexport LD_LIBRARY_PATH=$ORACLE_HOME/libexport ORACLE_SID=bostonexport NLS_LANG=AMERICAN_AMERICA.ZHS16GBKexport NLS_DATE_FORMAT=&quot;YYYY:MM:DD HH24:MI:SS&quot;export EDITOR=viset -o viumask 022alias bdump=&quot;cd $ORACLE_BASE/diag/rdbms/otcdb/$&#123;ORACLE_SID&#125;/trace&quot;alias sql=&apos;sqlplus &quot;/ as sysdba&quot;&apos; 使环境变量生效1source ~/.bash_profile 6、解压软件包，在database/response目录下找到db_install.rsp文件编辑db_install.rsp文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384#选择安装类型：1.INSTALL_DB_SWONLY只装数据库软件 2.INSTALL_DB_AND_CONFIG安装数据库软件并建库 3.UPGRADE_DB升级数据库oracle.install.option=INSTALL_DB_SWONLY#指定操作系统主机名，通过hostname命令获得ORACLE_HOSTNAME=dax-mysql-slave#指定oracle inventory目录的所有者，通常会是oinstall或者dbaUNIX_GROUP_NAME=oinstall#指定产品清单oracle inventory目录的路径,如果是Win平台下可以省略INVENTORY_LOCATION=/data/u01/app/oracle/oraInventory#指定数据库语言，可以选择多个，用逗号隔开。选择en, zh_CN(英文和简体中文)SELECTED_LANGUAGES=en,zh_CN# Specify the complete path of the OracleHome.设置ORALCE_HOME的路径ORACLE_HOME=/data/u01/app/oracle/product/11.2.0.4/dbhome_1# Specify the complete path of the OracleBase. 设置ORALCE_BASE的路径ORACLE_BASE=/data/u01/app/oracle#选择Oracle安装数据库软件的版本（企业版，标准版，标准版1），不同的版本功能不同oracle.install.db.InstallEdition=EE#是否自定义Oracle的组件，如果选择false，则会使用默认的组件#如果选择true否则需要自己在下面一条参数将要安装的组件一一列出。#安装相应版权后会安装所有的组件，后期如果缺乏某个组件，再次安装会非常的麻烦。oracle.install.db.EEOptionsSelection=false# oracle.install.db.EEOptionsSelection=true的话下面的安装组件会安装oracle.install.db.optionalComponents=oracle.rdbms.partitioning:11.2.0.4.0,oracle.oraolap:11.2.0.4.0,oracle.rdbms.dm:11.2.0.4.0,oracle.rdbms.dv:11.2.0.4.0,oracle.rdbms.lbac:11.2.0.4.0,oracle.rdbms.rat:11.2.0.4.0#指定拥有OSDBA、OSOPER权限的用户组，通常会是dba组oracle.install.db.DBA_GROUP=dbaoracle.install.db.OPER_GROUP=oinstall#如果是RAC的安装，在这里指定所有的节点oracle.install.db.CLUSTER_NODES=oracle.install.db.isRACOneInstall=oracle.install.db.isRACOneInstall=--------安装数据库选项#选择数据库的用途，一般用途/事物处理，数据仓库oracle.install.db.config.starterdb.type=GENERAL_PURPOSE# Specify the Starter Database GlobalDatabase Name. 指定GlobalNameoracle.install.db.config.starterdb.globalDBName=boston# Specify the Starter Database SID.指定SIDoracle.install.db.config.starterdb.SID=boston#选择字符集。不正确的字符集会给数据显示和存储带来麻烦无数。#通常中文选择的有ZHS16GBK简体中文库，建议选择unicode的AL32UTF8国际字符集oracle.install.db.config.starterdb.characterSet=AL32UTF8#11g的新特性自动内存管理，也就是SGA_TARGET和PAG_AGGREGATE_TARGET都#不用设置了，Oracle会自动调配两部分大小。oracle.install.db.config.starterdb.memoryOption=true#指定Oracle自动管理内存的大小，最小是256MBoracle.install.db.config.starterdb.memoryLimit=5120#是否载入模板示例oracle.install.db.config.starterdb.installExampleSchemas=false# These settings may also be disabled. 是否启用安全设置oracle.install.db.config.starterdb.enableSecuritySettings=true#设置数据库用户密码oracle.install.db.config.starterdb.password.ALL=oracle#数据库本地管理工具DB_CONTROL，远程集中管理工具GRID_CONTROLoracle.install.db.config.starterdb.control=DB_CONTROL#.设置自动备份，和OUI里的自动备份一样。oracle.install.db.config.starterdb.automatedBackup.enable=false#自动备份会启动一个job，指定启动JOB的系统用户IDoracle.install.db.config.starterdb.automatedBackup.osuid=#自动备份会开启一个job，需要指定OSUser的密码oracle.install.db.config.starterdb.automatedBackup.ospwd=#自动备份，要求指定使用的文件系统存放数据库文件还是ASMoracle.install.db.config.starterdb.storageType=#使用文件系统存放数据库文件才需要指定数据文件、控制文件、Redo log的存放目录oracle.install.db.config.starterdb.fileSystemStorage.dataLocation=#使用文件系统存放数据库文件才需要指定备份恢复目录oracle.install.db.config.starterdb.fileSystemStorage.recoveryLocation=#使用ASM存放数据库文件才需要指定存放的磁盘组oracle.install.db.config.asm.diskGroup=#使用ASM存放数据库文件才需要指定ASM实例密码oracle.install.db.config.asm.ASMSNMPPassword=#指定metalink账户用户名MYORACLESUPPORT_USERNAME=# 指定metalink账户密码MYORACLESUPPORT_PASSWORD=# 用户是否可以设置metalink密码SECURITY_UPDATES_VIA_MYORACLESUPPORT=# False表示不需要设置安全更新，注意，在11.2的静默安装中疑似有一个BUG******# Response File中必须指定为true，否则会提示错误,不管是否正确填写了邮件地址DECLINE_SECURITY_UPDATES=true#代理服务器名PROXY_HOST=#代理服务器端口PROXY_PORT=#代理服务器用户名PROXY_USER=#代理服务器密码PROXY_PWD= 开始安装：1./runInstaller -silent -responseFile /data/soft/database/response/db_install.rsp 如果是测试环境可能碰到下面的问题：12345678910Checking swap space: 0 MB available, 150 MB required. Failed &lt;&lt;&lt;&lt;解决方法：dd if=/dev/zero of=/data/swapfile bs=1M count=1024mkswap /data/swapfileswapon /data/swapfile在/etc/fstab添加下面内容/data/swapfile swap swap defaults 0 0&gt;&gt;&gt;&gt;&gt;` oracle安装到最后提示，使用root用户执行以下两个脚本：12/data/u01/app/oracle/oraInventory/orainstRoot.sh/data/u01/app/oracle/product/11.2.0.4/dbhome_1/root.sh 7、静默配置监听通过response文件运行netca, 生成sqlnet.ora和listener.ora文件, 位于$ORACLE_HOME/network/admin目录下:1234# su - oracle$ORACLE_HOME/bin/netca -silent -responsefile /data/soft/database/response/netca.rspll $ORACLE_HOME/network/admin/*.oralsnrctl status 8、创建数据库修改配置文件dbca.rsp12345678910111213[GENERAL]RESPONSEFILE_VERSION = &quot;11.2.0&quot;OPERATION_TYPE = &quot;createDatabase&quot;[CREATEDATABASE]GDBNAME = &quot;boston.us.oracle.com&quot;TEMPLATENAME = &quot;General_Purpose.dbc&quot;SID = &quot;boston&quot;SYSPASSWORD = &quot;oracle&quot;SYSTEMPASSWORD = &quot;oracle&quot;SYSMANPASSWORD = &quot;oracle&quot;DBSNMPPASSWORD = &quot;oracle&quot;CHARACTERSET = &quot;ZHS16GBK&quot;TOTALMEMORY = &quot;2048&quot; 运行命令开始创建数据库：1dbca -silent -createDatabase -responseFile /data/soft/database/response/createdbca.rsp 如果不适用配置文件，也可使用下面命令直接指定参数：1##dbca -silent -createDatabase -templateName $ORACLE_HOME/assistants/dbca/templates/General_Purpose.dbc -gdbName orcogg -sid orcogg -sysPassword oracle -systemPassword oracle -datafileDestination /u01/app/oradata/orcogg -characterSet GBK16 -TOTALMEMORY 2048 各参数含义如下:-silent 表示以静默方式安装-responseFile 表示使用哪个响应文件,必需使用绝对路径RESPONSEFILE_VERSION 响应文件模板的版本,该参数不要更改OPERATION_TYPE 安装类型,该参数不要更改GDBNAME 全局数据库名,点号前面默认是db_name,点号后面默认就是db_domainTEMPLATENAME 建库模板名,参考各模板定义:$ORACLE_HOME/assistants/dbca/templates/*.dbcCHARACTERSET 字符集,默认是WE8MSWIN1252TOTALMEMORY 实例内存,默认是服务器物理内存的40%]]></content>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] 测试mysql where条件执行顺序对sql查询效率的影响]]></title>
    <url>%2F2018%2F10%2F30%2F%E6%B5%8B%E8%AF%95mysql%20where%E6%9D%A1%E4%BB%B6%E9%A1%BA%E5%BA%8F%E5%AF%B9sql%E6%9F%A5%E8%AF%A2%E6%95%88%E7%8E%87%E7%9A%84%E5%BD%B1%E5%93%8D%2F</url>
    <content type="text"><![CDATA[1、创建基表测试数据12345678CREATE TABLE `test1` ( `id` int(7) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=37 DEFAULT CHARSET=utf8mb4;insert into test1(name,age) values(&apos;lucy&apos;,10),(&apos;bobo&apos;,18),(&apos;david&apos;,20),(&apos;tom&apos;,21),(&apos;dobu&apos;,22),(&apos;dali&apos;,12); 2、创建中间表123456 CREATE TABLE `testfororder` ( `id` int(7) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=14699990 DEFAULT CHARSET=utf8mb4; 创建存储过程对中间表插入数据：1234567891011121314drop procedure if exists test;delimiter //create procedure test() begin declare i int; set i=0; while i&lt;300000 do insert into testfororder(newid,name,age) select concat(i,name),FLOOR(18 + (RAND() * 12)) from test1; set i=i+1; end while; end// delimiter ;调用存储过程：call test(); 3、创建测试表：123456789CREATE TABLE `sequence` ( `id` int(10) NOT NULL AUTO_INCREMENT, `newid` int(10) NOT NULL, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=89179437 DEFAULT CHARSET=utf8mb4;insert into sequence(newid,name,age) select * from testfororder; 数据量根据自己需求可以多次导入 4、查看sequence测试表数据：1234567select count(*) from sequence;+----------+| count(*) |+----------+| 12600000 |+----------+1 row in set (2.83 sec) age条件筛选数据量1234567select count(*) from sequence where age in (26,19,22,20,28,29,25);+----------+| count(*) |+----------+| 7341992 |+----------+1 row in set (3.82 sec) newid条件筛选数量1234567select count(*) from sequence where newid in (116670,116677,116684,116691,116698,116705,116719,116726);+----------+| count(*) |+----------+| 56 |+----------+1 row in set (3.28 sec) 5、where条件(age,newid)在没有创建索引的情况下：age在前，newid在后，查看执行计划：1234567891011121314151617explain select * from sequence where age in (26,19,22,20,28,29,25) and newid in (116670,116677,116684,116691,116698,116705,116719,116726);+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+| 1 | SIMPLE | sequence | NULL | ALL | NULL | NULL | NULL | NULL | 12292293 | 25.00 | Using where |+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+1 row in set, 1 warning (0.01 sec)select * from sequence where age in (26,19,22,20,28,29,25) and newid in (116670,116677,116684,116691,116698,116705,116719,116726);+----------+--------+-----------+------+| id | newid | name | age |+----------+--------+-----------+------+| 100003 | 116670 | 2381lucy | 26 |...| 76679487 | 116719 | 2382lucy | 29 |+----------+--------+-----------+------+49 rows in set (5.36 sec) newid在前，age在后：1234567891011121314151617explain select * from sequence where newid in (116670,116677,116684,116691,116698,116705,116719,116726) and age in (26,19,22,20,28,29,25);+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+| 1 | SIMPLE | sequence | NULL | ALL | NULL | NULL | NULL | NULL | 12292293 | 25.00 | Using where |+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+1 row in set, 1 warning (0.00 sec)select * from sequence where newid in (116670,116677,116684,116691,116698,116705,116719,116726) and age in (26,19,22,20,28,29,25);+----------+--------+-----------+------+| id | newid | name | age |+----------+--------+-----------+------+| 100003 | 116670 | 2381lucy | 26 |...| 76679487 | 116719 | 2382lucy | 29 |+----------+--------+-----------+------+49 rows in set (4.47 sec) 没有任何索引的情况下，where后面跟的条件从左到右，返回数据越小的条件在前面，效率会优于返回数据较大的条件在前面。 6、where条件(age)在没有创建索引，newid创建索引的情况下：创建newid的索引：1create index newid_ind on sequence(newid); age在前，newid在后：1234567891011121314151617explain select * from sequence where age in (26,19,22,20,28,29,25) and newid in (116670,116677,116684,116691,116698,116705,116719,116726);+----+-------------+----------+------------+-------+---------------+-----------+---------+------+------+----------+------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+-------+---------------+-----------+---------+------+------+----------+------------------------------------+| 1 | SIMPLE | sequence | NULL | range | newid_ind | newid_ind | 4 | NULL | 56 | 50.00 | Using index condition; Using where |+----+-------------+----------+------------+-------+---------------+-----------+---------+------+------+----------+------------------------------------+1 row in set, 1 warning (0.01 sec)select * from sequence where age in (26,19,22,20,28,29,25) and newid in (116670,116677,116684,116691,116698,116705,116719,116726);+----------+--------+-----------+------+| id | newid | name | age |+----------+--------+-----------+------+| 100003 | 116670 | 2381lucy | 26 |...| 76679487 | 116719 | 2382lucy | 29 |+----------+--------+-----------+------+49 rows in set (0.01 sec) newid在前，age在后：1234567891011121314151617explain select * from sequence where newid in (116670,116677,116684,116691,116698,116705,116719,116726) and age in (26,19,22,20,28,29,25);+----+-------------+----------+------------+-------+---------------+-----------+---------+------+------+----------+------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+-------+---------------+-----------+---------+------+------+----------+------------------------------------+| 1 | SIMPLE | sequence | NULL | range | newid_ind | newid_ind | 4 | NULL | 56 | 50.00 | Using index condition; Using where |+----+-------------+----------+------------+-------+---------------+-----------+---------+------+------+----------+------------------------------------+1 row in set, 1 warning (0.01 sec)select * from sequence where newid in (116670,116677,116684,116691,116698,116705,116719,116726) and age in (26,19,22,20,28,29,25);+----------+--------+-----------+------+| id | newid | name | age |+----------+--------+-----------+------+| 100003 | 116670 | 2381lucy | 26 |...| 76679487 | 116719 | 2382lucy | 29 |+----------+--------+-----------+------+49 rows in set (0.00 sec) 在newid创建索引的情况下，where后面的查询条件前后顺序对查询效率影响不大。 7、where条件(newid)在没有创建索引，age创建索引的情况下：删除newid的索引：1drop index newid_ind on sequence; 创建age的索引：1create index age_ind on sequence(age); age在前，newid在后：1234567891011121314151617explain select * from sequence where age in (26,19,22,20,28,29,25) and newid in (116670,116677,116684,116691,116698,116705,116719,116726);+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+| 1 | SIMPLE | sequence | NULL | ALL | age_ind | NULL | NULL | NULL | 12292293 | 50.00 | Using where |+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+1 row in set, 1 warning (0.00 sec)select * from sequence where age in (26,19,22,20,28,29,25) and newid in (116670,116677,116684,116691,116698,116705,116719,116726);+----------+--------+-----------+------+| id | newid | name | age |+----------+--------+-----------+------+| 100003 | 116670 | 2381lucy | 26 |...| 76679487 | 116719 | 2382lucy | 29 |+----------+--------+-----------+------+49 rows in set (4.97 sec) newid在前，age在后：1234567891011121314151617explain select * from sequence where newid in (116670,116677,116684,116691,116698,116705,116719,116726) and age in (26,19,22,20,28,29,25);+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+| 1 | SIMPLE | sequence | NULL | ALL | age_ind | NULL | NULL | NULL | 12292293 | 50.00 | Using where |+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+1 row in set, 1 warning (0.00 sec)select * from sequence where newid in (116670,116677,116684,116691,116698,116705,116719,116726) and age in (26,19,22,20,28,29,25);+----------+--------+-----------+------+| id | newid | name | age |+----------+--------+-----------+------+| 100003 | 116670 | 2381lucy | 26 |...| 76679487 | 116719 | 2382lucy | 29 |+----------+--------+-----------+------+49 rows in set (4.82 sec) 创建age索引之后，执行计划会默认使用索引执行，但是因为age的筛选出来的数据量比较大，使用索引也不是特别理想。 8、where条件(newid,age)创建联合索引的情况下：删除age索引，创建newid，age的联合索引：12drop index age_ind on sequence;create index newid_age_ind on sequence(newid,age); age在前，newid在后：12345678910111213141516explain select * from sequence where age in (26,19,22,20,28,29,25) and newid in (116670,116677,116684,116691,116698,116705,116719,116726);+----+-------------+----------+------------+-------+---------------+---------------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+-------+---------------+---------------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | sequence | NULL | range | newid_age_ind | newid_age_ind | 9 | NULL | 98 | 100.00 | Using index condition |+----+-------------+----------+------------+-------+---------------+---------------+---------+------+------+----------+-----------------------+ select * from sequence where age in (26,19,22,20,28,29,25) and newid in (116670,116677,116684,116691,116698,116705,116719,116726);+----------+--------+-----------+------+| id | newid | name | age |+----------+--------+-----------+------+| 100003 | 116670 | 2381lucy | 26 |...| 76679487 | 116719 | 2382lucy | 29 |+----------+--------+-----------+------+49 rows in set (0.00 sec) newid在前，age在后：1234567891011121314151617explain select * from sequence where newid in (116670,116677,116684,116691,116698,116705,116719,116726) and age in (26,19,22,20,28,29,25);+----+-------------+----------+------------+-------+---------------+---------------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+-------+---------------+---------------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | sequence | NULL | range | newid_age_ind | newid_age_ind | 9 | NULL | 98 | 100.00 | Using index condition |+----+-------------+----------+------------+-------+---------------+---------------+---------+------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec)select * from sequence where newid in (116670,116677,116684,116691,116698,116705,116719,116726) and age in (26,19,22,20,28,29,25);+----------+--------+-----------+------+| id | newid | name | age |+----------+--------+-----------+------+| 100003 | 116670 | 2381lucy | 26 |...| 76679487 | 116719 | 2382lucy | 29 |+----------+--------+-----------+------+49 rows in set (0.01 sec) 联合索引存在的情况下，where后面的查询条件前后顺序对查询效率影响不大，但是创建了联合索引之后要注意查询条件问题，如果是以（newid，age）的顺序创建的联合索引，如果where查询条件后面有newid会使用联合索引，但是如果where查询条件后面之后age则不会引用联合索引：123456789101112131415explain select * from sequence where newid in (116670,116677,116684,116691,116698,116705,116719,116726);+----+-------------+----------+------------+-------+---------------+---------------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+-------+---------------+---------------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | sequence | NULL | range | newid_age_ind | newid_age_ind | 4 | NULL | 56 | 100.00 | Using index condition |+----+-------------+----------+------------+-------+---------------+---------------+---------+------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec)explain select * from sequence where age in (26,19,22,20,28,29,25);+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+| 1 | SIMPLE | sequence | NULL | ALL | NULL | NULL | NULL | NULL | 12292293 | 50.00 | Using where |+----+-------------+----------+------------+------+---------------+------+---------+------+----------+----------+-------------+1 row in set, 1 warning (0.01 sec)]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] 测试mysql or和in对sql查询效率的影响]]></title>
    <url>%2F2018%2F10%2F30%2F%E6%B5%8B%E8%AF%95mysql%20or%E5%92%8Cin%E5%AF%B9sql%E6%9F%A5%E8%AF%A2%E6%95%88%E7%8E%87%E7%9A%84%E5%BD%B1%E5%93%8D%2F</url>
    <content type="text"><![CDATA[1、创建基表测试数据12345678CREATE TABLE `test1` ( `id` int(7) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=37 DEFAULT CHARSET=utf8mb4;insert into test1(name,age) values(&apos;lucy&apos;,10),(&apos;bobo&apos;,18),(&apos;david&apos;,20),(&apos;tom&apos;,21),(&apos;dobu&apos;,22),(&apos;dali&apos;,12); 2、创建中间表123456 CREATE TABLE `testfororder` ( `id` int(7) NOT NULL AUTO_INCREMENT, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=14699990 DEFAULT CHARSET=utf8mb4; 创建存储过程对中间表插入数据：1234567891011121314drop procedure if exists test;delimiter //create procedure test() begin declare i int; set i=0; while i&lt;300000 do insert into testfororder(newid,name,age) select concat(i,name),FLOOR(18 + (RAND() * 12)) from test1; set i=i+1; end while; end// delimiter ;调用存储过程：call test(); 3、创建测试表：123456789CREATE TABLE `sequence` ( `id` int(10) NOT NULL AUTO_INCREMENT, `newid` int(10) NOT NULL, `name` varchar(20) DEFAULT NULL, `age` int(4) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=89179437 DEFAULT CHARSET=utf8mb4;insert into sequence(newid,name,age) select * from testfororder; 数据量根据自己需求可以多次导入 4、查看sequence测试表数据：1234567select count(*) from sequence;+----------+| count(*) |+----------+| 12600000 |+----------+1 row in set (2.83 sec) 5、在newid没有索引的情况下，in和or的对比：12345678910111213141516171819repl@db 16:14: [aaaa]&gt; select * from sequence where newid in (116670,116677,116684,116691,116698,116705,116719,116726);+----------+--------+-----------+------+| id | newid | name | age |+----------+--------+-----------+------+| 100003 | 116670 | 2381lucy | 26 |...| 76679494 | 116726 | 2382bobo | 21 |+----------+--------+-----------+------+56 rows in set (4.47 sec)select * from sequence where newid =116670 or newid = 116677 or newid = 116684 or newid =116691 or newid = 116698 or newid=116705 or newid=116719 or newid=116726;+----------+--------+-----------+------+| id | newid | name | age |+----------+--------+-----------+------+| 100003 | 116670 | 2381lucy | 26 |...| 76679494 | 116726 | 2382bobo | 21 |+----------+--------+-----------+------+56 rows in set (6.77 sec) 没有索引的情况下使用in查询效率高于or 6、创建newid索引，再次进行测试：1234567891011121314151617181920 create index newid_ind on sequence(newid);select * from sequence where newid in (116670,116677,116684,116691,116698,116705,116719,116726);+----------+--------+-----------+------+| id | newid | name | age |+----------+--------+-----------+------+| 100003 | 116670 | 2381lucy | 26 |...| 76679494 | 116726 | 2382bobo | 21 |+----------+--------+-----------+------+56 rows in set (0.00 sec)select * from sequence where newid =116670 or newid = 116677 or newid = 116684 or newid =116691 or newid = 116698 or newid=116705 or newid=116719 or newid=116726;+----------+--------+-----------+------+| id | newid | name | age |+----------+--------+-----------+------+| 100003 | 116670 | 2381lucy | 26 |...| 76679494 | 116726 | 2382bobo | 21 |+----------+--------+-----------+------+56 rows in set (0.01 sec) 在newid存在索引的情况下，使用or和in查询对sql查询效率影响较小。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] Old incarnation found while trying to add node]]></title>
    <url>%2F2018%2F10%2F28%2FOld%20incarnation%20found%20while%20trying%20to%20add%20node%2F</url>
    <content type="text"><![CDATA[0、集群环境介绍1234mysql-mgr单主集群：mw-mysql-1：primary（节点1）mw-mysql-2：second （节点2）mw-mysql-3：second （节点3） 1、收到zabbix报警mysql-mgr集群有一个节点down，查看节点信息，内容如下12345678select * from performance_schema.replication_group_members;+---------------------------+--------------------------------------+-------------+-------------+--------------+| CHANNEL_NAME | MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE |+---------------------------+--------------------------------------+-------------+-------------+--------------+| group_replication_applier | 77039511-8e42-11e8-b6d4-000d3aa1a189 | mw-mysql-1 | 3306 | ONLINE || group_replication_applier | 81143c99-8e3d-11e8-a501-000d3aa1b575 | mw-mysql-2 | 3306 | ONLINE || group_replication_applier | 8bab920f-8e3d-11e8-a045-000d3aa09b70 | mw-mysql-3 | 3306 | UNREACHABLE |+---------------------------+--------------------------------------+-------------+-------------+--------------+ 因为还有两个节点节点存活，表示集群还是可用的。（后面才发现只依靠这个信息是错误的） 2、节点3集群状态变为不可达，查看节点3的日志：1234567 waited (count) when Workers occupied = 590 waited when Workers occupied = 114127953002018-10-27T06:56:10.116159Z 0 [Warning] Plugin group_replication reported: &apos;The member with address mw-mysql-2:3306 has already sent the stable set. Therefore discarding the second message.&apos;xdr_bytes: out of memoryxdr_bytes: out of memoryxdr_bytes: out of memory2018-10-27T06:58:03.284079Z 0 [Note] Plugin group_replication reported: &apos;dispatch_op /export/home/pb2/build/sb_0-27500212-1520171728.22/mysql-5.7.22/rapid/plugin/group_replication/libmysqlgcs/src/bindings/xcom/xcom/xcom_base.c:3810 die_op executed_msg=&#123;53f6a873 47559528 0&#125; delivered_msg=&#123;53f6a873 47559528 0&#125; p-&gt;synode=&#123;53f6a873 47559501 0&#125; p-&gt;delivered_msg=&#123;53f6a873 47559525 0&#125; p-&gt;max_synode=&#123;53f6a873 47559528 1&#125; &apos;2018-10-27T06:58:03.285626Z 0 [Note] Plugin group_replication reported: &apos;dispatch_op /export/home/pb2/build/sb_0-27500212-1520171728.22/mysql-5.7.22/rapid/plugin/group_replication/libmysqlgcs/src/bindings/xcom/xcom/xcom_base.c:3810 die_op executed_msg=&#123;53f6a873 47559528 0&#125; delivered_msg=&#123;53f6a873 47559528 0&#125; p-&gt;synode=&#123;53f6a873 47559501 0&#125; p-&gt;delivered_msg=&#123;53f6a873 47559525 0&#125; p-&gt;max_synode=&#123;53f6a873 47559528 2&#125; &apos; 3、节点3因为内存不足导致节点集群进程down掉，重新启动mysql，尝试将节点3加入集群，提示加入失败：1234567891011121314151617182018-10-27T07:49:40.493156Z 33 [Note] Plugin group_replication reported: &apos;Group communication SSL configuration: group_replication_ssl_mode: &quot;DISABLED&quot;&apos;2018-10-27T07:49:40.493724Z 33 [Note] Plugin group_replication reported: &apos;[GCS] Added automatically IP ranges 10.1.150.12/24,10.1.150.16/24,127.0.0.1/8 to the whitelist&apos;2018-10-27T07:49:40.494212Z 33 [Note] Plugin group_replication reported: &apos;[GCS] Translated &apos;mw-mysql-3&apos; to 10.1.150.16&apos;2018-10-27T07:49:40.494374Z 33 [Warning] Plugin group_replication reported: &apos;[GCS] Automatically adding IPv4 localhost address to the whitelist. It is mandatory that it is added.&apos;2018-10-27T07:49:40.494426Z 33 [Note] Plugin group_replication reported: &apos;[GCS] SSL was not enabled&apos;2018-10-27T07:49:40.494453Z 33 [Note] Plugin group_replication reported: &apos;Initialized group communication with configuration: group_replication_group_name: &quot;9275d4e4-8e42-11e8-b217-000d3aa1a189&quot;; group_replication_local_address: &quot;mw-mysql-3:24901&quot;; group_replication_group_seeds: &quot;mw-mysql-1:24901,mw-mysql-2:24901,mw-mysql-3:24901&quot;; group_replication_bootstrap_group: false; group_replication_poll_spin_loops: 0; group_replication_compression_threshold: 1000000; group_replication_ip_whitelist: &quot;AUTOMATIC&quot;&apos;2018-10-27T07:49:40.494471Z 33 [Note] Plugin group_replication reported: &apos;[GCS] Configured number of attempts to join: 0&apos;2018-10-27T07:49:40.494476Z 33 [Note] Plugin group_replication reported: &apos;[GCS] Configured time between attempts to join: 5 seconds&apos;2018-10-27T07:49:40.494526Z 33 [Note] Plugin group_replication reported: &apos;Member configuration: member_id: 3306101; member_uuid: &quot;8bab920f-8e3d-11e8-a045-000d3aa09b70&quot;; single-primary mode: &quot;true&quot;; group_replication_auto_increment_increment: 1; &apos;2018-10-27T07:49:40.494937Z 94 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_applier&apos; executed&apos;. Previous state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-10-27T07:49:40.545004Z 97 [Note] Slave SQL thread for channel &apos;group_replication_applier&apos; initialized, starting replication in log &apos;FIRST&apos; at position 0, relay log &apos;./relay-log-group_replication_applier.000508&apos; position: 42018-10-27T07:49:40.545034Z 33 [Note] Plugin group_replication reported: &apos;Group Replication applier module successfully initialized!&apos;2018-10-27T07:49:40.545059Z 33 [Note] Plugin group_replication reported: &apos;auto_increment_increment is set to 1&apos;2018-10-27T07:49:40.545063Z 33 [Note] Plugin group_replication reported: &apos;auto_increment_offset is set to 3306101&apos;2018-10-27T07:49:40.545486Z 0 [Note] Plugin group_replication reported: &apos;XCom protocol version: 3&apos;2018-10-27T07:49:40.545513Z 0 [Note] Plugin group_replication reported: &apos;XCom initialized and ready to accept incoming connections on port 24901&apos;2018-10-27T07:49:40.762663Z 0 [Warning] Plugin group_replication reported: &apos;read failed&apos;2018-10-27T07:49:40.780216Z 0 [ERROR] Plugin group_replication reported: &apos;[GCS] The member was unable to join the group. Local port: 24901&apos; 4、因为在节点3查不到任何有用的报错信息，尝试在节点1查看有没有其他报错，看到一条比较奇怪的报错：1[Note] Plugin group_replication reported: &apos;Old incarnation found while trying to add node mw-mysql-3:24901 15406269616484810.&apos; 官方文档没有关于这个信息的任何提示，在这个链接https://dba.stackexchange.com/questions/214779/how-to-delete-previous-incarnation-in-mysql-w-group-replication查到碰到这个问题只能重启集群。 5、查看节点2的信息，error.log没有任何更新，正常情况下如果集群节点正常应该会每个120s左右，会刷新一下信息：1[Note] Multi-threaded slave statistics for channel &apos;group_replication_applier&apos;: seconds elapsed = 131; events assigned = 3687425; worker queues filled over overrun level = 0; waited due a Worker queue full = 0; waited due the total size = 0; waited at clock conflicts = 65388998400 waited (count) when Workers occupied = 25728 waited when Workers occupied = 124142987600 6、查看节点1应用连接是正常的，但是dml操作一直hang主，没有任何结果，重新启动应用程序问题照旧。感觉集群状态虽然查询正常，但是已经不能提供对外服务了，最后决定把所有的库stop，重新集群系统，最后集群恢复正常，对外提供服务正常，查询集群服务正常：12345678select * from performance_schema.replication_group_members;+---------------------------+--------------------------------------+-------------+-------------+--------------+| CHANNEL_NAME | MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE |+---------------------------+--------------------------------------+-------------+-------------+--------------+| group_replication_applier | 77039511-8e42-11e8-b6d4-000d3aa1a189 | mw-mysql-1 | 3306 | ONLINE || group_replication_applier | 81143c99-8e3d-11e8-a501-000d3aa1b575 | mw-mysql-2 | 3306 | ONLINE || group_replication_applier | 8bab920f-8e3d-11e8-a045-000d3aa09b70 | mw-mysql-3 | 3306 | ONLINE |+---------------------------+--------------------------------------+-------------+-------------+--------------+]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] yum安装mysql-client客户端(centos7)]]></title>
    <url>%2F2018%2F10%2F25%2F%E5%AE%89%E8%A3%85mysqlclient%E5%AE%A2%E6%88%B7%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[1、配置yum源123456echo &quot;[mysql80-community]name=MySQL 8.0 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-8.0-community/el/7/\$basearch/enabled=1gpgcheck=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql&quot; &gt; /etc/yum.repos.d/mysql-community.repo 2、查看配置文件是否正确:1more /etc/yum.repos.d/mysql-community.repo 3、安装mysql-client：12yum clean allyum install mysql-community-client]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql数据库使用innobackupex和mysqldump备份恢复的对比]]></title>
    <url>%2F2018%2F10%2F25%2Fmysql%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8innobackupex%E5%92%8Cmysqldump%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D%E7%9A%84%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[1、查看原库数据文件大小：12du -sh *|grep data23G data 2、使用innodbackex备份数据在percona官方下载xtrabackup软件，并解压12wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.9/binary/tarball/percona-xtrabackup-2.4.9-Linux-x86_64.tar.gztar -zxvf percona-xtrabackup-2.4.9-Linux-x86_64.tar.gz 开始执行备份，指定配置文件、用户名、密码、端口、备份的目录1./innobackupex --defaults-file=/etc/my.cnf --user=root --password=12345678 --port=3306 /data/ （总体耗时25分钟左右，备份文件大小23G） 3、采用innodbackex备份出来的数据去恢复,将数据文件拷贝到节点1,执行恢复操作:先把事务日志恢复（–apply-log）1/data/soft/percona-xtrabackup/bin/innobackupex --defaults-file=/etc/my.cnf --user=root --password=12345678 --port=3306 --apply-log /data/2018-09-14_13-34-38 恢复数据之前需要清除数据目录下的所有数据：1rm -rf /data/mysql/data/* 开始恢复数据1/data/soft/percona-xtrabackup/bin/innobackupex --defaults-file=/etc/my.cnf --user=root --password=12345678 --port=3306 --copy-back --rsync /data/2018-09-14_13-34-38 数据恢复完成之后，删除datadir下的事务日志log文件（innodb引擎才会有）12shell &gt; cd /data/mysql/datashell &gt; rm -rf ib_logfile* 设置数据目录权限12shell &gt; chown -R mysql:mysql /data/mysql/datashell &gt; /data/mysql/bin/mysqld_safe --defaults-file=/data/mysql/etc/my.cnf &amp; 启动节点1的mysql1/etc/init.d/mysql restart 恢复总体耗时（35分钟） 4、原库使用mysqldump方法备份一份数据，用于恢复节点2：原库操作1/data/mysql/bin/mysqldump --all-databases --set-gtid-purged=ON --single-transaction -uroot -p12345678 &gt; /db/test.dump 备份需要指定–set-gtid-purged=ON参数在备份文件中输出global.gtid_purged信息，指定–single-transaction参数避免备份过程中产生锁（备份耗时10分钟，备份文件大小11G） 5、将mysqldump备份出的数据拷贝到节点2，节点2进行恢复操作： 如果从库@@GLOBAL.GTID_EXECUTED值不为空需要执行reset master;1234root@db 07:55: [(none)]&gt; stop group_replication;Query OK, 0 rows affected (1.01 sec)root@db 07:55: [(none)]&gt; reset master;Query OK, 0 rows affected (0.05 sec) 开始执行恢复12[root@dax-mysql-mha bin]# mysql -u root -p12345678 &lt; /data/test.dumpmysql: [Warning] Using a password on the command line interface can be insecure. 总体耗时（60分钟左右） 6、关于使用mysqldump备份恢复和innobackupex备份恢复的比较：123数据文件目录大小23G情况下，mysqldump逻辑备份出来数据大小11G，备份耗时十分钟，恢复耗时一小时；innobackupex物理备份备份出来数据大小23G，备份耗时25分钟，恢复耗时35分钟。使用物理备份备份时间比逻辑备份时间较长，但恢复时间会小于逻辑备份时间,数据量比较大的情况下使用innobackupex会更好一点。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql-MGR集群配置]]></title>
    <url>%2F2018%2F10%2F25%2Fmysql-MGR%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前提须知：mgr限制条件12345仅支持InnoDB表，并且每张表一定要有一个主键，用于做write set的冲突检测；必须打开GTID特性，二进制日志格式必须设置为ROW，用于选主与write setCOMMIT可能会导致失败，类似于快照事务隔离级别的失败场景目前一个MGR集群最多支持9个节点不支持外键于save point特性，无法做全局间的约束检测与部分部分回滚二进制日志不支持binlog event checksum 0、修改每个节点hosts文件，如下面所示(三个节点均需修改)[root@dax-mysql-master log]# cat /etc/hosts12345127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain610.0.7.53 dax-mysql-master10.0.7.50 dax-mysql-slave10.0.7.51 dax-mysql-mha 1、修改三个节点的my.cnf配置文件，在[mysqld]后添加如下信息，local_address这一行配置为自己的ip地址，其他不变123456789101112131415[mysqld]master_info_repository = &quot;TABLE&quot;relay_log_info_repository = &quot;TABLE&quot;transaction_write_set_extraction = &quot;XXHASH64&quot;slave_parallel_workers = 2slave_preserve_commit_order = 1slave_parallel_type = &quot;LOGICAL_CLOCK&quot;binlog_checksum = NONE#group_replication_allow_local_disjoint_gtids_join = ON#group_replication_auto_increment_increment = 1loose-group_replication_group_name = &quot;867fee38-746b-11e8-b006-000d3a800ed3&quot;loose-group_replication_start_on_boot = offloose-group_replication_local_address = &quot;dax-mysql-mha:24901&quot;loose-group_replication_group_seeds = &quot;dax-mysql-master:24901,dax-mysql-slave:24901,dax-mysql-mha:24901&quot;loose-group_replication_bootstrap_group = off 集群参数详解：1234567#group_replication_auto_increment_increment = 1 单主模式下可以设置为1（每个节点都要设置，不设置默认仍然为7）#loose-用于上述group_replication变量 的前缀 指示服务器在服务器启动时尚未加载组复制插件时继续启动。#配置会 transaction_write_set_extraction 指示服务器为每个事务处理它必须收集写入集并使用XXHASH64散列算法将其编码为 散列。#配置会 group_replication_group_name 告诉插件它正在加入或创建的组名为“aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa”。#值 group_replication_group_name 必须是有效的UUID。在二进制日志中为组复制事件设置GTID时，此UUID在内部使用。使用SELECT UUID();生成一个UUID。#配置 group_replication_start_on_boot 指示插件在服务器启动时不自动启动操作。这在设置组复制时很重要，因为它可以确保您可以在手动启动#插件之前配置服务器。成员配置完成后，您可以将其设置 group_replication_start_on_boot 为开启，以便在服务器引导时自动启动组复制。#配置 group_replication_local_address 告诉插件使用网络地址127.0.0.1和端口24901与组中的其他成员进行内部通信。 添加完参数之后重启数据库（三个节点均需重启） 2、主节点1(dax-mysql-master)执行：1234567891011mysql&gt; SET SQL_LOG_BIN=0;CREATE USER repl@&apos;%&apos; IDENTIFIED BY &apos;12345678&apos;;GRANT REPLICATION SLAVE ON *.* TO repl@&apos;%&apos;;FLUSH PRIVILEGES;SET SQL_LOG_BIN=1;CHANGE MASTER TO MASTER_USER=&apos;repl&apos;, MASTER_PASSWORD=&apos;12345678&apos; FOR CHANNEL &apos;group_replication_recovery&apos;;#安装组复制插件INSTALL PLUGIN group_replication SONAME &apos;group_replication.so&apos;;#查看插件是否安装成功SHOW PLUGINS; 插件安装完成之后，将配置文件注释的那两行#去掉，重启数据库，并启动组复制进程：123456#启动组复制SET GLOBAL group_replication_bootstrap_group=ON;START GROUP_REPLICATION;SET GLOBAL group_replication_bootstrap_group=OFF;#查看组成员SELECT * FROM performance_schema.replication_group_members; 2.1、创建数据，可在添加其他节点之前加，或者之后加（测试在节点之前加，添加其他节点后，会自动同步）(该步骤可选择性执行)12345678mysql&gt;CREATE DATABASE test1;USE test;CREATE TABLE t4 (c1 INT PRIMARY KEY, c2 TEXT NOT NULL);INSERT INTO t4 VALUES (1, &apos;Luis&apos;);检查表t1和二进制日志的内容。SELECT * FROM t4;SHOW BINLOG EVENTS; 3、从节点2（dax-mysql-slave）执行:12345678SET SQL_LOG_BIN=0;CREATE USER repl@&apos;%&apos; IDENTIFIED BY &apos;12345678&apos;;GRANT REPLICATION SLAVE ON *.* TO repl@&apos;%&apos;;FLUSH PRIVILEGES;SET SQL_LOG_BIN=1;CHANGE MASTER TO MASTER_USER=&apos;repl&apos;, MASTER_PASSWORD=&apos;12345678&apos; FOR CHANNEL &apos;group_replication_recovery&apos;;INSTALL PLUGIN group_replication SONAME &apos;group_replication.so&apos;;show plugins; 将配置文件注释的那两行#去掉，重启数据库1mysql&gt;START GROUP_REPLICATION; 4、从节点3(dax-mysql-mha)执行:12345678SET SQL_LOG_BIN=0;CREATE USER repl@&apos;%&apos; IDENTIFIED BY &apos;12345678&apos;;GRANT REPLICATION SLAVE ON *.* TO repl@&apos;%&apos;;FLUSH PRIVILEGES;SET SQL_LOG_BIN=1;CHANGE MASTER TO MASTER_USER=&apos;repl&apos;, MASTER_PASSWORD=&apos;12345678&apos; FOR CHANNEL &apos;group_replication_recovery&apos;;INSTALL PLUGIN group_replication SONAME &apos;group_replication.so&apos;;show plugins; 将配置文件注释的那两行#去掉，重启数据库1mysql&gt;START GROUP_REPLICATION; 5、安装mysql-shell(只在主节点执行便可)下载mysql-shell1wget https://dev.mysql.com/get/Downloads/MySQL-Shell/mysql-shell-8.0.11-linux-glibc2.12-x86-64bit.tar.gz MySQL Shell用来配置服务器以供InnoDB集群使用的供应脚本需要访问Python版本2.7。对于沙箱部署，在用于部署的单台机器上需要Python，生产部署需要在每个服务器实例上使用Python。12tar -zxvf mysql-shell-8.0.11-linux-glibc2.12-x86-64bit.tar.gzmv mysql-shell-8.0.11-linux-glibc2.12-x86-64bit mysql-shell 主节点1（dax-mysql-master),连接主节点,授权repl用户集群管理权限1234GRANT ALL PRIVILEGES ON mysql_innodb_cluster_metadata.* TO repl@&apos;%&apos; identified by &apos;12345678&apos; WITH GRANT OPTION;GRANT RELOAD, SHUTDOWN, PROCESS, FILE, SUPER, REPLICATION SLAVE, REPLICATION CLIENT, CREATE USER ON *.* TO repl@&apos;%&apos; identified by &apos;12345678&apos; WITH GRANT OPTION;GRANT SELECT ON *.* TO repl@&apos;%&apos; identified by &apos;12345678&apos; WITH GRANT OPTION;flush privileges; 通过mysql-shell连接到主节点1配置集群:12345678/data/soft/mysql-shell/bin/mysqlsh --uri repl@dax-mysql-master:3306&lt;输入用户名mysql-shell&gt;dba.verbose=2创建集群mysql-shell&gt;var cluster = dba.createCluster(&apos;prodCluster&apos;, &#123;adoptFromGR: true&#125;);查看集群状态mysql-shell&gt;cluster = dba.getCluster(&quot;prodCluster&quot;)mysql-shell&gt;cluster.status() 6、在应用端安装mysql-router123456sudo bashmkdir -p /data/softcd /data/softwget https://dev.mysql.com/get/Downloads/MySQL-Router/mysql-router-8.0.11-linux-glibc2.12-x86-64bit.tar.gztar -zxvf mysql-router-8.0.11-linux-glibc2.12-x86-64bit.tar.gzmv mysql-router-8.0.11-linux-glibc2.12-x86-64bit mysql-router 配置mysqlrouter1/data/soft/mysql-router/bin/mysqlrouter --bootstrap repl@10.5.0.6:3306 --directory /data/soft/myrouter --user=root --conf-use-sockets --force 在/data/soft/myrouter目录下生成关于mysqlrouter的配置文件，用于记录用于连接到mgr集群其中mysqlrouter.conf配置文件内容如下:123456789101112131415161718192021222324252627282930313233343536373839[metadata_cache:prodCluster]router_id=7bootstrap_server_addresses=mysql://dax-mysql-slave:3306,mysql://dax-mysql-master:3306,mysql://dax-mysql-mha:3306user=mysql_router7_umuf7ikos8fpmetadata_cluster=prodClusterttl=5[routing:prodCluster_default_rw]bind_address=0.0.0.0bind_port=6446socket=/tmp/myrouter/mysql.sockdestinations=metadata-cache://prodCluster/default?role=PRIMARYrouting_strategy=round-robinprotocol=classic[routing:prodCluster_default_ro]bind_address=0.0.0.0bind_port=6447socket=/tmp/myrouter/mysqlro.sockdestinations=metadata-cache://prodCluster/default?role=SECONDARYrouting_strategy=round-robinprotocol=classic[routing:prodCluster_default_x_rw]bind_address=0.0.0.0bind_port=64460socket=/tmp/myrouter/mysqlx.sockdestinations=metadata-cache://prodCluster/default?role=PRIMARYrouting_strategy=round-robinprotocol=x[routing:prodCluster_default_x_ro]bind_address=0.0.0.0bind_port=64470socket=/tmp/myrouter/mysqlxro.sockdestinations=metadata-cache://prodCluster/default?role=SECONDARYrouting_strategy=round-robinprotocol=x 启动mysqlrouter命令：12/data/soft/myrouter/start.shnetstat -tunlp|grep mysql 加入开机启动1234echo &quot;rm -rf /data/soft/myrouter/mysqlrouter.pid /data/soft/myrouter/start.sh &quot; &gt;&gt; /etc/rc.localcat /etc/rc.local 授权脚本执行权限12chmod +x /etc/rc.d/rc.localll /etc/rc.d/rc.local ####7、测试连接库是否有问题，并查看连接的库是主还是从连接读写库1mysql -u root -h 127.0.0.1 -P 6446 -p 连接只读库1mysql -u root -h 127.0.0.1 -P 6447 -p 8、查看集群状态查看组成员1SELECT * FROM performance_schema.replication_group_members; 查看主节点1SHOW STATUS LIKE &apos;group_replication_primary_member&apos;; 或者使用如下方法12set global show_compatibility_56 = on;select MEMBER_HOST from performance_schema.replication_group_members where MEMBER_ID=(select VARIABLE_VALUE from information_schema.global_status where variable_name=&apos;GROUP_REPLICATION_PRIMARY_MEMBER&apos;);]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] 一台服务器安装多个mysql数据库程序]]></title>
    <url>%2F2018%2F10%2F25%2F%E4%B8%80%E5%8F%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85%E5%A4%9A%E4%B8%AAmysql%E6%95%B0%E6%8D%AE%E5%BA%93%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[1、已安装第一个mysql程序的相关信息：123端口：3306目录：/data/mysql/配置文件:/etc/my.cnf 2、新建用于第二个mysql程序的目录：1mkdir -p /data/twomysql 将安装包解压到twomysql目录（下载地址https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz）123tar -zxvf mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz -C /data/twomysqlcd /data/twomysqlmv mysql-5.7.22-linux-glibc2.12-x86_64 twomysql 创建相关数据、日志、配置文件目录：12345678910mkdir -p /data/twomysql/twomysql/logtouch /data/twomysql/twomysql/log/mysqld.logchown -R mysql.mysql /data/twomysql/twomysql/logmkdir -p /data/twomysql/twomysql/datachown -R mysql.mysql /data/twomysql/twomysql/datamkdir -p /data/twomysql/twomysql/tmpchown -R mysql.mysql /data/twomysql/twomysql/tmpmkdir -p /data/twomysql/twomysql/etcchown -R mysql.mysql /data/twomysql/twomysql/etccd /data/twomysql/twomysql/etc 配置文件信息如下，端口设为3307，目录设置跟上面创建的目录匹配：vim my.cnf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107[client]port = 3307socket = /data/twomysql/twomysql/tmp/mysql.sock[mysql]prompt=&quot;\u@db \R:\m:\s [\d]&gt; &quot;no-auto-rehash[mysqld]user = mysqlport = 3307basedir = /data/twomysql/twomysqldatadir = /data/twomysql/twomysql/datasocket = /data/twomysql/twomysql/tmp/mysql.sockpid-file = /data/twomysql/twomysql/tmp/mysql.pidcharacter-set-server = utf8mb4skip_name_resolve = 1open_files_limit = 65535back_log = 1024max_connections = 2000max_connect_errors = 100net_read_timeout = 30net_write_timeout = 60slave_net_timeout = 90table_open_cache = 1024table_definition_cache = 1024table_open_cache_instances = 64thread_stack = 512Kexternal-locking = FALSEmax_allowed_packet = 32Msort_buffer_size = 4Mjoin_buffer_size = 4Mthread_cache_size = 768query_cache_size = 0query_cache_type = 0interactive_timeout = 600wait_timeout = 600tmp_table_size = 32Mmax_heap_table_size = 32Mslow_query_log = 1slow_query_log_file = /data/twomysql/twomysql/log/slow.loglog-error = /data/twomysql/twomysql/log/error.loglong_query_time = 0.1server-id = 3306101log-bin = /data/twomysql/twomysql/log/mysql-binlogsync_binlog = 1binlog_cache_size = 4Mmax_binlog_cache_size = 1Gmax_binlog_size = 1Gexpire_logs_days = 7gtid_mode = onenforce_gtid_consistency = 1log_slave_updatesbinlog_format = rowrelay_log_recovery = 1relay-log = relay-logrelay-log-purge = 1key_buffer_size = 32Mread_buffer_size = 8Mread_rnd_buffer_size = 4Mbulk_insert_buffer_size = 64Mlock_wait_timeout = 60explicit_defaults_for_timestamp = 1innodb_thread_concurrency = 0innodb_sync_spin_loops = 100innodb_spin_wait_delay = 30transaction_isolation = REPEATABLE-READinnodb_buffer_pool_size = 2048Minnodb_buffer_pool_instances = 8innodb_buffer_pool_load_at_startup = 1innodb_buffer_pool_dump_at_shutdown = 1innodb_data_file_path = ibdata1:1G:autoextendinnodb_flush_log_at_trx_commit = 1innodb_log_buffer_size = 32Minnodb_log_file_size = 1Ginnodb_log_files_in_group = 3innodb_max_undo_log_size = 4Ginnodb_io_capacity = 4000innodb_io_capacity_max = 8000innodb_flush_neighbors = 0innodb_write_io_threads = 8innodb_read_io_threads = 8innodb_purge_threads = 4innodb_page_cleaners = 4innodb_open_files = 65535innodb_max_dirty_pages_pct = 50innodb_flush_method = O_DIRECTinnodb_lru_scan_depth = 4000innodb_checksum_algorithm = crc32innodb_lock_wait_timeout = 30innodb_rollback_on_timeout = 1innodb_print_all_deadlocks = 1innodb_file_per_table = 1innodb_online_alter_log_max_size = 4Ginternal_tmp_disk_storage_engine = InnoDBinnodb_stats_on_metadata = 0innodb_status_file = 1innodb_status_output = 0innodb_status_output_locks = 0performance_schema = 1performance_schema_instrument = &apos;%=on&apos;lower_case_table_names = 1explicit_defaults_for_timestamp = offsql_mode = STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION[mysqldump]quickmax_allowed_packet = 32M[mysql.server]basedir=/data/twomysql/twomysql 3、初始化数据库123sudo mkdir /data/twomysql/twomysql/mysql-filessudo chown mysql:mysql /data/twomysql/twomysql/mysql-filessudo chmod 750 /data/twomysql/twomysql/mysql-files 设置无密码登录，在后面在重新创建密码：12/data/twomysql/twomysql/bin/mysqld --defaults-file=/data/twomysql/twomysql/etc/my.cnf --initialize-insecure --user=mysql sudo /data/twomysql/twomysql/bin/mysql_ssl_rsa_setup 4、初始化完成之后，启动数据库：1/data/twomysql/twomysql/bin/mysqld_safe --defaults-file=/data/twomysql/twomysql/etc/my.cnf --socket=/data/twomysql/twomysql/tmp/mysql.sock --port=3307 &amp; 5、修改用户密码1/data/twomysql/twomysql/bin/mysqladmin -u root -P 3307 -S /data/twomysql/twomysql/tmp/mysql.sock password 123456 6、登录mysql，需要输入密码：1/data/twomysql/twomysql/bin/mysql --socket=/data/twomysql/twomysql/tmp/mysql.sock --port=3307 -p]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Zabbix] zabbix监控hp-switch]]></title>
    <url>%2F2018%2F10%2F24%2Fhp%E4%BA%A4%E6%8D%A2%E6%9C%BA%E5%90%AF%E7%94%A8snmp%EF%BC%8C%E5%B9%B6%E6%B7%BB%E5%8A%A0%E5%88%B0zabbix%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[1、通过console口、telnet、ssh连接到switch的命令行，登录具有管理员权限的用户。使用system-view进入特权模式1# system-view 2、配置snmp服务，设置版本、联系人信息、设备位置、团体名12345# snmp-agent# snmp-agent sys-info version v2c# snmp-agent sys-info contact Zamasu &lt;zamasu@dbsuper.com&gt;# snmp-agent sys-info location Universe10 - IT Room# snmp-agent community read GokuBlack 3、保存配置信息：1#save 4、保存完成之后hp交换机snmp服务已经配置完成，下面测试zabbix服务器和hp交换机之间的snmp团体名。在zabbix安装snmp测试snmp服务：12# yum install snmp(apt-get install snmp)# snmpwalk -v2c -c GokuBlack 192.168.0.200 192.168.0.200为hp交换机的地址下面为snmpwalk输出结果：1234567SNMPv2-MIB::sysDescr.0 = STRING: HPE V1910-48G Switch Software Version 5.20, Release 1519P03Copyright(c) 2010-2017 Hewlett Packard Enterprise Development, L.P.SNMPv2-MIB::sysObjectID.0 = OID: SNMPv2-SMI::enterprises.25506.11.1.85DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: (197804302) 22 days, 21:27:23.02SNMPv2-MIB::sysContact.0 = STRING: Zamasu &lt;zamasu@dbsuper.com&gt;SNMPv2-MIB::sysName.0 = STRING: TECH-SW01SNMPv2-MIB::sysLocation.0 = STRING: Universe10 - IT Room 5、配置zabbix仪表板：创建主机下面配置snmp团体名用于zabbix连接hp交换机根据对应的交换机型号，去zabbix官网下载对应得模板：http://www.zabbix.org/wiki/Zabbix_Templates，点击[跳转](http://www.zabbix.org/wiki/Zabbix_Templates)]]></content>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Oracle] oracle 定期删除trace文件]]></title>
    <url>%2F2018%2F10%2F22%2Foracle%20%E5%AE%9A%E6%9C%9F%E5%88%A0%E9%99%A4trace%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[oracle 定期删除trace文件cat del_trace.sh1234567891011121314151617181920212223242526272829303132333435363738#!/bin/bash#oracle 11gR2 rdbms#声明oracle环境变量export ORACLE_BASE=/u01/app/oracleexport ORACLE_HOME=$ORACLE_BASE/product/11.2.0.4/dbhome_1#声明主机名export HOST_NAME=`hostname`#声明日期，用于删除几天前的文件export DAYS=7#一台服务器装有多个实例，可使用循环进行删除for dbname in instancea instancebdoexport DB_NAME=$dbnameexport DB_UNIQUE_NAME=$&#123;DB_NAME&#125;export ORACLE_SID=$&#123;DB_NAME&#125;#当前实例为rac模式节点1，启用该变量#export ORACLE_SID=$&#123;DB_NAME&#125;1#当前实例为rac模式节点2，启用该变量#export ORACLE_SID=$&#123;DB_NAME&#125;2#指定存放日志的路径DIAGNOSTIC_DEST=$&#123;ORACLE_BASE&#125;#如果audit_trail_dest的一个默认值不生效，会使用下面第二个默认值：AUDIT_FILE_DEST=$&#123;ORACLE_BASE&#125;/admin/$&#123;DB_NAME&#125;/adump#删除audit_trail_dest第一个默认值下的审计日志，并删除7天前的日志find $&#123;ORACLE_HOME&#125;/rdbms/audit -name &quot;$&#123;ORACLE_SID&#125;_*.aud&quot; -mtime +$&#123;DAYS&#125; -exec rm -f &#123;&#125; \;#如果audit_trail_dest的一个默认值不生效，查找第二个默认值下的审计日志，并删除7天前的日志：find $&#123;AUDIT_FILE_DEST&#125; -name &quot;$&#123;ORACLE_SID&#125;_*.aud&quot; -mtime +$&#123;DAYS&#125; -exec rm -f &#123;&#125; \;#删除指定路径alert下的文件find $&#123;DIAGNOSTIC_DEST&#125;/diag/rdbms/$&#123;DB_UNIQUE_NAME&#125;/$&#123;ORACLE_SID&#125;/alert -name &quot;log_[0-9]*.xml&quot; -mtime +$&#123;DAYS&#125; -exec rm -f &#123;&#125; \;#删除指定路径trace下的文件find $&#123;DIAGNOSTIC_DEST&#125;/diag/rdbms/$&#123;DB_UNIQUE_NAME&#125;/$&#123;ORACLE_SID&#125;/trace -name &quot;$&#123;ORACLE_SID&#125;_*.tr[c|m]&quot; -mtime +$&#123;DAYS&#125; -exec rm -f &#123;&#125; \;find $&#123;DIAGNOSTIC_DEST&#125;/diag/rdbms/$&#123;DB_UNIQUE_NAME&#125;/$&#123;ORACLE_SID&#125;/trace -name &quot;cdmp_*&quot; -mtime +$&#123;DAYS&#125; -exec rm -rf &#123;&#125; \;done 将该脚本，添加到cron下每日定期删除]]></content>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Oracle] oracle-audit_file_dest和audit_trail参数详解]]></title>
    <url>%2F2018%2F10%2F22%2Foracle-audit_file_dest%E5%92%8Caudit_trail%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1、AUDIT_FILE_DEST参数：当AUDIT_TRAIL初始化参数被设置为os, xml, or xml,extended，AUDIT_FILE_DEST指定操作系统目录，用于记录跟踪审计。AUDIT_FILE_DEST的第一个默认值为：1ORACLE_BASE/admin/ORACLE_SID/adump 如果第一个值指定的目录不存在或者该值不可用，会启用第二个默认值：1ORACLE_HOME/rdbms/audit 这两个默认值只适用于unix系统，其他平台系统默认值不同。在多租户cdb模式下,默认值会被追加pdb的guid，用于存储属于pdb的审计记录。例如，pdb的guid为03E1F908EE04252CE053B280E80AAAA3，第一个默认目录是:1ORACLE_BASE/admin/ORACLE_SID/adump/03E1F908EE04252CE053B280E80AAAA3 关于pdb的guid可以通过V$CONTAINERS视图参看。 如果AUDIT_TRAIL参数设置为xml, or xml,extended，审计记录会以xml的格式存储。如果AUDIT_SYS_OPERATIONS参数启用，用于审计sys用户的记录，审计信息也会强制写入这个位置。在多租户容器数据库，audit_file_dest的值设置范围是cdb。虽然审计记录是每个pdb提供的，但是每个pdb不能修改该初始化参数。 2、audit_trail参数value值详解：none：禁用数据库审计os：开启数据库审计，并将所有审计记录定向到操作系统的审计跟踪。db：开启数据库审计，并将所有审计记录定向到数据库审计跟踪（sys.aud$)db, extended:开启数据库审计，并将所有审计记录定向到数据库审计跟踪（sys.aud$),另外，sys.aud$表中类型为clob的两列（sqlbind和sqltext会被写入数据）xml：开启数据库审计，并将所有审计记录以xml格式写入操作系统文件xml,extended:开启数据库审计，并打印所有的审计记录列，包括SqlText和SqlBind字段]]></content>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Linux] linux区分当前服务器是物理机还是虚拟机方法]]></title>
    <url>%2F2018%2F10%2F18%2Flinux%E5%8C%BA%E5%88%86%E5%BD%93%E5%89%8D%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%98%AF%E7%89%A9%E7%90%86%E6%9C%BA%E8%BF%98%E6%98%AF%E8%99%9A%E6%8B%9F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[1、查看系统产品名称：dmidecode -s system-product-name物理机：Azure微软云服务器aws亚马逊云服务器阿里云服务器使用vmware建立的虚拟机使用kvm建立的虚拟机 2、查看scsi供应商：cat /proc/scsi/scsi|grep Vendor物理机：Azure微软云服务器aws亚马逊云服务器（看不到任何信息）阿里云服务器使用vmware建立的虚拟机使用kvm建立的虚拟机 3、查看内核缓冲信息：dmesg | grep -i virtual物理机：Azure微软云服务器aws亚马逊云服务器阿里云服务器使用vmware建立的虚拟机使用kvm建立的虚拟机]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql忘记root密码，修改密码方法]]></title>
    <url>%2F2018%2F10%2F18%2Fmysql%E5%BF%98%E8%AE%B0root%E5%AF%86%E7%A0%81%EF%BC%8C%E4%BF%AE%E6%94%B9%E5%AF%86%E7%A0%81%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1、修改my.cnf文件，在[mysqld]下添加’skip-grant-tables’参数：cat /usr/local/mysql/etc/my.cnf12345678[mysqld]!include /usr/local/mysql/etc/mysqld.cnfport = 3306basedir = /usr/local/mysql/socket = /usr/local/mysql/tmp/mysql.sockpid-file = /usr/local/mysql/var/mysql.piddatadir = /usr/local/mysql/var/skip-grant-tables 2、重启mysql进程：1/etc/init.d/mysql restart 3、进入mysql控制台，修改mysql密码：12345678mysql&gt; USE mysql ; Database changedmysql&gt; UPDATE user SET Password = password ( &apos;12345678&apos; ) WHERE User = &apos;root&apos; ; Query OK, 1 row affected (0.06 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; flush privileges ; Query OK, 0 rows affected (0.00 sec)mysql&gt; quit 4、注释掉my.cnf配置文件下的skip-grant-tables这行参数，重新启动mysql：1/etc/init.d/mysql restart 5、测试密码是否修改成功,能否正常登陆控制台：1mysql -u root -p12345678]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] ERROR 1805 (HY000): Column count of mysql.user is wrong. Expected 45, found 43. The table is probably corrupted]]></title>
    <url>%2F2018%2F10%2F17%2Fmysql--The%20table%20is%20probably%20corrupted%2F</url>
    <content type="text"><![CDATA[1、从mysql5.6 msyqldump全备出来数据，导入到mysql5.7的库，导入导出均正常，在5.7的库新增用户时，提示以下错误：12root@db 09:21: [(none)]&gt; grant select,update,delete,insert on *.* to &apos;clevergo&apos;@&apos;%&apos; identified by &apos;123123&apos;;ERROR 1805 (HY000): Column count of mysql.user is wrong. Expected 45, found 43. The table is probably corrupted 2、造成该问题是因为把5.6的库数据，全部导入5.7造成的，因此需要使用mysql_upgrade将库升级到最新版本：/data/mysql/bin/mysql_upgrade -uroot -p12345612345678910111213141516171819202122232425262728293031323334353637383940414243mysql_upgrade: [Warning] Using a password on the command line interface can be insecure.Checking if update is needed.Checking server version.Running queries to upgrade MySQL server.Checking system database.mysql.columns_priv OKmysql.db OKmysql.engine_cost OKmysql.event OKmysql.func OKmysql.general_log OKmysql.gtid_executed OKmysql.help_category OKmysql.help_keyword OKmysql.help_relation OKmysql.help_topic OKmysql.innodb_index_stats OKmysql.innodb_table_stats OKmysql.ndb_binlog_index OKmysql.plugin OKmysql.proc OKmysql.procs_priv OKmysql.proxies_priv OKmysql.server_cost OKmysql.servers OKmysql.slave_master_info OKmysql.slave_relay_log_info OKmysql.slave_worker_info OKmysql.slow_log OKmysql.tables_priv OKmysql.time_zone OKmysql.time_zone_leap_second OKmysql.time_zone_name OKmysql.time_zone_transition OKmysql.time_zone_transition_type OKmysql.user OKThe sys schema is already up to date (version 1.5.1).Found 0 sys functions, but expected 22. Re-installing the sys schema.Upgrading the sys schema.Checking databases.t.t_test OKUpgrade process completed successfully.Checking if update is needed. 3、进入mysql，再次执行创建用户：12345678910111213141516171819root@db 09:26: [(none)]&gt; grant select,update,delete,insert on *.* to &apos;clevergo&apos;@&apos;%&apos; identified by &apos;123123&apos;;Query OK, 0 rows affected, 1 warning (0.01 sec)root@db 09:26: [(none)]&gt; flush privileges;Query OK, 0 rows affected (0.01 sec)root@db 09:26: [(none)]&gt; select host,user from mysql.user;+-----------+----------------+| host | user |+-----------+----------------+| % | clevergo || localhost | mysql.session || localhost | mysql.sys || localhost | root |+-----------+----------------+6 rows in set (0.00 sec)root@db 09:26: [(none)]&gt; exitBye 执行成功。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] 使用mysqldiff和mysqldbcompare检查数据一致性]]></title>
    <url>%2F2018%2F10%2F08%2F%E4%BD%BF%E7%94%A8mysqldiff%E5%92%8Cmysqldbcompare%E6%AF%94%E8%BE%83%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B7%AE%E5%BC%82%2F</url>
    <content type="text"><![CDATA[1、官网下载mysql-utilities工具1wget https://cdn.mysql.com/archives/mysql-utilities/mysql-utilities-1.6.5.tar.gz 2、解压mysql-utilities工具：1234567tar -zxvf mysql-utilities-1.6.5.tar.gzcd mysql-utilities-1.6.5/cd scripts/[root@dax-mysql-slave scripts]# lsmysqlauditadmin.py mysqlbinlogpurge.py mysqldbcopy.py mysqldiff.py mysqlfrm.py mysqlmetagrep.py mysqlrpladmin.py mysqlrplshow.py mysqlserverinfo.py mysqluserclone.pymysqlauditgrep.py mysqlbinlogrotate.py mysqldbexport.py mysqldiskusage.py mysqlgrants.py mysqlprocgrep.py mysqlrplcheck.py mysqlrplsync.py mysqlslavetrx.pymysqlbinlogmove.py mysqldbcompare.py mysqldbimport.py mysqlfailover.py mysqlindexcheck.py mysqlreplicate.py mysqlrplms.py mysqlserverclone.py mysqluc.py 在scripts目录下存在mysqldiff和mysqldbcompare用于比对数据的脚本 3、使用mysqldbcompare需要安装connector-python依赖关系：官网下载1wget https://cdn.mysql.com/archives/mysql-connector-python-2.2/mysql-connector-python-2.2.3-0.1.el7.x86_64.rpm 安装1rpm -ivh mysql-connector-python-2.2.3-0.1.el7.x86_64.rpm 4、安装完成之后测试mysqldbcompare能否正常使用： ./mysqldbcompare.py1234Traceback (most recent call last): File &quot;./mysqldbcompare.py&quot;, line 28, in &lt;module&gt; from mysql.utilities.common.tools import check_python_versionImportError: No module named utilities.common.tools 将python2.7下的模块，软连接到lib64目录下：1ln -s /usr/lib/python2.7/site-packages/mysql/utilities /usr/lib64/python2.7/site-packages/mysql/utilities 重新测试mysqldbcompare能否正常使用：./mysqldbcompare.py123Usage: mysqldbcompare --server1=user:pass@host:port:socket --server2=user:pass@host:port:socket db1:db2mysqldbcompare: error: You must specify at least one database to compare or use the --all option to compare all databases. 提示需要指定对表的库：./mysqldbcompare.py –server1=root:12345678@10.0.7.50:3306:/data/mysql/tmp/mysql.sock –server2=root:12345678@10.0.7.51:3306:/data/mysql/tmp/mysql.sock test:test或者./mysqldbcompare.py –server1=root:12345678@10.0.7.50:3306 –server2=root:12345678@10.0.7.51:3306 test:test执行结果：123456# WARNING: Using a password on the command line interface can be insecure.# server1 on 10.0.7.50: ... connected.# server2 on 10.0.7.51: ... connected.# Checking databases test on server1 and test on server2#ERROR: The list of objects differs among database test and test. 5、准备测试数据：server1：1234567891011121314151617181920212223root@db 03:40: [test1]&gt; select * from test;+----+| ID |+----+| 1 || 2 || 3 || 4 || 5 || 6 |+----+6 rows in set (0.00 sec)root@db 03:40: [test1]&gt; show create table test;+-------+-------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+-------------------------------------------------------------------------------------------------------------+| test | CREATE TABLE `test` ( `ID` int(11) NOT NULL, PRIMARY KEY (`ID`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+-------+-------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) server2:12345678910111213141516171819202122root@db 03:37: [test1]&gt; select * from test;+----+| ID |+----+| 1 || 2 || 3 || 4 || 5 |+----+root@db 06:20: [test1]&gt; show create table test -&gt; ;+-------+-------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+-------------------------------------------------------------------------------------------------------------+| test | CREATE TABLE `test` ( `ID` int(11) NOT NULL, PRIMARY KEY (`ID`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+-------+-------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 6、mysqldiff用于检测表结构的差异，如果某个对比表表结构相同，数据不同，mysqldiff并不会检测出来：./mysqldiff.py –server1=root:12345678@10.0.7.50:3306 –server2=root:12345678@10.0.7.51:3306 test1:test11234567# WARNING: Using a password on the command line interface can be insecure.# server1 on 10.0.7.50: ... connected.# server2 on 10.0.7.51: ... connected.# Comparing `test1` to `test1` [PASS]# Comparing `test1`.`aa` to `test1`.`aa` [PASS]# Comparing `test1`.`test` to `test1`.`test` [PASS]# Success. All objects are the same. 单独对比某一张表：./mysqldiff.py –server1=root:12345678@10.0.7.50:3306 –server2=root:12345678@10.0.7.51:3306 test1.test:test1.test12345# WARNING: Using a password on the command line interface can be insecure.# server1 on 10.0.7.50: ... connected.# server2 on 10.0.7.51: ... connected.# Comparing test1.test to test1.test [PASS]# Success. All objects are the same. 7、结构一致，数据内容不一致，使用mysqldbcompare（用于检测数据库字符集，表结构，表数据等）检测：./mysqldbcompare.py –server1=root:12345678@10.0.7.50:3306 –server2=root:12345678@10.0.7.51:3306 test1 –changes-for=server1 –difftype=sql123456789101112# WARNING: Using a password on the command line interface can be insecure.# server1 on 10.0.7.50: ... connected.# server2 on 10.0.7.51: ... connected.# Checking databases test1 on server1 and test1 on server2## Defn Row Data # Type Object Name Diff Count Check # ------------------------------------------------------------------------- # TABLE aa pass pass - # - Compare table checksum pass # TABLE test pass FAIL ERROR: Row counts are not the same among `test1`.`test` and `test1`.`test`.# 提示test1库下面的test表数据不一致。 8、创建测试数据，表结构不同，数据内容相同：server1：123456789101112create table test1 (id int(10) primary key);insert into test1 values (1),(2),(3),(4),(5);select * from test1;+----+| id |+----+| 1 || 2 || 3 || 4 || 5 |+----+ server2:123456789101112create table test1 (id bigint(20));insert into test1 values (1),(2),(3),(4),(5);root@db 06:28: [test1]&gt; select * from test1;+------+| id |+------+| 1 || 2 || 3 || 4 || 5 |+------+ 9、使用mysqldiff检测： ./mysqldiff.py –server1=root:12345678@10.0.7.50:3306 –server2=root:12345678@10.0.7.51:3306 test1.test1:test1.test112345678910111213141516# WARNING: Using a password on the command line interface can be insecure.# server1 on 10.0.7.50: ... connected.# server2 on 10.0.7.51: ... connected.# Comparing test1.test1 to test1.test1 [FAIL]# Object definitions differ. (--changes-for=server1)#--- test1.test1+++ test1.test1@@ -1,4 +1,3 @@ CREATE TABLE `test1` (- `id` int(10) NOT NULL,- PRIMARY KEY (`id`)+ `id` bigint(20) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4# Compare failed. One or more differences found. 提示表结构存在差异。 10、使用mysqldbcompare检测：./mysqldbcompare.py –server1=root:12345678@10.0.7.50:3306 –server2=root:12345678@10.0.7.51:3306 test1 –changes-for=server1 –difftype=sql123456789101112# WARNING: Using a password on the command line interface can be insecure.# server1 on 10.0.7.50: ... connected.# server2 on 10.0.7.51: ... connected.# Checking databases test1 on server1 and test1 on server2## Defn Row Data # Type Object Name Diff Count Check # ------------------------------------------------------------------------- # TABLE aa pass pass - # - Compare table checksum pass # TABLE test pass FAIL ERROR: Row counts are not the same among `test1`.`test` and `test1`.`test`.# 当检测到异常之后会直接退出，不进行下面的比较，因此未检测到test1表的内容，加上-t（-t, –run-all-tests do not abort when a diff test fails）参数，运行测试模式，遇到异常之后仍然执行下面的比较：./mysqldbcompare.py –server1=root:12345678@10.0.7.50:3306 –server2=root:12345678@10.0.7.51:3306 test1 –changes-for=server1 –difftype=sql -t123456789101112131415161718192021222324252627282930313233343536373839# WARNING: Using a password on the command line interface can be insecure.# server1 on 10.0.7.50: ... connected.# server2 on 10.0.7.51: ... connected.# Checking databases test1 on server1 and test1 on server2## Defn Row Data # Type Object Name Diff Count Check # ------------------------------------------------------------------------- # TABLE aa pass pass - # - Compare table checksum pass # TABLE test pass FAIL - # - Compare table checksum FAIL # - Find row differences FAIL ## Row counts are not the same among `test1`.`test` and `test1`.`test`.## Transformation for --changes-for=server1:#DELETE FROM `test1`.`test` WHERE `ID` = &apos;6&apos;;# TABLE test1 FAIL pass - # - Compare table checksum FAIL # - Find row differences SKIP ## Transformation for --changes-for=server1:#ALTER TABLE `test1`.`test1` DROP PRIMARY KEY, CHANGE COLUMN id id bigint(20) NULL;# The table test1 does not have an usable Index or primary key.# Database consistency check failed.## ...done 11、创建两个test库，server1比server2实例多一个gtid_test14的表：server1:12345678910111213141516171819202122show tables;+----------------+| Tables_in_test |+----------------+| articles || gtid_test10 || gtid_test11 || gtid_test12 || gtid_test13 || gtid_test14 || gtid_test15 || gtid_test2 || gtid_test3 || gtid_test4 || gtid_test5 || gtid_test6 || gtid_test7 || gtid_test8 || gtid_test9 || ratings |+----------------+16 rows in set (0.00 sec) server2:123456789101112131415161718192021show tables;+----------------+| Tables_in_test |+----------------+| articles || gtid_test10 || gtid_test11 || gtid_test12 || gtid_test13 || gtid_test15 || gtid_test2 || gtid_test3 || gtid_test4 || gtid_test5 || gtid_test6 || gtid_test7 || gtid_test8 || gtid_test9 || ratings |+----------------+15 rows in set (0.00 sec) 12、使用mysqldiff测试： ./mysqldiff.py –server1=root:12345678@10.0.7.50:3306 –server2=root:12345678@10.0.7.51:3306 test:test123456# WARNING: Using a password on the command line interface can be insecure.# server1 on 10.0.7.50: ... connected.# server2 on 10.0.7.51: ... connected.# WARNING: Objects in server1.test but not in server2.test:# TABLE: gtid_test14# Compare failed. One or more differences found. 提示server2.test库下不存在gtid_test14表。 13、使用mysqldbcompare测试： ./mysqldbcompare.py –server1=root:12345678@10.0.7.50:3306 –server2=root:12345678@10.0.7.51:3306 test123456# WARNING: Using a password on the command line interface can be insecure.# server1 on 10.0.7.50: ... connected.# server2 on 10.0.7.51: ... connected.# Checking databases test on server1 and test on server2#ERROR: The list of objects differs among database test and test. 只会提示两个实例的test库有差异，并没有列出详细信息。在对比数据时可先使用mysqldiff工具检测两个server端的表结构是否一致，如果没有差异，再使用mysqldbcompare工具去检测表数据是否一致。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Linux] 使用certbot为域名生成免费证书(apache版)]]></title>
    <url>%2F2018%2F09%2F28%2F%E4%BD%BF%E7%94%A8certbot%E4%B8%BA%E5%9F%9F%E5%90%8D%E7%94%9F%E6%88%90%E5%85%8D%E8%B4%B9%E8%AF%81%E4%B9%A6(apache%E7%89%88)%2F</url>
    <content type="text"><![CDATA[1、下载certbot123cd /data/softwget https://dl.eff.org/certbot-autochmod a+x certbot-auto 2、生成证书/data/soft/certbot-auto –apache certonly12345Saving debug log to /var/log/letsencrypt/letsencrypt.logCould not choose appropriate plugin: The apache plugin is not working; there may be problems with your existing configuration.The error was: NoInstallationError(&apos;Cannot find Apache executable apachectl&apos;,)The apache plugin is not working; there may be problems with your existing configuration.The error was: NoInstallationError(&apos;Cannot find Apache executable apachectl&apos;,) 3、上面报错提示找不到执行路径，需要指定apache的路径sudo env PATH=$PATH:/usr/local/apache2/bin ./certbot-auto –apache certonly12345Saving debug log to /var/log/letsencrypt/letsencrypt.logCould not choose appropriate plugin: The apache plugin is not working; there may be problems with your existing configuration.The error was: NoInstallationError(&apos;Could not find configuration root&apos;,)The apache plugin is not working; there may be problems with your existing configuration.The error was: NoInstallationError(&apos;Could not find configuration root&apos;,) 4、上面报错提示找不到配置目录，需要指定–apache-server-root sudo env PATH=$PATH:/usr/local/apache2/bin ./certbot-auto –apache –apache-server-root /usr/local/apache2123456789Saving debug log to /var/log/letsencrypt/letsencrypt.logPlugins selected: Authenticator apache, Installer apacheNo names were found in your configuration files. Please enter in your domainname(s) (comma and/or space separated) (Enter &apos;c&apos; to cancel): www.test.comObtaining a new certificatePerforming the following challenges:http-01 challenge for www.test.comCleaning up challengesUnable to find a virtual host listening on port 80 which is currently needed for Certbot to prove to the CA that you control your domain. Please add a virtual host for port 80. 5、使用certbot申请申请域名免费证书，默认会访问80端口，如果80端口不存在，会报以上错误，修改httpd.conf配置文件，添加上80端口，并重启apache12345678Listen 80&lt;VirtualHost *:80&gt; ServerAdmin test@test.example.com ServerName www.test.com ServerAlias test DocumentRoot /var/www/html &lt;/VirtualHost&gt; 6、重新生成证书，成功之后会在/etc/letsencrypt/live/ebank.cbibank.com目录下生成四个文件.pem文件和一个README文件1cert.pem chain.pem fullchain.pem privkey.pem README 7、修改conf/httpd.conf文件12#Include conf/extra/httpd-ssl.conf#LoadModule ssl_module modules/mod_ssl.so 将这两行的#去掉 8、配置conf/extra/httpd-ssl.conf文件，修改对应的域名和证书路径：12345678&lt;VirtualHost *:443&gt;DocumentRoot &quot;/var/www/html&quot;ServerName ebank.cbibank.comSSLEngine onSSLCertificateFile /etc/letsencrypt/live/ebank.cbibank.com/cert.pemSSLCertificateKeyFile /etc/letsencrypt/live/ebank.cbibank.com/privkey.pemSSLCertificateChainFile /etc/letsencrypt/live/ebank.cbibank.com/chain.pem&lt;/VirtualHost&gt; 9、修改完成后重启apache：1/usr/local/apache2/bin/apachectl restart 重启过程报错，无法关闭apache提示以下错误：1httpd: Syntax error on line 434 of /usr/local/apache2/conf/httpd.conf: Cannot load /usr/local/apache2/modules/mod_ssl.so into server: /usr/local/apache2/modules/mod_ssl.so: cannot open shared object file: No such file or directory 在/usr/lib64/下面没有httpd的模块，yum安装mod_ssl：1yum install mod_ssl 安装完成之后在/usr/lib64/httpd/modules/下面会有mod_ssl.so12/usr/lib64/httpd/modules/mod_ssl.soln -s /usr/lib64/httpd/modules/mod_ssl.so /usr/local/apache2/modules/mod_ssl.so 再次尝试重启apache，报错：1httpd: Syntax error on line 434 of /usr/local/apache2/conf/httpd.conf: Cannot load /usr/local/apache2/modules/mod_ssl.so into server: /usr/local/apache2/modules/mod_ssl.so:undefined symbol: ap_global_mutex_create google了一下，有说yum安装的mod_ssl与apache的安装版本不兼容的问题，因此尝试使用对应版本的tar包将模块文件拷过去:拷贝modules目录下的ssl目录和loggers的内容到/usr/local/apache2/modules/ssl目录下、拷贝include目录下的内容到/usr/local/apache2/modules/ssl目录下，拷贝完之后，在/usr/local/apache2/modules/ssl目录下执行以下命令：1/usr/local/apache2/bin/apxs -a -i -c mod_ssl.c 执行完成之后再次重启apache，依旧报错:1httpd: Syntax error on line 434 of /usr/local/apache2/conf/httpd.conf: Cannot load /usr/local/apache2/modules/mod_ssl.so into server: /usr/local/apache2/modules/mod_ssl.so: undefined symbol: ssl_cmd_SSLPassPhraseDialog 需要指定openssl路径，执行以下命令：1/usr/local/apache2/bin/apxs -a -i -c -L/usr/lib/openssl/engines/lib -c *.c -lcrypto -lssl -ldl 再次重启apache1httpd: Syntax error on line 434 of /usr/local/apache2/conf/httpd.conf: Cannot load /usr/local/apache2/modules/mod_ssl.so into server: /usr/local/apache2/modules/mod_ssl.so:undefined symbol: ap_global_mutex_create 重启apache依旧报错undefinedsymbol:ap_global_mutex_create，没找到任何解决办法，最后只能添加-enable-ssl参数，重新编译安装apache。]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql勿操作drop table之后，利用mysqldump备份和binlog恢复]]></title>
    <url>%2F2018%2F09%2F26%2Fmysql%20%E5%8B%BF%E6%93%8D%E4%BD%9Cdrop%20table%E4%B9%8B%E5%90%8E%EF%BC%8C%E5%88%A9%E7%94%A8mysqldump%E5%A4%87%E4%BB%BD%E5%92%8Cbinlog%E5%8E%BB%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[1、查看备份表数据：12345678root@db 07:30: [test1]&gt; select * from test;+----+| ID |+----+| 1 || 2 || 3 |+----+ 2、查看当前binlog位置12345678910show master status;+---------------------+----------+--------------+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------------+----------+--------------+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| mysql-binlog.000036 | 80147 | | | 8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-195:1000036-1000040 |+---------------------+----------+--------------+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 3、开始备份test1数据库下的test数据表1mysqldump -uroot -p12345678 test1 test --master-data=2 --single-transaction &gt; /data/test.dump 4、查看备份数据文件：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869 more /data/test.dump-- MySQL dump 10.13 Distrib 5.7.22, for linux-glibc2.12 (x86_64)---- Host: localhost Database: test1-- -------------------------------------------------------- Server version 5.7.22-log/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;/*!40101 SET NAMES utf8 */;/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;/*!40103 SET TIME_ZONE=&apos;+00:00&apos; */;/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=&apos;NO_AUTO_VALUE_ON_ZERO&apos; */;/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;SET @MYSQLDUMP_TEMP_LOG_BIN = @@SESSION.SQL_LOG_BIN;SET @@SESSION.SQL_LOG_BIN= 0;---- GTID state at the beginning of the backup --SET @@GLOBAL.GTID_PURGED=&apos;8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-195:1000036-1000040&apos;;---- Position to start replication or point-in-time recovery from---- CHANGE MASTER TO MASTER_LOG_FILE=&apos;mysql-binlog.000036&apos;, MASTER_LOG_POS=80147;---- Table structure for table `test`--DROP TABLE IF EXISTS `test`;/*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `test` ( `ID` int(11) NOT NULL, PRIMARY KEY (`ID`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;/*!40101 SET character_set_client = @saved_cs_client */;---- Dumping data for table `test`--LOCK TABLES `test` WRITE;/*!40000 ALTER TABLE `test` DISABLE KEYS */;INSERT INTO `test` VALUES (1),(2),(3);/*!40000 ALTER TABLE `test` ENABLE KEYS */;UNLOCK TABLES;SET @@SESSION.SQL_LOG_BIN = @MYSQLDUMP_TEMP_LOG_BIN;/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;-- Dump completed on 2018-09-25 7:32:05 5、备份完成之后插入新的数据12root@db 07:34: [test1]&gt; insert into test values(4);Query OK, 1 row affected (0.01 sec) 6、刷新binlog文件12root@db 07:44: [test1]&gt; flush binary logs;Query OK, 0 rows affected (0.04 sec) 7、查看binlog文件12345678910root@db 07:44: [test1]&gt; show master status;+---------------------+----------+--------------+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------------+----------+--------------+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| mysql-binlog.000037 | 326 | | | 8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-196:1000036-1000040 |+---------------------+----------+--------------+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 8、在新的binlog文件里面再次插入一条数据：12root@db 07:44: [test1]&gt; insert into test values(5);Query OK, 1 row affected (0.01 sec) 9、查看当前测试的数据1234567891011root@db 07:45: [test1]&gt; select * from test;+----+| ID |+----+| 1 || 2 || 3 || 4 || 5 |+----+5 rows in set (0.00 sec) 10、再次刷新binlog文件12345678910111213root@db 07:45: [test1]&gt; flush binary logs;Query OK, 0 rows affected (0.04 sec)root@db 07:49: [test1]&gt; show master status;+---------------------+----------+--------------+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+---------------------+----------+--------------+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| mysql-binlog.000038 | 326 | | | 8182e5ae-af54-11e8-af0e-000d3a801ae2:1-13253,c42e3372-ba21-11e8-99ed-000d3a800ed3:1-2,d240752c-b809-11e8-8947-000d3a800ed3:1,dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:1-197:1000036-1000040 |+---------------------+----------+--------------+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 11、删除测试表12root@db 07:49: [test1]&gt; drop table test;Query OK, 0 rows affected (0.04 sec) 12、根据备份文件获取测试表test的创建语句1sed -e&apos;/./&#123;H;$!d;&#125;&apos; -e &apos;x;/CREATE TABLE `test`/!d;q&apos; /data/test.dump 12345678DROP TABLE IF EXISTS `test`;/*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `test` ( `ID` int(11) NOT NULL, PRIMARY KEY (`ID`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;/*!40101 SET character_set_client = @saved_cs_client */; 13、获取测试表test的数据，并指定到数据文件1grep &apos;INSERT INTO `test`&apos; /data/test.dump &gt; insert.sql cat insert.sql1INSERT INTO `test` VALUES (1),(2),(3); 14、根据test.dump备份文件记录的log-file文件位置和log-pos参数，去获取未备份的关于test表的增删改数据：从mysql-binlog.000036文件开始，–start-position=80147：1mysqlbinlog -v --base64-output=decode-rows --set-charset=UTF-8 --database=test1 --start-position=80147 mysql-binlog.000036 &gt; restore.sql 15、获取test表被删除是的pos，文件为mysql-binlog.000038，位置为507：1mysqlbinlog -v --base64-output=DECODE-ROWS --set-charset=UTF-8 /data/mysql/log/mysql-binlog.000038 |grep DROP -A15 -B15 12345678910111213141516171819202122232425# at 326#180925 7:49:39 server id 3306101 end_log_pos 387 GTID last_committed=0 sequence_number=1 rbr_only=noSET @@SESSION.GTID_NEXT= &apos;dd412cc2-ba1f-11e8-9ba2-000d3a801ae2:198&apos;/*!*/;# at 387#180925 7:49:39 server id 3306101 end_log_pos 507 Query thread_id=267 exec_time=0 error_code=0use `test1`/*!*/;SET TIMESTAMP=1537861779/*!*/;SET @@session.pseudo_thread_id=267/*!*/;SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/;SET @@session.sql_mode=0/*!*/;SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=29301/*!*/;/*!\C utf8 *//*!*/;SET @@session.character_set_client=33,@@session.collation_connection=33,@@session.collation_server=45/*!*/;SET @@session.lc_time_names=0/*!*/;SET @@session.collation_database=DEFAULT/*!*/;DROP TABLE `test` /* generated by server *//*!*/;SET @@SESSION.GTID_NEXT= &apos;AUTOMATIC&apos; /* added by mysqlbinlog */ /*!*/;DELIMITER ;# End of log file/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/; 16、在备份开始binlog文件和记录drop操作的binlog文件之间还存在一个mysql-binlog.000037文件，需要将该文件内记录的信息都导出：1mysqlbinlog -v --base64-output=DECODE-ROWS --set-charset=UTF-8 mysql-binlog.000037 &gt;&gt; restore.sql 17：将记录drop操作的binlog文件里面drop之前的信息导出：1mysqlbinlog -v --base64-output=DECODE-ROWS --set-charset=UTF-8 --stop-position=507 mysql-binlog.000038 &gt;&gt; restore.sql 18:对导出的文件进行筛选，过滤出test表的相关信息：1more restore.sql |grep --ignore-case -E &apos;insert|update|delete&apos; -A3|grep &apos;`test1`.`test`&apos; -A2 1234567### INSERT INTO `test1`.`test`### SET### @1=4--### INSERT INTO `test1`.`test`### SET### @1=5 将过滤出来的内容进行编辑，@1为第一列的列名，建议使用sublime或者vim进行批量编辑 ：cat test_insert.sql12INSERT INTO `test1`.`test` SET id=4 ;INSERT INTO `test1`.`test` SET id=5 ; 19、执行create table语句，并引用insert.sql和test_insert.sql文件，至此drop掉的test表被恢复。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Cassandra] cassandra安装部署及日常使用命令]]></title>
    <url>%2F2018%2F09%2F14%2Fcassandra%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[1、添加yum源vim /etc/yum.repos.d/cassandra.repo123456[cassandra]name=Apache Cassandrabaseurl=https://www.apache.org/dist/cassandra/redhat/311x/gpgcheck=1repo_gpgcheck=1gpgkey=https://www.apache.org/dist/cassandra/KEYS 2、安装cassandra数据库1sudo yum install cassandra -y 3、修改配置文件/etc/cassandra/conf/cassandra.yaml12345678910cluster_name: &apos;My Cluster&apos;hints_directory: /data/cassandra/hintsdata_file_directories: - /data/cassandra/dbdatacommitlog_directory: /data/cassandra/commitlogsaved_caches_directory: /data/cassandra/caches - seeds: &quot;10.10.8.3,10.10.8.4,10.10.8.5&quot;listen_address: dax-mysql-mharpc_address: dax-mysql-mha 每个节点需要修改的基本参数：12345cluster_name:修改集群名称修改存放目录hint、data、logseeds:添加节点iplisten_address：修改为本地ip地址rpc_address：修改为本地ip地址 配置文件部分参数解释：12345678910111213141516cluster_name: &apos;Test Cluster&apos;storage_port: 7000listen_address: dax-mysql-mhastart_native_transport: true #开启native协议 native_transport_port: 9042 #客户端的交互端口 data_file_directories: - /data/cassandra/dbdata # 数据位置，多盘的话可以写多个目录 commitlog_directory: - /data/cassandra/commitlog #commitlog的路径，与data目录分开磁盘，提高性能saved_caches_directory: - /data/cassandra/caches #缓存数据目录 commitlog_sync: batch #批量记录commitlog，每隔一段时间将数据commitlogcommitlog_sync_batch_window_in_ms: 2 #batch模式下，批量操作缓存的时间间隔#commitlog_sync: periodic #周期记录commitlog，每一次有数据更新都commitlog#commitlog_sync_period_in_ms: 10000 #periodic模式，刷新commitlog的时间间隔 rpc_address: dax-mysql-mha 4、创建配置文件内指定的相关目录：12345mkdir -p /data/cassandra/hintsmkdir -p /data/cassandra/dbdatamkdir -p /data/cassandra/commitlogmkdir -p /data/cassandra/cacheschmod 777 -R /data/cassandra 5、启动数据库，并添加到开机启动：12service cassandra startchkconfig cassandra on 查看运行状态1nodetool status 进入控制台,9042端口为默认控制台端口，可自行修改配置文件设置：1cqlsh ip 9042 6、如果没有修改cluster-name启动该数据库，之后想要修改cluster-name需要使用下面方法去修改：1234567进入控制台：UPDATE system.local SET cluster_name = &apos;daxcluster&apos; where key=&apos;local&apos;;bash $ ./nodetool flush修改cassandra.yaml配置文件cluster配置参数cluster_name: &apos;daxcluster&apos;重启cassandra/etc/init.d/cassandra restart 7、给数据库设置用户名密码1）、首先修改配置文件 cassandra.yaml把默认的authenticator: AllowAllAuthenticator运行所有人登录设置为用密码登录：1authenticator: PasswordAuthenticator 2）、登录cassandra创建用户使用默认账户登录cassandra1cqlsh -ucassandra -pcassandra ip 9042 创建用户1CREATE USER test WITH PASSWORD &apos;test&apos; SUPERUSER; 3）、使用新用户登录1cqlsh -utest -ptest ip 9042 删除默认帐号：1DROP USER cassandra; 4）、java使用用户名密码访问cassandra1234Cluster cluster = Cluster.builder().addContactPoint("192.168.22.161").withCredentials("myusername", "mypassword").build(); 8、账号权限分配命令123456789101112授权：GRANT permission_name PERMISSION ON resource TO user_name;GRANT ALL PERMISSIONS ON resource TO user_name;收回权限：REVOKE permission_name PERMISSION ON resource FROM user_name;REVOKE ALL PERMISSIONS ON resource FROM user_name;查看权限：LIST permission_name PERMISSION ON resource OF user_name NORECURSIVE;LIST ALL PERMISSIONS ON resource OF user_name NORECURSIVE;其中，permission_name为： ALL/ALTER/AUTHORIZE/CREATE/DROP/MODIFY/SELECTresource为：ALL KEYSPACES/KEYSPACE keyspace_name/TABLE keyspace_name.table_name 9、其他使用命令：123456789101112131415161718desc KEYSPACEs枚举所有数据库list users;查看所有用户show version;显示当前cqlsh,cassandra，cql spec,native protocol 版本信息show host;显示当前集群节点的ip地址和端口SHOW SESSION &lt;session id&gt;跟踪一个会话信息SOURCE &apos;/home/thobbs/commands.cql&apos;读取文件内容，执行cql语句CAPTURE &apos;&lt;file&gt;&apos;;将会话查询内容指定到一个文件内CAPTURE OFF;停止将查询结果指定到文件内，使其正常输出到屏幕上CAPTURE;查看当前抓取信息是否指定到什么目录下 10、如果nodetool decommission命令将某节点提出,执行结束之后，如果需要把节点再重新加入集群，需要把数据目录下数据删除掉，重启cassandra服务。]]></content>
      <tags>
        <tag>Cassandra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mgr从节点down恢复过程遇到的问题]]></title>
    <url>%2F2018%2F09%2F14%2Fmysql%20mgr%E4%BB%8E%E8%8A%82%E7%82%B9down%E6%81%A2%E5%A4%8D%E8%BF%87%E7%A8%8B%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1、mgr集群有节点down掉，查看日志一台服务器（3）因网络问题与其他两个节点（1,2）断掉，3节点被踢出了集群123 [Warning] Plugin group_replication reported: &apos;Member with address wallet-mysql-2:3306 has become unreachable.&apos;[Warning] Plugin group_replication reported: &apos;Member with address wallet-mysql-1:3306 has become unreachable.&apos; [ERROR] Plugin group_replication reported: &apos;Member was expelled from the group due to network failures, changing member status to ERROR.&apos; 2、登录节点3尝试重新启动group_replication：12mysql&gt;stop group_replicaiton;mysql&gt;start group_replication; 3、查看集群成员状态SELECT * FROM performance_schema.replication_group_members;1234567+---------------------------+--------------------------------------+----------------+-------------+--------------+| CHANNEL_NAME | MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE |+---------------------------+--------------------------------------+----------------+-------------+--------------+| group_replication_applier | 6a41c0b6-8e3d-11e8-b076-000d3aa05296 | wallet-mysql-2 | 3306 | ONLINE || group_replication_applier | 71540a17-8e3d-11e8-8cee-000d3aa1d767 | wallet-mysql-1 | 3306 | ONLINE || group_replication_applier | 7dd813bf-8e3d-11e8-8d92-000d3aa1c31e | wallet-mysql-3 | 3306 | RECOVERING |+---------------------------+--------------------------------------+----------------+-------------+--------------+ 4、节点3正在进行恢复，与主库同步数据，但是恢复过程error.log出现了以下问题：12[ERROR] Error reading packet from server for channel &apos;group_replication_recovery&apos;: The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires. (server_errno=1236)[ERROR] Slave I/O for channel &apos;group_replication_recovery&apos;: Got fatal error 1236 from master when reading data from binary log: &apos;The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires.&apos;, Error_code: 1236 提示无法读取主库的binary log 5、尝试去在另一个节点从库2上，全量dump一份数据，在3节点上应用并重新启动mgr：5.1、节点2操作：1shell&gt;/data/mysql/bin/mysqldump --all-databases --set-gtid-purged=ON --single-transaction -uroot -P3306 -p &gt; /tmp/alldb.sql 拷贝alldb.sql到节点3的/tmp目录 5.2、节点3操作：1234mysql&gt;stop group_replication;mysql&gt;reset master;mysql&gt;set global read_only=0;mysql&gt;source /tmp/alldb.sql 重新启动mysql数据库1/etc/init.d/mysql restart 将节点3加入mgr集群：1mysql&gt;start group_replication; 查看集群成员状态，集群状态恢复：123456789mysql&gt;SELECT * FROM performance_schema.replication_group_members;+---------------------------+--------------------------------------+----------------+-------------+--------------+| CHANNEL_NAME | MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE |+---------------------------+--------------------------------------+----------------+-------------+--------------+| group_replication_applier | 6a41c0b6-8e3d-11e8-b076-000d3aa05296 | wallet-mysql-2 | 3306 | ONLINE || group_replication_applier | 71540a17-8e3d-11e8-8cee-000d3aa1d767 | wallet-mysql-1 | 3306 | ONLINE || group_replication_applier | 7dd813bf-8e3d-11e8-8d92-000d3aa1c31e | wallet-mysql-3 | 3306 | ONLINE |+---------------------------+--------------------------------------+----------------+-------------+--------------+3 rows in set (0.00 sec)]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] Multi-threaded slave statistics for channel]]></title>
    <url>%2F2018%2F09%2F13%2FMulti-threaded%20slave%20statistics%20for%20channel%20'group_replication_applier'%2F</url>
    <content type="text"><![CDATA[0、从库error日志提示信息如下：1[Note] Multi-threaded slave statistics for channel &apos;group_replication_applier&apos;: seconds elapsed = 267; events assigned = 10241; worker queues filled over overrun level = 0; waited due a Worker queue full = 0; waited due the total size = 0; waited at clock conflicts = 981623300 waited (count) when Workers occupied = 1238 waited when Workers occupied = 6762438500 1、mysqlerror-log出现上述提示信息是因为启用了mts（Multi-threaded slave）需要启用slave_parallel_workers参数（默认值为0，最大值为1024），并且log_warning（该参数将于v8.0.3去除，被log_error_verbosity 替代）参数要大于1，在error_log里面会有上述提示。123456789seconds elapsed 就是上一次统计跟这一次统计的时间间隔。events assigned：总共有多少个event被分配执行，计的是总数。worker queues filled over overrun level：mts在所有的并行workers之间倾向于加载平衡的时间。slave_parrllel_workers参数决定workers数量。这个统计参数显示了当前线程承受的饱和等级。如果以一个并行线程序列趋近与饱和，这个数会递增，线程复制时间会被推迟，避免达到线程序列限制。Waited due to a Worker queue full：因为worker队列爆满，协调线程必须等待该统计参数会增长Waited due to the total size:该参数代表因为达到了可用内存的限制，worker队列持有未应用事件造成协调线程睡眠的次数。如果这个值持续增长，需要增大slave_pending_jobs_size_max值来避免协调线程等待时间。slave_pending_jobs_size_max：此变量代表用于保存尚未应用的事件的从worker队列的最大内存量（以字节为单位），如果没有启动mts，修改该参数不会有任何效果。（v8.0.11之前默认值为16M,v8.0.12默认值为128M，最小值为1024，最大值为16eib）Waited at clock conflicts:在事务之间存在依赖的情况下，该参数显示等待时间相当于冲突检测和解决方案的逻辑时间。Waited (count) when used occupied:协调进程监控worker足额（enough）分配的统计次数。enough定义取决于调度类型（基于每个库和时钟）Waited when workers occupied:对任何可用worker计算协调线程等待的次数，仅适用于提交时钟调度程序。 参考文档]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql切割慢日志]]></title>
    <url>%2F2018%2F09%2F12%2Fmysql%20%E5%88%87%E5%89%B2%E6%85%A2%E6%97%A5%E5%BF%97%E5%8F%A6%E9%99%84%E5%AE%9A%E6%9C%9F%E5%88%87%E5%89%B2%E6%97%A5%E5%BF%97%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[0、去查看一个测试库的慢日志文件发现有11g的大小，根本没有办法使用mysqldumpslow去查询，因此想要先对日志进行切割：12345split几个主要参数：-b 分割后的文档大小，单位是byte-C 分割后的文档，单行最大byte数-d 使用数字作为后缀，同时使用-a length指定后缀长度-l 分割后文档的行数 1、使用split，按大小切割日志文件，每个文件1G大小，切割后的文件名前缀为test.log，不指定后缀会使用默认后缀名aa,ab,ac….等split slow.log -b 1G test.log123456789101112ls -ltrh test.log*-rw-r--r-- 1 root root 1.0G Sep 12 03:08 test.logaa-rw-r--r-- 1 root root 1.0G Sep 12 03:09 test.logab-rw-r--r-- 1 root root 1.0G Sep 12 03:10 test.logac-rw-r--r-- 1 root root 1.0G Sep 12 03:11 test.logad-rw-r--r-- 1 root root 1.0G Sep 12 03:12 test.logae-rw-r--r-- 1 root root 1.0G Sep 12 03:13 test.logaf-rw-r--r-- 1 root root 1.0G Sep 12 03:14 test.logag-rw-r--r-- 1 root root 1.0G Sep 12 03:15 test.logah-rw-r--r-- 1 root root 1.0G Sep 12 03:16 test.logai-rw-r--r-- 1 root root 1.0G Sep 12 03:17 test.logaj-rw-r--r-- 1 root root 270M Sep 12 03:17 test.logak 2、如果需要指定后缀可使用-d -a 参数（n）代表指定附加后缀的个数，比如切割test.logaa日志文件：split test.logaa -b 100M -d -a 2 logaa123456789101112 ll logaa*-rw-r--r-- 1 root root 100M Sep 12 09:03 logaa00-rw-r--r-- 1 root root 100M Sep 12 09:03 logaa01-rw-r--r-- 1 root root 100M Sep 12 09:03 logaa02-rw-r--r-- 1 root root 100M Sep 12 09:03 logaa03-rw-r--r-- 1 root root 100M Sep 12 09:03 logaa04-rw-r--r-- 1 root root 100M Sep 12 09:03 logaa05-rw-r--r-- 1 root root 100M Sep 12 09:03 logaa06-rw-r--r-- 1 root root 100M Sep 12 09:03 logaa07-rw-r--r-- 1 root root 100M Sep 12 09:03 logaa08-rw-r--r-- 1 root root 100M Sep 12 09:04 logaa09-rw-r--r-- 1 root root 24M Sep 12 09:04 logaa10 split test.logab -b 100M -d -a 4 logab123456789101112ls -ltrh logab*-rw-r--r-- 1 root root 100M Sep 12 09:05 logab0000-rw-r--r-- 1 root root 100M Sep 12 09:05 logab0001-rw-r--r-- 1 root root 100M Sep 12 09:05 logab0002-rw-r--r-- 1 root root 100M Sep 12 09:05 logab0003-rw-r--r-- 1 root root 100M Sep 12 09:05 logab0004-rw-r--r-- 1 root root 100M Sep 12 09:05 logab0005-rw-r--r-- 1 root root 100M Sep 12 09:05 logab0006-rw-r--r-- 1 root root 100M Sep 12 09:05 logab0007-rw-r--r-- 1 root root 100M Sep 12 09:06 logab0008-rw-r--r-- 1 root root 100M Sep 12 09:06 logab0009-rw-r--r-- 1 root root 24M Sep 12 09:06 logab0010 4、按行切割日志文件，查看准备切割的文件有多少行：12cat logab0001 |wc -l505 按每个文件60行切割split logab0001 -l 60 logab00011234567891011ll logab0001*-rw-r--r-- 1 root root 104857600 Sep 12 09:05 logab0001-rw-r--r-- 1 root root 11922880 Sep 12 09:10 logab0001aa-rw-r--r-- 1 root root 12506422 Sep 12 09:10 logab0001ab-rw-r--r-- 1 root root 12506026 Sep 12 09:10 logab0001ac-rw-r--r-- 1 root root 12506635 Sep 12 09:10 logab0001ad-rw-r--r-- 1 root root 12506065 Sep 12 09:10 logab0001ae-rw-r--r-- 1 root root 12506313 Sep 12 09:10 logab0001af-rw-r--r-- 1 root root 12506206 Sep 12 09:10 logab0001ag-rw-r--r-- 1 root root 12506325 Sep 12 09:10 logab0001ah-rw-r--r-- 1 root root 5390728 Sep 12 09:10 logab0001ai 查看每个文件的行数1234567891011for i in `ls logab0001*`;do echo &quot;$i文件行数&quot;`cat $i|wc -l` ;done logab0001文件行数505logab0001aa文件行数60logab0001ab文件行数60logab0001ac文件行数60logab0001ad文件行数60logab0001ae文件行数60logab0001af文件行数60logab0001ag文件行数60logab0001ah文件行数60logab0001ai文件行数25 5、按每行字节数切割指定每行字节数最多不超过2000000个字节split logab0001aa -C 2000000 testlog123456789101112ls -lthr testlog*-rw-r--r-- 1 root root 1.5M Sep 12 10:19 testlogaa-rw-r--r-- 1 root root 1018K Sep 12 10:19 testlogab-rw-r--r-- 1 root root 1018K Sep 12 10:19 testlogac-rw-r--r-- 1 root root 1018K Sep 12 10:19 testlogad-rw-r--r-- 1 root root 1018K Sep 12 10:19 testlogae-rw-r--r-- 1 root root 1018K Sep 12 10:19 testlogaf-rw-r--r-- 1 root root 1018K Sep 12 10:19 testlogag-rw-r--r-- 1 root root 1018K Sep 12 10:19 testlogah-rw-r--r-- 1 root root 1018K Sep 12 10:19 testlogai-rw-r--r-- 1 root root 1018K Sep 12 10:19 testlogaj-rw-r--r-- 1 root root 1018K Sep 12 10:19 testlogak 查看某一行的字节数12[root@dax-mysql-slave log]# sed &apos;1p&apos; testlogaa|wc -c1960374 字节数小于20000001rm -rf testlog* 删除生成的日志文件，生成每行字节最大不超过1500000的日志文件：split logab0001aa -C 1500000 testlog12345678910111213ls -lthr testlog*-rw-r--r-- 1 root root 449K Sep 12 10:22 testlogaa-rw-r--r-- 1 root root 1018K Sep 12 10:22 testlogab-rw-r--r-- 1 root root 1018K Sep 12 10:22 testlogac-rw-r--r-- 1 root root 1018K Sep 12 10:22 testlogad-rw-r--r-- 1 root root 1018K Sep 12 10:22 testlogae-rw-r--r-- 1 root root 1018K Sep 12 10:22 testlogaf-rw-r--r-- 1 root root 1018K Sep 12 10:22 testlogag-rw-r--r-- 1 root root 1018K Sep 12 10:22 testlogah-rw-r--r-- 1 root root 1018K Sep 12 10:22 testlogai-rw-r--r-- 1 root root 1018K Sep 12 10:22 testlogaj-rw-r--r-- 1 root root 1018K Sep 12 10:22 testlogak-rw-r--r-- 1 root root 1018K Sep 12 10:22 testlogal 查看某一行的字节数12[root@dax-mysql-slave log]# sed &apos;1p&apos; testlogaa|wc -c918219 字节数小于1500000 6、上面产生11g的慢日志文件主要是由于日志没有定期切割造成的，因此写了个脚本，定期对慢日志进行切割：12345678910111213141516171819202122232425#!/bin/bash#mysql慢日志所在路径slowlogpath=/data/mysql/log#mysql慢日志文件名slowlogname=slow.log#当前系统时间stamp=`date +&apos;%Y%m%d&apos;`#与当前系统时间对比，七天前的时间oldstamp=`date +&quot;%Y%m%d&quot; -d &quot;-7 day&quot;`#mysql用户名username=root#mysql用户名密码password=12345678#mysql安装路径dbpath=/data/mysql/bin#移动mysql慢日志mv $&#123;slowlogpath&#125;/$&#123;slowlogname&#125; $&#123;slowlogpath&#125;/$&#123;stamp&#125;_$&#123;slowlogname&#125;#重新生成slow日志文件：$&#123;dbpath&#125;/mysqladmin -u $&#123;username&#125; -p$&#123;password&#125; flush-logs slow#删除7天前的日志文件if [ -e $&#123;slowlogpath&#125;/$&#123;oldstamp&#125;_$&#123;slowlogname&#125; ];then sudo rm -rf $&#123;slowlogpath&#125;/$&#123;oldstamp&#125;_$&#123;slowlogname&#125;else :fi 将脚本添加到定时任务：crontab -l10 1 * * * sh /data/script/split_mysqlslowlog.sh]]></content>
      <tags>
        <tag>mysql</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysqldumpslow参数详解]]></title>
    <url>%2F2018%2F09%2F12%2Fmysqldumpslow%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1、mysql慢查询日志包含了执行花费较长时间的查询语句信息。mysqldumpslow解析慢日志文件并打印出内容摘要。mysqldumpslow进行分组查询，除了有特殊数值和字符串数据值之外。当现实摘要输出是，将数值抽象化为n（数字）和s（字符串）。参数-a和-n可以用于修改抽象化行为。 0、-a 表示不使用抽象的字符串s，数字n替换查询sql语句的内容不指定-a参数，查询结果中数值和字符串分别用n和s代替/data/mysql/bin/mysqldumpslow -t 2 /data/mysql/log/slow.log12345Reading mysql slow query log from /data/mysql/log/slow.logCount: 7 Time=132.03s (924s) Lock=94.51s (661s) Rows=0.0 (0), root[root]@localhost UPDATE test SET state = &apos;S&apos; where state=&apos;S&apos; and id in (select id from (select id from test ORDER BY id ASC LIMIT N,N) as tmp)Count: 7 Time=81.63s (571s) Lock=75.80s (530s) Rows=0.0 (0), root[root]@localhost UPDATE test SET state = &apos;S&apos; where state=&apos;S&apos; and id &gt;=N and id &lt;= N 添加-a参数，查询结果中数值和字符串用各自的对应值表示，未被抽象化：/data/mysql/bin/mysqldumpslow -a -t 2 /data/mysql/log/slow.log12345Reading mysql slow query log from /data/mysql/log/slow.logCount: 1 Time=244.60s (244s) Lock=0.00s (0s) Rows=0.0 (0), root[root]@localhost UPDATE test SET state = &apos;CANCELLED&apos; where state=&apos;CANCEL&apos; and id in (select id from (select id from test ORDER BY id ASC LIMIT 1,1000000) as tmp)Count: 1 Time=230.44s (230s) Lock=0.00s (0s) Rows=0.0 (0), root[root]@localhost UPDATE test SET state = &apos;CANCELLED&apos; where state=&apos;CANCEL&apos; and id in (select id from (select id from test ORDER BY id ASC LIMIT 4000000,1000000) as tmp) 2、-n 名称中抽象化数字的最少个数3、–debug 写入调试信息4、-g 后面可以写正则表达式匹配，大小写不敏感。根据个人需要，过滤需要的关键字：/data/mysql/bin/mysqldumpslow -t 5 -l -g ‘test’ /data/mysql/log/slow.log1234567891011Reading mysql slow query log from /data/mysql/log/slow.logCount: 7 Time=226.54s (1585s) Lock=94.51s (661s) Rows=0.0 (0), root[root]@localhost UPDATE test SET state = &apos;S&apos; where state=&apos;S&apos; and id in (select id from (select id from test ORDER BY id ASC LIMIT N,N) as tmp)Count: 7 Time=157.43s (1101s) Lock=75.80s (530s) Rows=0.0 (0), root[root]@localhost UPDATE test SET state = &apos;S&apos; where state=&apos;S&apos; and id &gt;=N and id &lt;= NCount: 35 Time=59.29s (2075s) Lock=0.00s (0s) Rows=9207179.6 (322251287), root[root]@localhost SELECT /*!N SQL_NO_CACHE */ * FROM `test`Count: 1 Time=44.68s (44s) Lock=0.00s (0s) Rows=2.0 (2), root[root]@localhost select distinct state from test order by idCount: 2 Time=27.04s (54s) Lock=0.00s (0s) Rows=2.0 (4), root[root]@localhost select distinct state from test where id in (select id from (select id from test ORDER BY id ASC LIMIT N,N) as tmp) 5、–help 显示帮助信息并退出6、-h 日志文件名中服务的主机名使用-h hostname参数，可使用通配符去过滤慢日志查看当前目录下的慢日志文件名称：123ll *-slow.log-rw-r--r-- 1 root root 583 Sep 12 06:21 10.0.7.4-slow.log-rw-r----- 1 mysql mysql 1889 Sep 12 05:51 test-slow.log 如果使用-slow.log去过滤慢日志，会把所有-slow.log为后缀的文件都过滤。mysqldumpslow -a -t 20 -h dax-mysql-slave /data/mysql/log/-slow.log12345678910Reading mysql slow query log from /data/mysql/log/10.0.7.4-slow.log /data/mysql/log/test-slow.logCount: 1 Time=137.68s (137s) Lock=0.00s (0s) Rows=5.0 (5), root[root]@localhost SELECT * FROM test WHERE trade_asset = &apos;BCH&apos; AND price_asset = &apos;ETH&apos; AND broker_id = &apos;1022668762972741633&apos; AND state in (&apos;WAITING&apos;, &apos;PROCESSING&apos;) order by id limit 5Count: 1 Time=120.01s (120s) Lock=0.00s (0s) Rows=15.0 (15), repl[repl]@[10.0.7.51] SELECT * FROM test WHERE trade_asset = &apos;BCH&apos; AND price_asset = &apos;ETH&apos; AND broker_id = &apos;1022668762972741633&apos; AND state in (&apos;WAITING&apos;, &apos;PROCESSING&apos;) order by id limit 20Count: 2 Time=90.84s (181s) Lock=0.00s (0s) Rows=10.0 (20), 2users@2hosts SELECT * FROM test WHERE trade_asset = &apos;BCH&apos; AND price_asset = &apos;ETH&apos; AND broker_id = &apos;1022668762972741633&apos; AND state in (&apos;WAITING&apos;, &apos;PROCESSING&apos;) order by id limit 10Count: 1 Time=82.82s (82s) Lock=0.00s (0s) Rows=12.0 (12), repl[repl]@[10.0.7.51] SELECT * FROM test WHERE trade_asset = &apos;BCH&apos; AND price_asset = &apos;ETH&apos; AND broker_id = &apos;1022668762972741633&apos; AND state in (&apos;WAITING&apos;, &apos;PROCESSING&apos;) order by id limit 12Died at /data/mysql/bin/mysqldumpslow line 161, &lt;&gt; chunk 5. 也可以指定匹配主机范围：mysqldumpslow -a -t 20 -h dax-mysql-slave /data/mysql/log/10.0.7.*-slow.log1234Reading mysql slow query log from /data/mysql/log/10.0.7.4-slow.logCount: 1 Time=82.82s (82s) Lock=0.00s (0s) Rows=12.0 (12), repl[repl]@[10.0.7.51] SELECT * FROM test WHERE trade_asset = &apos;BCH&apos; AND price_asset = &apos;ETH&apos; AND broker_id = &apos;1022668762972741633&apos; AND state in (&apos;WAITING&apos;, &apos;PROCESSING&apos;) order by id limit 12Died at /data/mysql/bin/mysqldumpslow line 161, &lt;&gt; chunk 1. -h 后面如果不加任何参数，会匹配出test-slow.log为日志文件名的日志：[root@dax-mysql-slave log]# mysqldumpslow -a -t 20 -h /data/mysql/log/*-slow.log12345678Reading mysql slow query log from /data/mysql/log/test-slow.logCount: 1 Time=137.68s (137s) Lock=0.00s (0s) Rows=5.0 (5), root[root]@localhost SELECT * FROM test WHERE trade_asset = &apos;BCH&apos; AND price_asset = &apos;ETH&apos; AND broker_id = &apos;1022668762972741633&apos; AND state in (&apos;WAITING&apos;, &apos;PROCESSING&apos;) order by id limit 5Count: 1 Time=120.01s (120s) Lock=0.00s (0s) Rows=15.0 (15), repl[repl]@[10.0.7.51] SELECT * FROM test WHERE trade_asset = &apos;BCH&apos; AND price_asset = &apos;ETH&apos; AND broker_id = &apos;1022668762972741633&apos; AND state in (&apos;WAITING&apos;, &apos;PROCESSING&apos;) order by id limit 20Count: 2 Time=90.84s (181s) Lock=0.00s (0s) Rows=10.0 (20), 2users@2hosts SELECT * FROM test WHERE trade_asset = &apos;BCH&apos; AND price_asset = &apos;ETH&apos; AND broker_id &apos;1022668762972741633&apos; AND state in (&apos;WAITING&apos;, &apos;PROCESSING&apos;) order by id limit 10Died at /data/mysql/bin/mysqldumpslow line 161, &lt;&gt; chunk 4. -h 后面不加任何参数，日志文件名使用过滤条件会报错：[root@dax-mysql-slave log]# mysqldumpslow -a -t 20 -h /data/mysql/log/10.0.7.*-slow.log123Reading mysql slow query log from /data/mysql/data//data/mysql/log/10.0.7.4-slow.log-slow.logCan&apos;t open /data/mysql/data//data/mysql/log/10.0.7.4-slow.log-slow.log: No such file or directory at /data/mysql/bin/mysqldumpslow line 91.Died at /data/mysql/bin/mysqldumpslow line 161. 7、-i 服务的实例名（官方文档说通过mysql.server启动查看服务实例名，测试之后未找到）8、-l 不从总时间内减去锁时间9、-r 反转排序顺序10、-s 何种方式排序：t，at：按查询时间或平均查询时间排序;l，al：按锁定时间或平均锁定时间排序;r，ar：按发送的行或发送的平均行排序;c：按计数排序;默认情况下，按平均查询时间（相当于-s at）排序。按总时间排序：/data/mysql/bin/mysqldumpslow -s t -t 5 -l /data/mysql/log/slow.log12345678910Count: 50831 Time=1.49s (75892s) Lock=0.00s (4s) Rows=1.0 (50831), cdax[cdax]@4hosts SELECT count(N) FROM test WHERE (trade_asset = &apos;S&apos; AND price_asset = &apos;S&apos; AND broker_id = &apos;S&apos; AND broker_uid = N AND state IN (&apos;S&apos;, &apos;S&apos;) AND create_time &gt;= &apos;S&apos; AND create_time &lt;= &apos;S&apos;)Count: 12053 Time=1.14s (13735s) Lock=0.00s (0s) Rows=1.0 (12053), 2users@5hosts SELECT count(N) FROM test WHERE (trade_asset = &apos;S&apos; AND price_asset = &apos;S&apos; AND broker_id = &apos;S&apos; AND broker_uid = N AND state IN (&apos;S&apos;, &apos;S&apos;))Count: 6674 Time=1.52s (10151s) Lock=0.00s (0s) Rows=9.9 (66259), 2users@5hosts SELECT id, order_no, broker_id, broker_uid, plat_id, trade_asset, price_asset, trade_type, order_type, price, number, client_order_no, traded_number, over_number, traded_money, fee_asset, fee, broker_fee, cloud_fee, create_time, update_time, state, send_state, error_code, error_msg FROM test WHERE (trade_asset = &apos;S&apos; AND price_asset = &apos;S&apos; AND broker_id = &apos;S&apos; AND broker_uid = N AND state IN (&apos;S&apos;, &apos;S&apos;)) order by create_time desc LIMIT NCount: 84 Time=40.42s (3394s) Lock=0.00s (0s) Rows=1.0 (84), 42users@42hosts SELECT member_id, member_host, member_port, member_state, @@group_replication_single_primary_mode FROM performance_schema.replication_group_members WHERE channel_name = &apos;S&apos;Count: 16300 Time=0.20s (3291s) Lock=0.00s (1s) Rows=1.0 (16300), cdax[cdax]@[10.1.110.9] SELECT count(N) FROM test WHERE (state = &apos;S&apos; AND send_state = &apos;S&apos; AND update_time &lt; &apos;S&apos;) 按平均时间排序：/data/mysql/bin/mysqldumpslow -s at -t 5 -l /data/mysql/log/slow.log1234567891011Reading mysql slow query log from /data/mysql/log/slow.logCount: 7 Time=226.54s (1585s) Lock=94.51s (661s) Rows=0.0 (0), root[root]@localhost UPDATE test SET state = &apos;S&apos; where state=&apos;S&apos; and id in (select id from (select id from test ORDER BY id ASC LIMIT N,N) as tmp)Count: 7 Time=157.43s (1101s) Lock=75.80s (530s) Rows=0.0 (0), root[root]@localhost UPDATE test SET state = &apos;S&apos; where state=&apos;S&apos; and id &gt;=N and id &lt;= NCount: 35 Time=79.40s (2779s) Lock=0.00s (0s) Rows=18592104.7 (650723665), root[root]@localhost SELECT /*!N SQL_NO_CACHE */ * FROM `finance_detail`Count: 35 Time=59.29s (2075s) Lock=0.00s (0s) Rows=9207179.6 (322251287), root[root]@localhost SELECT /*!N SQL_NO_CACHE */ * FROM `test`Count: 1 Time=44.68s (44s) Lock=0.00s (0s) Rows=2.0 (2), root[root]@localhost select distinct state from test order by id 按锁表时间排序[root@mw-mysql-2 ~]# /data/mysql/bin/mysqldumpslow -s l -t 5 -l /data/mysql/log/slow.log1234567891011Reading mysql slow query log from /data/mysql/log/slow.logCount: 7 Time=226.54s (1585s) Lock=94.51s (661s) Rows=0.0 (0), root[root]@localhost UPDATE test SET state = &apos;S&apos; where state=&apos;S&apos; and id in (select id from (select id from test ORDER BY id ASC LIMIT N,N) as tmp)Count: 7 Time=157.43s (1101s) Lock=75.80s (530s) Rows=0.0 (0), root[root]@localhost UPDATE test SET state = &apos;S&apos; where state=&apos;S&apos; and id &gt;=N and id &lt;= NCount: 50831 Time=1.49s (75892s) Lock=0.00s (4s) Rows=1.0 (50831), cdax[cdax]@4hosts SELECT count(N) FROM test WHERE (trade_asset = &apos;S&apos; AND price_asset = &apos;S&apos; AND broker_id = &apos;S&apos; AND broker_uid = N AND state IN (&apos;S&apos;, &apos;S&apos;) AND create_time &gt;= &apos;S&apos; AND create_time &lt;= &apos;S&apos;)Count: 16300 Time=0.20s (3291s) Lock=0.00s (1s) Rows=1.0 (16300), cdax[cdax]@[10.1.110.9] SELECT count(N) FROM test WHERE (state = &apos;S&apos; AND send_state = &apos;S&apos; AND update_time &lt; &apos;S&apos;)Count: 12053 Time=1.14s (13735s) Lock=0.00s (0s) Rows=1.0 (12053), 2users@5hosts SELECT count(N) FROM test WHERE (trade_asset = &apos;S&apos; AND price_asset = &apos;S&apos; AND broker_id = &apos;S&apos; AND broker_uid = N AND state IN (&apos;S&apos;, &apos;S&apos;)) 按锁表平均时间排序[root@mw-mysql-2 ~]# /data/mysql/bin/mysqldumpslow -s al -t 5 -l /data/mysql/log/slow.log1234567891011Reading mysql slow query log from /data/mysql/log/slow.logCount: 7 Time=226.54s (1585s) Lock=94.51s (661s) Rows=0.0 (0), root[root]@localhost UPDATE test SET state = &apos;S&apos; where state=&apos;S&apos; and id in (select id from (select id from test ORDER BY id ASC LIMIT N,N) as tmp)Count: 7 Time=157.43s (1101s) Lock=75.80s (530s) Rows=0.0 (0), root[root]@localhost UPDATE test SET state = &apos;S&apos; where state=&apos;S&apos; and id &gt;=N and id &lt;= NCount: 1 Time=44.68s (44s) Lock=0.00s (0s) Rows=2.0 (2), root[root]@localhost select distinct state from test order by idCount: 1 Time=0.98s (0s) Lock=0.00s (0s) Rows=1.0 (1), root[root]@localhost select count(*) from test where broker_id = &apos;S&apos; AND broker_uid = NCount: 2 Time=27.04s (54s) Lock=0.00s (0s) Rows=2.0 (4), root[root]@localhost select distinct state from test where id in (select id from (select id from test ORDER BY id ASC LIMIT N,N) as tmp) 按返回行排序/data/mysql/bin/mysqldumpslow -s r -t 5 -l /data/mysql/log/slow.log1234567891011Reading mysql slow query log from /data/mysql/log/slow.logCount: 35 Time=79.40s (2779s) Lock=0.00s (0s) Rows=18592104.7 (650723665), root[root]@localhost SELECT /*!N SQL_NO_CACHE */ * FROM `finance_detail`Count: 35 Time=59.29s (2075s) Lock=0.00s (0s) Rows=9207179.6 (322251287), root[root]@localhost SELECT /*!N SQL_NO_CACHE */ * FROM `test`Count: 4 Time=7.10s (28s) Lock=0.00s (0s) Rows=1121601.2 (4486405), root[root]@localhost SELECT /*!N SQL_NO_CACHE */ * FROM `mq_produce_log_1`Count: 4 Time=4.29s (17s) Lock=0.00s (0s) Rows=1121591.2 (4486365), root[root]@localhost SELECT /*!N SQL_NO_CACHE */ * FROM `mq_produce_log_2`Count: 4 Time=4.72s (18s) Lock=0.00s (0s) Rows=1121587.5 (4486350), root[root]@localhost SELECT /*!N SQL_NO_CACHE */ * FROM `mq_produce_log_3` 按平均返回行排序/data/mysql/bin/mysqldumpslow -s ar -t 5 -l /data/mysql/log/slow.log1234567891011Reading mysql slow query log from /data/mysql/log/slow.logCount: 35 Time=79.40s (2779s) Lock=0.00s (0s) Rows=18592104.7 (650723665), root[root]@localhost SELECT /*!N SQL_NO_CACHE */ * FROM `finance_detail`Count: 35 Time=59.29s (2075s) Lock=0.00s (0s) Rows=9207179.6 (322251287), root[root]@localhost SELECT /*!N SQL_NO_CACHE */ * FROM `test`Count: 4 Time=7.10s (28s) Lock=0.00s (0s) Rows=1121601.2 (4486405), root[root]@localhost SELECT /*!N SQL_NO_CACHE */ * FROM `mq_produce_log_1`Count: 4 Time=4.29s (17s) Lock=0.00s (0s) Rows=1121591.2 (4486365), root[root]@localhost SELECT /*!N SQL_NO_CACHE */ * FROM `mq_produce_log_2`Count: 4 Time=4.72s (18s) Lock=0.00s (0s) Rows=1121587.5 (4486350), root[root]@localhost SELECT /*!N SQL_NO_CACHE */ * FROM `mq_produce_log_3` 按查询次数排序/data/mysql/bin/mysqldumpslow -s c -t 5 -l /data/mysql/log/slow.log12345678910111213Reading mysql slow query log from /data/mysql/log/slow.logCount: 50831 Time=1.49s (75892s) Lock=0.00s (4s) Rows=1.0 (50831), cdax[cdax]@4hosts SELECT count(N) FROM test WHERE (trade_asset = &apos;S&apos; AND price_asset = &apos;S&apos; AND broker_id = &apos;S&apos; AND broker_uid = N AND state IN (&apos;S&apos;, &apos;S&apos;) AND create_time &gt;= &apos;S&apos; AND create_time &lt;= &apos;S&apos;)Count: 16300 Time=0.20s (3291s) Lock=0.00s (1s) Rows=1.0 (16300), cdax[cdax]@[10.1.110.9] SELECT count(N) FROM test WHERE (state = &apos;S&apos; AND send_state = &apos;S&apos; AND update_time &lt; &apos;S&apos;)Count: 12053 Time=1.14s (13735s) Lock=0.00s (0s) Rows=1.0 (12053), 2users@5hosts SELECT count(N) FROM test WHERE (trade_asset = &apos;S&apos; AND price_asset = &apos;S&apos; AND broker_id = &apos;S&apos; AND broker_uid = N AND state IN (&apos;S&apos;, &apos;S&apos;))Count: 6674 Time=1.52s (10151s) Lock=0.00s (0s) Rows=9.9 (66259), 2users@5hosts SELECT id, order_no, broker_id, broker_uid, plat_id, trade_asset, price_asset, trade_type, order_type, price, number, client_order_no, traded_number, over_number, traded_money, fee_asset, fee, broker_fee, cloud_fee, create_time, update_time, state, send_state, error_code, error_msg FROM test WHERE (trade_asset = &apos;S&apos; AND price_asset = &apos;S&apos; AND broker_id = &apos;S&apos; AND broker_uid = N AND state IN (&apos;S&apos;, &apos;S&apos;)) order by create_time desc LIMIT NCount: 619 Time=1.20s (741s) Lock=0.00s (0s) Rows=10.0 (6181), cdax[cdax]@[10.1.110.9] SELECT id,order_no,broker_id,broker_uid,plat_id,trade_asset,price_asset,trade_type,order_type,price,number,client_order_no,traded_number,over_number,traded_money,fee_asset,fee,broker_fee,cloud_fee,create_time,update_time,state,send_state,error_code,error_msg FROM test WHERE ( state = &apos;S&apos; and send_state = &apos;S&apos; and update_time &lt; &apos;S&apos; ) LIMIT N 添加-r参数对取到值反转排序：/data/mysql/bin/mysqldumpslow -s c -t 5 -l -r /data/mysql/log/slow.log12345678910111213Reading mysql slow query log from /data/mysql/log/slow.logCount: 619 Time=1.20s (741s) Lock=0.00s (0s) Rows=10.0 (6181), cdax[cdax]@[10.1.110.9] SELECT id,order_no,broker_id,broker_uid,plat_id,trade_asset,price_asset,trade_type,order_type,price,number,client_order_no,traded_number,over_number,traded_money,fee_asset,fee,broker_fee,cloud_fee,create_time,update_time,state,send_state,error_code,error_msg FROM test WHERE ( state = &apos;S&apos; and send_state = &apos;S&apos; and update_time &lt; &apos;S&apos; ) LIMIT NCount: 6674 Time=1.52s (10151s) Lock=0.00s (0s) Rows=9.9 (66259), 2users@5hosts SELECT id, order_no, broker_id, broker_uid, plat_id, trade_asset, price_asset, trade_type, order_type, price, number, client_order_no, traded_number, over_number, traded_money, fee_asset, fee, broker_fee, cloud_fee, create_time, update_time, state, send_state, error_code, error_msg FROM test WHERE (trade_asset = &apos;S&apos; AND price_asset = &apos;S&apos; AND broker_id = &apos;S&apos; AND broker_uid = N AND state IN (&apos;S&apos;, &apos;S&apos;)) order by create_time desc LIMIT NCount: 12053 Time=1.14s (13735s) Lock=0.00s (0s) Rows=1.0 (12053), 2users@5hosts SELECT count(N) FROM test WHERE (trade_asset = &apos;S&apos; AND price_asset = &apos;S&apos; AND broker_id = &apos;S&apos; AND broker_uid = N AND state IN (&apos;S&apos;, &apos;S&apos;))Count: 16300 Time=0.20s (3291s) Lock=0.00s (1s) Rows=1.0 (16300), cdax[cdax]@[10.1.110.9] SELECT count(N) FROM test WHERE (state = &apos;S&apos; AND send_state = &apos;S&apos; AND update_time &lt; &apos;S&apos;)Count: 50831 Time=1.49s (75892s) Lock=0.00s (4s) Rows=1.0 (50831), cdax[cdax]@4hosts SELECT count(N) FROM test WHERE (trade_asset = &apos;S&apos; AND price_asset = &apos;S&apos; AND broker_id = &apos;S&apos; AND broker_uid = N AND state IN (&apos;S&apos;, &apos;S&apos;) AND create_time &gt;= &apos;S&apos; AND create_time &lt;= &apos;S&apos;) 11、-t 按指定数字显示行输出12、–verbose 详细模式13、如果不小心删掉了slow.log可通过flush-log参数来重新生成慢日志文件（定期做日志切割的时候可能会用到）1mysqladmin -u root -ptest flush-logs slow]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql limit参数详解及使用过程遇到的问题]]></title>
    <url>%2F2018%2F09%2F11%2Fmysql%20limit%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%BD%BF%E7%94%A8%E8%BF%87%E7%A8%8B%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前提：开发反应一条sql查询很慢，查看sql执行计划：查看到没有走设置的索引，而是走了主键索引，尝试去掉limit行限制之后：发现去掉limit走索引正常。是用group by和limit测试执行计划是否正常：group by与limit组合会影响执行计划。只使用limit，去掉order by和group by实验：执行计划没有受影响。结论：order by、group by与limit在一起执行的时候要注意执行计划是否影响，如果不得不使用该组合，担心执行计划被更改，建议使用force index(index_name)，或者多次测试执行计划不会被影响再放到生产环境里。下面介绍官方文档就limit参数的解释： 0、如果是想要在结果集中获取固定的行数，在查询语句中使用limit子句，而不是获取全部的数据之后扔掉额外的数据。1、如果使用limit子句选择少数行，正常情况下倾向于进行全表扫描的时候，mysql会使用索引。2、如果使用order by与limit row_count组合，mysql只要找到排序结果row_count行数就会停止排序，而不是把所有的结果排序。排序使用索引效率更高。如果一个文件排序必须完成，在row_count行被找到之前，选择不带限制语句的所有行对他们进行排序。在初始化行被找到之后，mysql将不在对结果集排序。如果order by列多行值相同，服务器返回这些行没有任何顺序，根据全部的执行计划会有所不同。换句话说，这些行的排序顺序是不确定的相对于非排序列。limit会影响执行计划，因此orderby带有limit和不带有limit返回行会是不同排序。 3、官方文档的实验如下：123456789101112mysql&gt; SELECT * FROM ratings ORDER BY category;+----+----------+--------+| id | category | rating |+----+----------+--------+| 1 | 1 | 4.5 || 5 | 1 | 3.2 || 3 | 2 | 3.7 || 4 | 2 | 3.5 || 6 | 2 | 3.5 || 2 | 3 | 5.0 || 7 | 3 | 2.7 |+----+----------+--------+ 带有limit子句会影响每一个行category的返回值：12345678910mysql&gt; SELECT * FROM ratings ORDER BY category LIMIT 5;+----+----------+--------+| id | category | rating |+----+----------+--------+| 1 | 1 | 4.5 || 5 | 1 | 3.2 || 4 | 2 | 3.5 || 3 | 2 | 3.7 || 6 | 2 | 3.5 |+----+----------+--------+ 对于带有limit和不带有limit的子句返回相同的行排序是很重要的，增加合适的列在order by子句中确保顺序。例如在order by后加一个id的排序：12345678910111213141516171819202122mysql&gt; SELECT * FROM ratings ORDER BY category, id;+----+----------+--------+| id | category | rating |+----+----------+--------+| 1 | 1 | 4.5 || 5 | 1 | 3.2 || 3 | 2 | 3.7 || 4 | 2 | 3.5 || 6 | 2 | 3.5 || 2 | 3 | 5.0 || 7 | 3 | 2.7 |+----+----------+--------+mysql&gt; SELECT * FROM ratings ORDER BY category, id LIMIT 5;+----+----------+--------+| id | category | rating |+----+----------+--------+| 1 | 1 | 4.5 || 5 | 1 | 3.2 || 3 | 2 | 3.7 || 4 | 2 | 3.5 || 6 | 2 | 3.5 |+----+----------+--------+ 4、根据官方文档提供的信息，自己动手实验，创建测试数据：1234567891011121314151617181920212223242526create table ratings (id int,category int,rating varchar(10));insert into ratings values(5,1,&apos;3.2&apos;),(1,1,&apos;4.5&apos;),(4,2,&apos;3.5&apos;),(3,2,&apos;3.7&apos;),(6,2,&apos;3.5&apos;),(7,3,&apos;2.7&apos;),(2,3,&apos;5.0&apos;);root@db 08:47: [test]&gt; SELECT * FROM ratings order by category;+------+----------+--------+| id | category | rating |+------+----------+--------+| 5 | 1 | 3.2 || 1 | 1 | 4.5 || 4 | 2 | 3.5 || 3 | 2 | 3.7 || 6 | 2 | 3.5 || 7 | 3 | 2.7 || 2 | 3 | 5.0 |+------+----------+--------+7 rows in set (0.00 sec)root@db 08:47: [test]&gt; SELECT * FROM ratings order by category limit 5;+------+----------+--------+| id | category | rating |+------+----------+--------+| 5 | 1 | 3.2 || 1 | 1 | 4.5 || 4 | 2 | 3.5 || 3 | 2 | 3.7 || 6 | 2 | 3.5 |+------+----------+--------+5 rows in set (0.00 sec) id列没有做任何排序。12345678910111213root@db 08:47: [test]&gt; SELECT * FROM ratings order by category,id;+------+----------+--------+| id | category | rating |+------+----------+--------+| 1 | 1 | 4.5 || 5 | 1 | 3.2 || 3 | 2 | 3.7 || 4 | 2 | 3.5 || 6 | 2 | 3.5 || 2 | 3 | 5.0 || 7 | 3 | 2.7 |+------+----------+--------+7 rows in set (0.00 sec) 在order by后多加一个id列，category和id列都按大小做了排序。 5、尝试在对id列添加主键索引123456789101112131415161718192021222324252627root@db 09:10: [test]&gt; alter table ratings add primary key(id);Query OK, 0 rows affected (0.11 sec)Records: 0 Duplicates: 0 Warnings: 0root@db 09:10: [test]&gt; SELECT * FROM ratings order by category;+----+----------+--------+| id | category | rating |+----+----------+--------+| 1 | 1 | 4.5 || 5 | 1 | 3.2 || 3 | 2 | 3.7 || 4 | 2 | 3.5 || 6 | 2 | 3.5 || 2 | 3 | 5.0 || 7 | 3 | 2.7 |+----+----------+--------+7 rows in set (0.00 sec)root@db 09:10: [test]&gt; SELECT * FROM ratings order by category limit 5;+----+----------+--------+| id | category | rating |+----+----------+--------+| 1 | 1 | 4.5 || 5 | 1 | 3.2 || 3 | 2 | 3.7 || 4 | 2 | 3.5 || 6 | 2 | 3.5 |+----+----------+--------+5 rows in set (0.00 sec) 添加主键之后id和category列都按大小排序。 6、如果使用limit row_count和distinct联合，mysql停止查询一旦找到row_count唯一行数。7、一些情况下，group by通过顺序读取索引来解决（或者在索引上做一个排序），然后统计信息直到索引值发生变化。在一些情况下，limit row_count不计算任何不必要group by值。8、只要mysql发送需要的行数据到客户端，并中止查询除非使用sql_cal_found_rows函数。在这种情况下，可以使用select found_rows()获取行数。9、limit 0快速返回一个空集。对于检查查询有效性很用帮助。10、如果服务使用临时表来解决查询，使用limit row_count子句来计算需要多少空间。11、如果order by没有用索引并且limit子句存在，优化器为了避免合并文件会使用内存filesort操作对内存内的行排序。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql-Lock wait timeout exceeded; try restarting transaction]]></title>
    <url>%2F2018%2F09%2F10%2Fmysql-Lock%20wait%20timeout%20exceeded%3B%20try%20restarting%20transaction%2F</url>
    <content type="text"><![CDATA[1、mysql执行一条update语句报错：1ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 查看当前系统锁等待超时时间:1234567show variables like &apos;%lock_wait_timeout%&apos;;+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| innodb_lock_wait_timeout | 30 || lock_wait_timeout | 60 |+--------------------------+-------+ innodb事务锁等待超时事件30s，其他非innodb事务锁等待事件超时为60s， 2、查看造成事务阻塞的信息：select * from information_schema.innodb_trx\G；查看innodb_trx各列参数，点击此处线程id为1308的事务被线程id为1311的事务阻塞，查看不到线程id为1311正在执行的语句 3、查看事务锁信息，记录了当前锁的相关信息；查看innodb_lock各列参数，点击此处：select * from information_schema.innodb_locks\G 4、查看线程id为1311的线程信息，select * from information_schema.processlist where id=1311\G，该事务command为sleep，表示事务sql语句已经执行完成，但是锁仍然存在，没有释放手动kill掉该线程，在执行其他操作正常。1kill 1311]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql show innodb_locks详解]]></title>
    <url>%2F2018%2F09%2F10%2Fmysql%20innodb_locks%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1、INFORMATION_SCHEMA INNODB_LOCKS 提供innodb事务去请求但没有获取到的锁信息和事务阻塞其他事务的锁信息。执行命令如下：1select * from information_schema.innodb_locks\G 2、innodb_locks各列参数详解：12345678910lock_id:innodb唯一lock id。把他当做一个不透明的字符串。虽然lock_id当前包含trx_id，lock_id的数据格式在任何时间都肯能改变。不要写用于解析lock_id值得应用程序。lock_trx_id：持有锁的事务id。查询事务信息，与innodb_trx表中trx_id列匹配。lock_mode:锁请求。该值包括： S, X, IS, IX, GAP, AUTO_INC, and UNKNOWN。锁模式标识符可以组合用于识别特定的锁模式。查看更多信息，点击[此处]((https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html))lock_type:锁类型。行锁为record，表锁为table。lock_table:被锁的表名，或者包含锁记录的表名。lock_index:lock_type为行锁时，该值为索引名，否则为空。lock_space:lock_type为行锁时，该值为锁记录的表空间的id，否则为空。lock_page：lock_type为行锁时，该值为锁记录页数量，否则为空。lock_rec:lock_type为行锁时，页内锁记录的堆数，否则为空。lock_data：与锁相关的数据。如果lock_type为行锁时，该值是锁记录的主键值，否则为空。这列包含锁定行的主键列的值，转化为一个有效的字符串，如果没有主键，lock_data是唯一innodb内部行id号。如果是键值或者范围大于索引的最大值会使用间隙锁，lock_data表示为supremum pseudo-record。当包含锁记录的页不在buffer pool内，innodb不去从磁盘获取页，为了避免不必要的磁盘操作，lock_data为空。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql innodb_trx参数详解]]></title>
    <url>%2F2018%2F09%2F10%2Fmysql%20innodb_trx%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1、innodb_trx表提供了当前innodb引擎内每个事务的信息（只读事务除外），包括当一个事务启动，事务是否在等待一个锁，以及交易正在执行的语句（如果有的话）。查询语句：1select * from information_schema.innodb_trx; 1select * from information_schema.innodb_trx\G 2、innodb_trx表列信息详解：123456789101112131415161718192021222324trx_id：唯一事务id号，只读事务和非锁事务是不会创建id的。TRX_WEIGHT：事务的高度，代表修改的行数（不一定准确）和被事务锁住的行数。为了解决死锁，innodb会选择一个高度最小的事务来当做牺牲品进行回滚。已经被更改的非交易型表的事务权重比其他事务高，即使改变的行和锁住的行比其他事务低。TRX_STATE：事务的执行状态，值一般分为：RUNNING, LOCK WAIT, ROLLING BACK, and COMMITTING.TRX_STARTED：事务的开始时间TRX_REQUESTED_LOCK_ID:如果trx_state是lockwait,显示事务当前等待锁的id，不是则为空。想要获取锁的信息，根据该lock_id，以innodb_locks表中lock_id列匹配条件进行查询，获取相关信息。TRX_WAIT_STARTED：如果trx_state是lockwait,该值代表事务开始等待锁的时间；否则为空。TRX_MYSQL_THREAD_ID：mysql线程id。想要获取该线程的信息，根据该thread_id，以INFORMATION_SCHEMA.PROCESSLIST表的id列为匹配条件进行查询。TRX_QUERY：事务正在执行的sql语句。TRX_OPERATION_STATE：事务当前的操作状态，没有则为空。TRX_TABLES_IN_USE：事务在处理当前sql语句使用innodb引擎表的数量。TRX_TABLES_LOCKED：当前sql语句有行锁的innodb表的数量。（因为只是行锁，不是表锁，表仍然可以被多个事务读和写）TRX_LOCK_STRUCTS：事务保留锁的数量。TRX_LOCK_MEMORY_BYTES：在内存中事务索结构占得空间大小。TRX_ROWS_LOCKED：事务行锁最准确的数量。这个值可能包括对于事务在物理上存在，实际不可见的删除标记的行。TRX_ROWS_MODIFIED：事务修改和插入的行数TRX_CONCURRENCY_TICKETS：该值代表当前事务在被清掉之前可以多少工作，由 innodb_concurrency_tickets系统变量值指定。TRX_ISOLATION_LEVEL：事务隔离等级。TRX_UNIQUE_CHECKS：当前事务唯一性检查启用还是禁用。当批量数据导入时，这个参数是关闭的。TRX_FOREIGN_KEY_CHECKS：当前事务的外键坚持是启用还是禁用。当批量数据导入时，这个参数是关闭的。TRX_LAST_FOREIGN_KEY_ERROR：最新一个外键错误信息，没有则为空。TRX_ADAPTIVE_HASH_LATCHED：自适应哈希索引是否被当前事务阻塞。当自适应哈希索引查找系统分区，一个单独的事务不会阻塞全部的自适应hash索引。自适应hash索引分区通过 innodb_adaptive_hash_index_parts参数控制，默认值为8。TRX_ADAPTIVE_HASH_TIMEOUT：是否为了自适应hash索引立即放弃查询锁，或者通过调用mysql函数保留它。当没有自适应hash索引冲突，该值为0并且语句保持锁直到结束。在冲突过程中，该值被计数为0，每句查询完之后立即释放门闩。当自适应hash索引查询系统被分区（由 innodb_adaptive_hash_index_parts参数控制），值保持为0。TRX_IS_READ_ONLY：值为1表示事务是read only。TRX_AUTOCOMMIT_NON_LOCKING：值为1表示事务是一个select语句，该语句没有使用for update或者shared mode锁，并且执行开启了autocommit，因此事务只包含一个语句。当TRX_AUTOCOMMIT_NON_LOCKING和TRX_IS_READ_ONLY同时为1，innodb通过降低事务开销和改变表数据库来优化事务。 3、注意事项12该表用于当系统负载较高时，诊断性能问题。查询该表必须有process权限。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql show processlist详解]]></title>
    <url>%2F2018%2F09%2F09%2Fmysql%20show%20processlist%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[0、show processlist查看正在运行的线程，如果你有process权限，能够看到所有的线程，如果没有权限只能看到自己的线程。有管理员权限的用户，如下图所示：没有管理权限的用户，如下图所示：：如果不加full关键字，每条语句的info信息栏只能显示前100个字符。如果看到“too many connections”的错误信息，想要看看什么线程正在运行，使用show processlist语句是很有用的。mysql数据库会额外保持一个连接，供具有connection_admin或者super权限用户的使用，用以确保管理员能随时连接进来检查系统状况（假设你没有将此权限授予所有用户）。show processlist输出列详解： 1、id：连接标识符，与INFORMATION_SCHEMA PROCESSLIST表的id列显示的是同类型的值，如下图所示：Performance_Schema threads表的processlist_id列也是相同值，由connection_id())函数返回值。通过kill 语句可以杀掉查询到的线程，执行命令如下：1kill id号 2、user: 代表数据库用户，如果是system user代表非客户端线程，是服务器发起的处理内部事务的线程。这些线程可能是io线程、复制从库的sql线程、延迟行处理线程。对于system user，host列没有任何信息。 unauthenticated user是有客户端发起的，但是尚未对客户端用户进行验证的线程。event_scheduler 是监控调度事件的线程。3、host：发起线程的客户的主机名（对于system user，host这一列为空）。tcp/ip连接的主机名是：hostname：client_port格式，用这种格式来更方面查看客户端在做什么操作。4、db：如果指定了库名，会使用当前的库名；如果没有则为空。5、command：线程的命令类型。下面列举几种常见的线程命令，如果想查看详细信息点击此处1234567891011121314connect：从复制已经连接到主库connect out：从复制正在连接到主库create db：正在执行一个创建库的操作。execute：正在执行一个准备好的语句。field list：提取表列信息线程kill：当前线程被其他线程杀掉。query：正在整型一条语句。quit：线程被终止。refresh：刷新表、日志、缓存、重置状态变量值或从服务信息线程。shutdown：关闭服务线程sleep：等待客户端发送一条新语句线程。table dump：发送表内容到从服务器statics：获取当前服务状态信息。如果一个线程耗时比较久，需要重点关注造成该线程。 6、time：当前线程耗时。对于slave sql 线程，该值代表最后一次复制时间的时间戳和从服务器的真正时间的秒数。7、state：行为，事件，状态指明当前线程在做什么。下面列举几种常见状态值，查看详细信息点击此处1234567891011121314151617181920212223242526272829after create：执行完create table函数之后，创建表的线程（包括内部临时表），即使因为某些错误创建表失败，该状态也会显示。analyzing：统计myisam表键分布（例如执行analyze table）checking permissions：检查是否有权限执行该语句。check tables：执行表检查操作。cleaning up：线程正在处理一条命名，准备释放内存重置某些状态变量。closing tables:刷新更改的表到磁盘，关闭使用的表。该操作应该快速完成，如果没有，请确保是否有足够的磁盘空间或者磁盘io是否太高。copy to tmp table:处理alter table语句。该状态发生在表新结构被创建之后，行数据被拷贝到表之前。copying to group table：该执行语句中order by和group by有不同的关键字，行被组处理拷贝到临时表。copying to tmp table：正在复制内存中的临时表。altering table：正在执行alter tableCopying to tmp table on disk:正在拷贝临时表到磁盘。临时结果变得太大，因此，线程将临时表从内存转到磁盘来节省内存。creating index：对myisam表进行alter table ... enable keys操作creating sort index:使用内部临时表处理一个select语句。creating table：正在创建表（包括创建临时表）creating tmp table：在内存或磁盘创建一个临时表。如果一个表刚开始再内存创建，之后转到磁盘，该状态会变为：Copying to tmp table on disk。committing alter table to storage engine：alter table已经执行完成，正在提交 结果。executing：已经开始执行一个语句freeing items：已经执行了一个命令，该状态通常在cleaning up之后。query end：，在freeing items状态之前，处理一个查询之后logging slow query：写语句到slow-query日志sending data：正在读取和处理一个select语句的行，并发送数据到客户端，因为可能会发生物理读，该状态可能是给定生命周期时间最长的。update：准备更新表updating：正在搜索更新的行，并更新他们system lock：调用了mysql_lock_tables()函数，线程状态一直没有更新。产生该问题原因很多：比如：请求或等待一个表的内部或者外部系统锁。当innodb等待一个表锁等级为lock tables时会发生。如果这个问题是由请求外部锁，并且你没有用多个mysqld服务访问相同的myisam引擎表，你可以通过参数 --skip-external-locking禁用外部系统锁。但是外部锁默认情况下是禁用的，因此该参数可能没有影响。对于show profile命令，该状态代表正在请求锁（不是在等待锁）对于系统表锁状态是Locking system tables。waiting for commit lock：flush tables with read lock正在等待一个commit 锁waiting for tables：线程收到通知，表的底层架构已经更改，需要重新打开表后去一个新的结构。但是重新打开一个表，需要等待其他正在访问该表的线程。通常在其他线程使用flush tables或者执行下面语句: FLUSH TABLES tbl_name, ALTER TABLE, RENAME TABLE, REPAIR TABLE, ANALYZE TABLE, or OPTIMIZE TABLE. 时会发生此通知。 8、info：正在执行的语句，如果没有语句则为空。语句可能是发送的服务端的，或一个内部语句（如果一个语句执行其他语句）例如：call调用一个（select语句）存储过程，info值会显示select语句。9、另外使用mysqladmin processlist也可以查看进程信息，如下图所示： 10、如果想通过sql命令去获取自己设定的行，可按以下方法设置：12select * from information_schema.processlist where state !=&apos; &apos;;![](https://i.imgur.com/pGGuMER.png) 可根据自己需求添加条件。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql update一条语句过程遇到的问题]]></title>
    <url>%2F2018%2F09%2F09%2Fmysql%20update%E4%B8%80%E6%9D%A1%E8%AF%AD%E5%8F%A5%E8%BF%87%E7%A8%8B%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1、因为业务已停掉不担心会出现大量的锁等待事件，因此也没有考虑批量去更新（数据量有1700万左右），直接尝试去跑这一条update语句，第一次跑出现以下问题：12UPDATE trade_order SET state = &apos;CANCELLED&apos; where state=&apos;CANCEL&apos;;error：Multi-statement TRANSACTION required more THAN &apos;max_binlog_cache_size&apos; bytes of STORAGE; increase this mysqld variable AND try again 2、查看binlog_cache_size，最大值为1G：12345678mysql&gt; show variables like &apos;%binlog_cache_size%&apos;;+-----------------------+------------+| Variable_name | Value |+-----------------------+------------+| binlog_cache_size | 4194304 || max_binlog_cache_size | 1073741824 |+-----------------------+------------+2 rows in set (0.00 sec) 3、尝试去调高max_binlog_cache_size，在此执行update语句：12345678910mysql&gt; set global max_binlog_cache_size=4073741824;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; show variables like &apos;%binlog_cache_size%&apos;;+-----------------------+------------+| Variable_name | Value |+-----------------------+------------+| binlog_cache_size | 4194304 || max_binlog_cache_size | 4073738240 |+-----------------------+------------+2 rows in set (0.00 sec) 4、将至调到4G、8G去执行该语句，均报以上的错误，考虑用批量更新的方法去处理该update问题，尝试每次更新500万数据，binlog_cache_size仍然不满足，每次更新100万，执行正常：12345678mysql&gt; UPDATE trade_order SET state = &apos;CANCELLED&apos; where state=&apos;CANCEL&apos; and id &gt;=1 and id &lt;= 1000000;Query OK, 974190 rows affected (2 min 3.48 sec)Rows matched: 974190 Changed: 974190 Warnings: 0......mysql&gt; UPDATE trade_order SET state = &apos;CANCELLED&apos; where state=&apos;CANCEL&apos; and id &gt;=16000000 and id &lt;= 17000000;Query OK, 982238 rows affected (2 min 37.44 sec)Rows matched: 982238 Changed: 982238 Warnings: 0 每100万更新耗时为2min 40s左右 5、上述方法适用于当前表有主键id且递增的情况。如果该id值从其他关联表获取，可考虑一下方法：1mysql&gt; UPDATE trade_order SET state = &apos;CANCELLED&apos; where state=&apos;CANCEL&apos; and id in (select id from (select id from trade_order ORDER BY id ASC LIMIT 1,1000000) as tmp); 该方法涉及到一个子查询，总体执行速度会下降，根据取值范围和当前实际更新的行数，耗时在4min左右：123UPDATE trade_order SET state = &apos;CANCELLED&apos; where state=&apos;CANCEL&apos; and id in (select id from (select id from trade_order ORDER BY id ASC LIMIT 1,1000000) as tmp);Query OK, 981631 rows affected (3 min 49.27 sec)Rows matched: 981631 Changed: 981631 Warnings: 0]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql报错：[ERROR] InnoDB: Unable to lock ./ibdata1]]></title>
    <url>%2F2018%2F09%2F08%2Fmysql%E6%8A%A5%E9%94%99-%5BERROR%5D%20InnoDB-Unable%20to%20lock%20-ibdata1%2F</url>
    <content type="text"><![CDATA[1、 登录mysql报错：123mysql -u root -pEnter password: ERROR 2002 (HY000): Can&apos;t connect to local MySQL server through socket &apos;/data/mysql/tmp/mysql.sock&apos; (2) 2、netstat -tunlp|grep 3306发现mysql进程不存在，尝试去启动mysql：1234/etc/init.d/mysql restart ERROR! MySQL server PID file could not be found!Starting MySQL....................................................................................^C启动失败，按ctrl+c退出。 3、查看log日志，一直在刷错误日志12[ERROR] InnoDB: Unable to lock ./ibdata1 error: 11[Note] InnoDB: Check that you do not already have 4、查看磁盘使用情况1234df -hFilesystem Size Used Avail Use% Mounted on/dev/sdc1 1.1T 59G 967G 6% /data数据目录还有很多空间。 5、查看mysql进程12345shell&gt;ps -ef|grep mysqlmysql 1486 130771 2 05:16 pts/0 00:00:01 /data/mysql/bin/mysqld --basedir=/data/mysql --datadir=/data/mysql/data --plugin-dir=/data/mysql/lib/plugin --user=mysql --log-error=/data/mysql/log/error.log --open-files-limit=65535 --pid-file=/data/mysql/tmp/mysql.pid --socket=/data/mysql/tmp/mysql.sock --port=3306root 77736 1 0 Jul25 ? 00:00:00 /bin/sh /data/mysql/bin/mysqld_safe --datadir=/data/mysql/data --pid-file=/data/mysql/tmp/mysql.pidmysql 128601 77736 85 05:13 ? 00:03:49 /data/mysql/bin/mysqld --basedir=/data/mysql --datadir=/data/mysql/data --plugin-dir=/data/mysql/lib/plugin --user=mysql --log-error=/data/mysql/log/error.log --open-files-limit=65535 --pid-file=/data/mysql/tmp/mysql.pid --socket=/data/mysql/tmp/mysql.sock --port=3306root 130771 1 0 05:16 pts/0 00:00:00 /bin/sh /data/mysql/bin/mysqld_safe --datadir=/data/mysql/data --pid-file=/data/mysql/tmp/mysql.pid 6、top查看进程运行情况1234567Tasks: 187 total, 1 running, 186 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.4 us, 0.3 sy, 0.0 ni, 99.2 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 32930580 total, 21548588 free, 7218592 used, 4163400 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 24748960 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 128601 mysql 20 0 21.711g 4.678g 12784 S 2.6 14.9 3:50.26 mysqld 7、杀掉进程12860112kill -9 128601/data/mysql/bin/mysqld_safe: line 198: 1486 Killed nohup /data/mysql/bin/mysqld --basedir=/data/mysql --datadir=/data/mysql/data --plugin-dir=/data/mysql/lib/plugin --user=mysql --log-error=/data/mysql/log/error.log --open-files-limit=65535 --pid-file=/data/mysql/tmp/mysql.pid --socket=/data/mysql/tmp/mysql.sock --port=3306 &lt; /dev/null &gt; /dev/null 2&gt;&amp;1 因为mysql的守护进程存在，会自动启动mysql进程，再次登录数据库mysql -uroot -p,登录正常。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql批量建库、修改多库中同一命名个表]]></title>
    <url>%2F2018%2F09%2F07%2F%E4%BA%91%E4%BA%A4%E6%98%93%E6%89%80%E5%BB%BA%E5%BA%93%E3%80%81%E4%BF%AE%E6%94%B9%E5%A4%9A%E4%B8%AA%E8%A1%A8%E7%9A%84%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[1、批量删除数据库1234for i in test_ETH_BTC test_BTC_USDT test_ETH_USDT test_BCH_BTC test_BCH_ETH test_BCH_USDT test_ETC_BTC test_ETC_ETH test_ETC_USDT test_LTC_BTC test_LTC_ETH test_LTC_USDT test_OMG_BTC test_OMG_ETH test_OMG_USDT test_ACT_BTC test_ACT_ETH test_ACT_USDT ;domysql -u root -ptest -e &quot;drop database $i&quot;;done 2、批量创建数据库1234for i in test_ETH_BTC test_BTC_USDT test_ETH_USDT test_BCH_BTC test_BCH_ETH test_BCH_USDT test_ETC_BTC test_ETC_ETH test_ETC_USDT test_LTC_BTC test_LTC_ETH test_LTC_USDT test_OMG_BTC test_OMG_ETH test_OMG_USDT test_ACT_BTC test_ACT_ETH test_ACT_USDT ;domysql -u root -ptest -e &quot;create database $i&quot;;done 3、批量在多个库中创建多个表123456789101112131415161718192021222324for i in test_ETH_BTC test_BTC_USDT test_ETH_USDT test_BCH_BTC test_BCH_ETH test_BCH_USDT test_ETC_BTC test_ETC_ETH test_ETC_USDT test_LTC_BTC test_LTC_ETH test_LTC_USDT test_OMG_BTC test_OMG_ETH test_OMG_USDT test_ACT_BTC test_ACT_ETH test_ACT_USDT ;dofor j in &#123;0..19&#125;do sql=&quot;CREATE TABLE \`match_record_$&#123;j&#125;\` ( \`id\` bigint(20) NOT NULL AUTO_INCREMENT , \`symbol\` varchar(30) NOT NULL COMMENT &apos;交易对&apos;, \`price\` decimal(30,15) NOT NULL COMMENT &apos;成交价格&apos;, \`quantity\` decimal(30,15) NOT NULL COMMENT &apos;成交量&apos;, \`buy_match_seq\` int(11) NOT NULL COMMENT &apos;买入成交序号&apos;, \`buy_tx_no\` varchar(32) NOT NULL COMMENT &apos;买入交易单号&apos;, \`sell_match_seq\` int(11) NOT NULL COMMENT &apos;卖出成交序号&apos;, \`sell_tx_no\` varchar(32) NOT NULL COMMENT &apos;卖出交易单号&apos;, \`is_maker\` tinyint(1) NOT NULL COMMENT &apos;买入方是否为maker&apos;, \`create_time\` bigint(20) NOT NULL COMMENT &apos;成交时间戳&apos;, PRIMARY KEY (\`id\`), UNIQUE KEY \`uniq_idsymbol\` (\`id\`,\`symbol\`), UNIQUE KEY \`uniq_txno\` (\`buy_tx_no\`,\`sell_tx_no\`), KEY \`idx_create_time\` (\`create_time\`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT=&apos;成交记录表&apos;;&quot;mysql -uroot -ptest -D $i -e&quot;$&#123;sql&#125;&quot;;donedone 4、批量修改多个库中的一个同名表做ddl语句操作对表增加列，添加唯一索引：12345678for i in test_ETH_BTC test_BTC_USDT test_ETH_USDT test_BCH_BTC test_BCH_ETH test_BCH_USDT test_ETC_BTC test_ETC_ETH test_ETC_USDT test_LTC_BTC test_LTC_ETH test_LTC_USDT test_OMG_BTC test_OMG_ETH test_OMG_USDT test_ACT_BTC test_ACT_ETH test_ACT_USDT ;dosymbol=`echo &quot;$i&quot;| awk -F &apos;test_&apos; &apos;&#123;print $2&#125;&apos;`sql=&quot;alter table match_record_0 add column symbol varchar(30) NOT NULL COMMENT &apos;交易对&apos; default &apos;&quot;$&#123;symbol&#125;&quot;&apos;&quot;alter1=&quot;alter table match_record_0 add UNIQUE KEY \`uniq_idsymbol\` (\`id\`,\`symbol\`)&quot;alter2=&quot;alter table match_record_0 add UNIQUE KEY \`uniq_txno\` (\`buy_tx_no\`,\`sell_tx_no\`)&quot;mysql -u root -ptest -D $i -e&quot;$&#123;sql&#125;;$&#123;alter1&#125;;$&#123;alter2&#125;&quot;;done 对表删除列，删除唯一索引：12345678for i in test_ETH_BTC test_BTC_USDT test_ETH_USDT test_BCH_BTC test_BCH_ETH test_BCH_USDT test_ETC_BTC test_ETC_ETH test_ETC_USDT test_LTC_BTC test_LTC_ETH test_LTC_USDT test_OMG_BTC test_OMG_ETH test_OMG_USDT test_ACT_BTC test_ACT_ETH test_ACT_USDT ;dosymbol=`echo &quot;$i&quot;| awk -F &apos;test_&apos; &apos;&#123;print $2&#125;&apos;`sql=&quot;alter table match_record_0 drop column symbol&quot;alter1=&quot;alter table match_record_0 drop index \`uniq_idsymbol\`&quot;alter2=&quot;alter table match_record_0 drop index \`uniq_txno\`&quot;mysql -u root -ptest -D $i -e&quot;$&#123;sql&#125;;$&#123;alter1&#125;;$&#123;alter2&#125;&quot;;done]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql-binary排序规则与_bin排序规则对比]]></title>
    <url>%2F2018%2F09%2F06%2Fmysql%20binary%E6%8E%92%E5%BA%8F%E8%A7%84%E5%88%99%E4%B8%8E_bin%E6%8E%92%E5%BA%8F%E8%A7%84%E5%88%99%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[1、binary排序规则与_bin排序规则对比二进制字符串（像用binary，barbinary，blob存储的数据类型）有一个字符集和以binary命名的排序规则。二进制字符串是序列字节，这些字节的数字值决定了比较和排序的顺序。非二进制字符串（像用char，varchar，text存储的数据类型）有一个字符集和排序规则（名称不是binary）。一个给定的非二进制字符集有几个排序规则，每一种排序规则对集合中的字符串定义一个特定的比较和排序的顺序。这些非二进制字符集的命名规则是在二进制字符集排序规则的名称后面添加_bin后缀。例如二进制排序规则latin1和utf8分别被命名latin1_bin,utf8_bin。 2、在某些方面，二进制排序规则与_bin排序规则不同。2.1、比较和排序单元：二进制字符串是字节序列。对于二进制排序规则，比较和排序是基于数字字节值。非二进制字符串是（可能是多字节）字符的序列。非二进制排序规则定义了比较和排序字符值的顺序。对于_bin排序规则，这个顺序基于数字字符串编码值，类似于二进制字符串顺序（多字节字符串编制值除外） 2.2、字符转换：一个非二进制字符串有一个字符集，在很多情况下（即使字符串有_bin排序规则）也可以自动转换为另一个字符集。123456789当从另一个有不同字符集的列分配列值：UPDATE t1 SET utf8_bin_column=latin1_column;INSERT INTO t1 (latin1_column) SELECT utf8_bin_column FROM t2;当使用字符串文字为insert或update分配值时：SET NAMES latin1;INSERT INTO t1 (utf8_bin_column) VALUES (&apos;string-in-latin1&apos;);当从服务端给客户端返回结果：SET NAMES latin1;SELECT utf8_bin_column FROM t2; 对于二进制字符串列，没有转换发生。对于前面的情况，字符串值是按字节复制的。 2.3、大小写转换：非二进制字符集排序规则提供关于字符字母大小写的信息，因此非二进制字符可以被转换从一个字母到另一个，即使_bin排序规则忽略字母大小写顺序：1234567mysql&gt; SET NAMES latin1 COLLATE latin1_bin;mysql&gt; SELECT LOWER(&apos;aA&apos;), UPPER(&apos;zZ&apos;);+-------------+-------------+| LOWER(&apos;aA&apos;) | UPPER(&apos;zZ&apos;) |+-------------+-------------+| aa | ZZ |+-------------+-------------+ 字母大小写的概念对二进制字符串的字节不适用，执行字母大小写转换，字符串必须转换为非二进制字符串1234567mysql&gt; SET NAMES binary;mysql&gt; SELECT LOWER(&apos;aA&apos;), LOWER(CONVERT(&apos;aA&apos; USING latin1));+-------------+-----------------------------------+| LOWER(&apos;aA&apos;) | LOWER(CONVERT(&apos;aA&apos; USING latin1)) |+-------------+-----------------------------------+| aA | aa |+-------------+-----------------------------------+ 2.4、比较过程的尾随空间处理：非二进制字符串为所有的排序规则进行填充空间，包括_bin排序规则，尾随空间在比较过程是无关紧要的：1234567mysql&gt; SET NAMES utf8 COLLATE utf8_bin;mysql&gt; SELECT &apos;a &apos; = &apos;a&apos;;+------------+| &apos;a &apos; = &apos;a&apos; |+------------+| 1 |+------------+ 对于二进制字符串，比较过程所有的字符都是重要的，包括尾随空间:1234567mysql&gt; SET NAMES binary;mysql&gt; SELECT &apos;a &apos; = &apos;a&apos;;+------------+| &apos;a &apos; = &apos;a&apos; |+------------+| 0 |+------------+ 2.5、在插入和检索中的尾随空间处理：char(n)列存储非二进制字符串，当insert操作是，值小于n字符会使用空格扩展。在检索时尾随空间会被移除。二进制列存储二进制字符串,插入过程中值小于n字节，会使用ox00扩展，在检索过程中不会被移除；总是返回声明长度的值。1234567891011mysql&gt; CREATE TABLE t1 ( a CHAR(10) CHARACTER SET utf8 COLLATE utf8_bin, b BINARY(10) );mysql&gt; INSERT INTO t1 VALUES (&apos;a&apos;,&apos;a&apos;);mysql&gt; SELECT HEX(a), HEX(b) FROM t1;+--------+----------------------+| HEX(a) | HEX(b) |+--------+----------------------+| 61 | 61000000000000000000 |+--------+----------------------+]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql唯一性索引详解]]></title>
    <url>%2F2018%2F09%2F05%2Fmysql%E5%94%AF%E4%B8%80%E6%80%A7%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[1、唯一性索引创建条件：唯一性索引创建一个约束条件要求索引列上的所有值必须是不同的，如果你尝试添加一行新数据，但该行的键值与已经存在的其他行的键值相同将会报错。如果为唯一性索引指定前缀值，前缀长度的列值要保证唯一。唯一性索引允许null值。 2、唯一非空索引_rowid的使用：如果一个表有主键索引或者唯一非空索引，且索引由一个单独整型列类型组成，你可以在select语句使用_rowid来引用索引列，具体如下：_rowid引用主键列如果主键列由单个整型列组成，如果存在主键列但是他不是由一个单独整型列组成，_rowid不能被使用。另外，_rowid引用第一个有单个整型列组成的唯一非空索引列，如果不是，_rowid不能被使用。 3、创建测试表，并将列修改为唯一非空进行试验：12345678910111213create table unique_t(id int,name varchar(10));alter table unique_t add unique index id_unique_ind(id);alter table unique_t modify id int not null;show create table unique_t;+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+| unique_t | CREATE TABLE `unique_t` ( `id` int(11) NOT NULL, `name` varchar(10) DEFAULT NULL, UNIQUE KEY `id_unique_ind` (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 插入测试数据：1234567891011insert into unique_t values(1,&apos;a&apos;),(2,&apos;b&apos;),(3,&apos;c&apos;),(4,&apos;d&apos;),(6,&apos;e&apos;);select *,_rowid from unique_t;+----+------+--------+| id | name | _rowid |+----+------+--------+| 1 | a | 1 || 2 | b | 2 || 3 | c | 3 || 4 | d | 4 || 6 | e | 6 |+----+------+--------+ 4、删除唯一性索引，修改id默认列可以为空：1234567891011alter table unique_t drop index id_unique_ind;alter table unique_t modify id int null;show create table unique_t;+----------+----------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+----------+----------------------------------------------------------------------------------------------------------------------------------+| unique_t | CREATE TABLE `unique_t` ( `id` int(11) DEFAULT NULL, `name` varchar(10) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+----------+----------------------------------------------------------------------------------------------------------------------------------+ 为id列创建主键索引123456789101112131415161718192021222324alter table unique_t add primary key(id);show create table unique_t;+----------+----------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+----------+----------------------------------------------------------------------------------------------------------------------------------------------------+| unique_t | CREATE TABLE `unique_t` ( `id` int(11) NOT NULL, `name` varchar(10) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+----------+----------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)select *,_rowid from unique_t;+----+------+--------+| id | name | _rowid |+----+------+--------+| 1 | a | 1 || 2 | b | 2 || 3 | c | 3 || 4 | d | 4 || 6 | e | 6 |+----+------+--------+5 rows in set (0.00 sec) 给id添加主键约束和唯一非空约束，_rowid都可以引用对应列。 5、删除id主键，将name列设置为主键测试alter table unique_t drop primary key;alter table unique_t add primary key(name);show index from unique_t;12345678910111213141516171819+----------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+----------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| unique_t | 0 | PRIMARY | 1 | name | A | 5 | NULL | NULL | | BTREE | | |+----------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+ show create table unique_t;+----------+--------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+----------+--------------------------------------------------------------------------------------------------------------------------------------------------+| unique_t | CREATE TABLE `unique_t` ( `id` int(11) NOT NULL, `name` varchar(10) NOT NULL, PRIMARY KEY (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 |+----------+--------------------------------------------------------------------------------------------------------------------------------------------------+root@db 07:11: [exchange_market]&gt; select *,_rowid from unique_t;ERROR 1054 (42S22): Unknown column &apos;_rowid&apos; in &apos;field list&apos; 非单个整型列的主键，_rowid无法使用。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql前缀索引详解]]></title>
    <url>%2F2018%2F09%2F05%2Fmysql%E5%89%8D%E7%BC%80%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[1、使用前缀索引注意事项：1)、列类型为char，varchar，binary，varbinary可以创建前缀索引；2)、列类型为blob，text必须创建前缀索引，而且只有innodb，myisam，blackhole的存储引擎前缀索引才生效；3)、前缀限制是以字节为单位的，但是在create table，alter table，create index语句中索引指定的前缀长度的值为：非二进制类型的（cahr varchar，text）的字符串数据量、二进制类型的（binary，varbinary，blob）的字节数量。当用多字节字符串集为非二进制字符串列创建前缀索引长度时，需要考虑这一点。是否支持前缀索引和前缀的长度跟存储引擎有关系。例如，innodb引擎表前缀索引支持767字节长度，如果开启innodb_large_prefix参数，支持3072字节长度。myisam存储引擎表，前缀长度为1000字节。NDB引擎不支持前缀索引。123456show variables like 'innodb_large_prefix';+---------------------+-------+| Variable_name | Value |+---------------------+-------+| innodb_large_prefix | ON |+---------------------+-------+ 4)、以mysql5.7.17为例，如果一个指定的索引前缀长度超过最大列数据类型大小，create index时会如下处理索引：对于非唯一索引，或者一个错误发生(严格sqlmode开启)，或者索引长度降低到最大列数据类型大小并产生一个警告信息(如果strcit sql mode没有开启)123456show variables like &apos;sql_mode&apos;;+---------------+------------------------------------------------------------------------------------------------------------------------+| Variable_name | Value |+---------------+------------------------------------------------------------------------------------------------------------------------+| sql_mode | STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |+---------------+------------------------------------------------------------------------------------------------------------------------+ 严格模式是指将SQL_MODE变量设置为STRICT_TRANS_TABLES或STRICT_ALL_TABLES中的至少一种。对于唯一索引，不管sqlmode模式是何种，都会直接报错，因为降低索引长度会插入非唯一的条目不满足唯一性需求。 5)、创建前缀索引语句：1CREATE INDEX part_of_name ON customer (name(10)); 如果列中的名字在前10个字节中通常是不同的，查询性能不应该慢于创建整个name列索引的查询。另外使用列前缀索引使索引文件更小，能够节省更多的磁盘空间并且增加插入速度。 2、下面是根据网上提供的方法做的测试，参考链接1)、插入测试数据1234567create table music(name varchar(30));insert into music values(&apos;BITE&apos;),(&apos;There for you&apos;),(&apos;Scarborough Fair&apos;),(&apos;Shape of You&apos;),(&apos;Marvin Gaye&apos;),(&apos;Pretty Girl&apos;),(&apos;Pretty Boy&apos;),(&apos;Walk Away&apos;),(&apos;YOUTH&apos;),(&apos;Paris&apos;);insert into music values (&apos;Scarborough Fair&apos;),(&apos;Shape of You&apos;),(&apos;Marvin Gaye&apos;),(&apos;Pretty Girl&apos;),(&apos;Pretty Boy&apos;),(&apos;Walk Away&apos;),(&apos;YOUTH&apos;),(&apos;Paris&apos;); 2)、查看完整列索引选择性123456root@db 03:31: [exchange_market]&gt; select count(distinct name) / count(*) from music;+---------------------------------+| count(distinct name) / count(*) |+---------------------------------+| 0.5556 |+---------------------------------+ 3)、查找合适的前缀索引长度123456select count(distinct left(name,1))/count(*) as sel1, count(distinct left(name,2))/count(*) as sel2, count(distinct left(name,3))/count(*) as sel3, count(distinct left(name,4))/count(*) as sel4 from music;+--------+--------+--------+--------+| sel1 | sel2 | sel3 | sel4 |+--------+--------+--------+--------+| 0.3889 | 0.5000 | 0.5000 | 0.5000 |+--------+--------+--------+--------+ 4)、可以看到当前缀长度为2是已经接近完整列索引选择性,添加前缀索引12345678910alter table music add index music_index(name(2));Query OK, 0 rows affected (0.06 sec)Records: 0 Duplicates: 0 Warnings: 0show index from music;+-------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+-------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| music | 1 | music_index | 1 | name | A | 9 | 2 | NULL | YES | BTREE | | |+-------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+ 5)、查询索引是否被正常应用：123456789101112131415161718select * from music where name like &apos;s%&apos;;+------------------+| name |+------------------+| Scarborough Fair || Scarborough Fair || Shape of You || Shape of You |+------------------+4 rows in set (0.07 sec)explain select * from music where name like &apos;s%&apos;;+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+-------------+| 1 | SIMPLE | music | NULL | range | music_index | music_index | 11 | NULL | 4 | 100.00 | Using where |+----+-------------+-------+------------+-------+---------------+-------------+---------+------+------+----------+-------------+1 row in set, 1 warning (0.00 sec)]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] 使用mysqladmin查看innodb_buffer_pool缓存命中率及缓存使用情况]]></title>
    <url>%2F2018%2F09%2F04%2F%E4%BD%BF%E7%94%A8mysqladmin%E6%9F%A5%E7%9C%8Binnodb_buffer_pool%E7%BC%93%E5%AD%98%E5%91%BD%E4%B8%AD%E7%8E%87%E5%8F%8A%E7%BC%93%E5%AD%98%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[1、查看缓存命中率计算缓存命中率相关参数：12Innodb_buffer_pool_reads：innodb缓存无法满足逻辑读，不得不从磁盘获取数据的逻辑读数量Innodb_buffer_pool_read_requests：逻辑读的请求数量 如果查看当前某一时刻的值：123 mysqladmin -uroot -p12345678 ext | grep Innodb_buffer_pool_readsmysqladmin: [Warning] Using a password on the command line interface can be insecure.| Innodb_buffer_pool_reads | 31763701 | 123 mysqladmin -uroot -p12345678 ext | grep Innodb_buffer_pool_read_requests mysqladmin: [Warning] Using a password on the command line interface can be insecure.| Innodb_buffer_pool_read_requests | 866018530 | 如果想要查看多次的取值，并计算间隔的差值(-i 1间隔为1s)(-r 获取前后两个值之间的差值)1234567 mysqladmin -uroot -p12345678 -ri1 ext | grep Innodb_buffer_pool_readsmysqladmin: [Warning] Using a password on the command line interface can be insecure.| Innodb_buffer_pool_reads | 31763701 || Innodb_buffer_pool_reads | 0 || Innodb_buffer_pool_reads | 0 || Innodb_buffer_pool_reads | 0 || Innodb_buffer_pool_reads | 0 | 123456789mysqladmin -uroot -p12345678 -ri1 ext | grep Innodb_buffer_pool_read_requests mysqladmin: [Warning] Using a password on the command line interface can be insecure.| Innodb_buffer_pool_read_requests | 866018530 || Innodb_buffer_pool_read_requests | 0 || Innodb_buffer_pool_read_requests | 0 || Innodb_buffer_pool_read_requests | 0 || Innodb_buffer_pool_read_requests | 0 || Innodb_buffer_pool_read_requests | 0 || Innodb_buffer_pool_read_requests | 0 | 也可指定-c(value)参数，指定获取几次的值：1mysqladmin -uroot -p12345678 -ri1 -c5 ext | grep Innodb_buffer_pool_read_requests 根据获取到的值计算缓存命中率：1(Innodb_buffer_pool_read_requests-Innodb_buffer_pool_reads)/Innodb_buffer_pool_read_requests 如果太低，建议增大innodb_buffer_pool 2、查看缓存池使用情况缓存池使用情况参数：123Innodb_buffer_pool_pages_free:innodb buffer pool空闲页；Innodb_buffer_pool_pages_data：innodb buffer pool包含数据的页数量：Innodb_buffer_pool_pages_total：innodb buffer pool内存页总数量(每页16k)； 1234567mysqladmin -uroot -p12345678 -ri1 ext | grep Innodb_buffer_pool_pages_freemysqladmin: [Warning] Using a password on the command line interface can be insecure.| Innodb_buffer_pool_pages_free | 31974 || Innodb_buffer_pool_pages_free | 0 || Innodb_buffer_pool_pages_free | 0 || Innodb_buffer_pool_pages_free | 0 || Innodb_buffer_pool_pages_free | 0 | 1234567mysqladmin -uroot -p12345678 -ri1 ext | grep Innodb_buffer_pool_pages_datamysqladmin: [Warning] Using a password on the command line interface can be insecure.| Innodb_buffer_pool_pages_data | 96923 || Innodb_buffer_pool_pages_data | 0 || Innodb_buffer_pool_pages_data | 0 || Innodb_buffer_pool_pages_data | 0 || Innodb_buffer_pool_pages_data | 0 | 1234567mysqladmin -uroot -p12345678 -ri1 ext | grep Innodb_buffer_pool_pages_totalmysqladmin: [Warning] Using a password on the command line interface can be insecure.| Innodb_buffer_pool_pages_total | 131056 || Innodb_buffer_pool_pages_total | 0 || Innodb_buffer_pool_pages_total | 0 || Innodb_buffer_pool_pages_total | 0 || Innodb_buffer_pool_pages_total | 0 | 如果Innodb_buffer_pool_pages_free偏大的话，证明有很多缓存没有被利用到，这时可以考虑减小缓存，相反Innodb_buffer_pool_pages_data过大就考虑增大缓存。 3、另附使用mysqladmin查询qps的方法，参考链接123456789101112131415161718192021222324mysqladmin -P3306 -uroot -p12345678 -ri1 ext |\awk -F&quot;|&quot; \&quot;BEGIN&#123; count=0; &#125;&quot;\&apos;&#123; if($2 ~ /Variable_name/ &amp;&amp; ((++count)%20 == 1))&#123;\ print &quot;----------|---------|--- MySQL Command Status --|----- Innodb row operation ----|-- Buffer Pool Read --&quot;;\ print &quot;---Time---|---QPS---|select insert update delete| read inserted updated deleted| logical physical&quot;;\&#125;\else if ($2 ~ /Queries/)&#123;queries=$3;&#125;\else if ($2 ~ /Com_select /)&#123;com_select=$3;&#125;\else if ($2 ~ /Com_insert /)&#123;com_insert=$3;&#125;\else if ($2 ~ /Com_update /)&#123;com_update=$3;&#125;\else if ($2 ~ /Com_delete /)&#123;com_delete=$3;&#125;\else if ($2 ~ /Innodb_rows_read/)&#123;innodb_rows_read=$3;&#125;\else if ($2 ~ /Innodb_rows_deleted/)&#123;innodb_rows_deleted=$3;&#125;\else if ($2 ~ /Innodb_rows_inserted/)&#123;innodb_rows_inserted=$3;&#125;\else if ($2 ~ /Innodb_rows_updated/)&#123;innodb_rows_updated=$3;&#125;\else if ($2 ~ /Innodb_buffer_pool_read_requests/)&#123;innodb_lor=$3;&#125;\else if ($2 ~ /Innodb_buffer_pool_reads/)&#123;innodb_phr=$3;&#125;\else if ($2 ~ /Uptime / &amp;&amp; count &gt;= 2)&#123;\ printf(&quot; %s |%9d&quot;,strftime(&quot;%H:%M:%S&quot;),queries);\ printf(&quot;|%6d %6d %6d %6d&quot;,com_select,com_insert,com_update,com_delete);\ printf(&quot;|%6d %8d %7d %7d&quot;,innodb_rows_read,innodb_rows_inserted,innodb_rows_updated,innodb_rows_deleted);\ printf(&quot;|%10d %11d\n&quot;,innodb_lor,innodb_phr);\&#125;&#125;&apos;]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql blackhole-engine详解]]></title>
    <url>%2F2018%2F09%2F04%2Fmysql%20blackhole-engine%2F</url>
    <content type="text"><![CDATA[1、mysql 黑洞引擎就像黑洞一样，能接受数据，但把接受的数据扔掉，不存储数据，检查结果返回为空。1234567mysql&gt; CREATE TABLE test(i INT, c CHAR(10)) ENGINE = BLACKHOLE;Query OK, 0 rows affected (0.03 sec)mysql&gt; INSERT INTO test VALUES(1,&apos;record one&apos;),(2,&apos;record two&apos;);Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; SELECT * FROM test;Empty set (0.00 sec) 2、创建黑洞引擎表，数据目录下回创建表格式文件，以表明开头后缀为.frm，没有与该表关联的其他文件黑洞引擎支持所有类型的索引，定义表时，可以声明索引。12ll test.*-rw-r----- 1 mysql mysql 8578 Sep 4 06:22 test.frm 3、可以通过show engines查看是否支持黑洞引擎：1234567891011121314show engines;+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+| Engine | Support | Comment | Transactions | XA | Savepoints |+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+| InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES || CSV | YES | CSV storage engine | NO | NO | NO || MyISAM | YES | MyISAM storage engine | NO | NO | NO || BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO || PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO || MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO || ARCHIVE | YES | Archive storage engine | NO | NO | NO || MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO || FEDERATED | NO | Federated MySQL storage engine | NULL | NULL | NULL |+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ 4、黑洞引擎表不存储任何数据，但是如果启用了二进制日志，sql语句会被记录并复制到从库，这对于中继机制和过滤机制很有用。1假设您的应用程序需要从属端过滤规则，但是传输所有的二进制数据到从库会造成很大的网络流量，在这种情况下，在主库设置存储引擎为黑洞的‘dummy’从进程，如下图所示： 123主库写入二级制日志，‘dummy’mysqld 进程作为从进程执行，合并replicate-do-* 和replicate-ignore-*规则，自己写入一个新的，过滤好的二进制日志，该过滤日志传递给从库。虚拟的进程不存储任何数据，因此外的mysqld进程复制主库信息产生少量的开销。黑洞引擎表，insert触发器可正常使用，但是因为不存储数据，update和delete引擎没有激活，因为没有行，for each row触发器定义不适用。 5、黑洞引擎的用途：1231、验证转储文件语法2、启用或禁用二级制日志，来测量二进制日志带来的开销3、黑洞引擎本质上是一个无操作的引擎，因此可以用于查找与引擎本身无关的性能瓶颈。 黑洞引擎是交易感知的，提交的事物写入二进制日志，回滚事物不写入。 6、黑洞引擎和自动增量列的问题：1234主从架构数据库：1、主库黑洞表有一个自动增量字段是主键；2、从服务器有相同表引擎为myisam，3、主库执行insert，并隐式设置增量值。在这种情况下，主从复制将失败，如果主站有许多从站，则在发送到从站之前进行过滤可能会减少网络流量。在基于行的复制中，引擎为行返回的值对于每个插入始终是相同的。这将导致从服务器尝试使用主键列的相同值重播两个插入日志条目，因此复制将失败。 7、黑洞引擎和列过滤问题：1234基于行的复制，从库支持表中缺少最后一列。从库端执行过滤，在执行过滤之前，先将数据传到从库，最少有两种情况不希望将列拷贝到从库：1、数据是机密的，从库没有权限访问它；2、主库有多个从库，在发送到从库之前执行过滤用于减少网络带宽 官方文档提供的案例如下,对于受信任和不受信任从库的语句未找到合理解释，没看懂：12345678使用BLACKHOLE引擎和 --replicate-do-table或 --replicate-ignore-table选项，可以实现主列过滤，主库配置：CREATE TABLE t1 (public_col_1, ..., public_col_N, secret_col_1, ..., secret_col_M) ENGINE=MyISAM;受信任的从库配置：CREATE TABLE t1 (public_col_1, ..., public_col_N) ENGINE=BLACKHOLE;不受信任的从库配置：CREATE TABLE t1 (public_col_1, ..., public_col_N) ENGINE=MyISAM;]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Grafana] grafana监控redis、mongodb、mysql数据库]]></title>
    <url>%2F2018%2F08%2F29%2Fgrafana%2Bprometheus%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E7%9B%91%E6%8E%A7mysql%E3%80%81redis%E3%80%81mongodb%2F</url>
    <content type="text"><![CDATA[0、参考链接https://mp.weixin.qq.com/s/Qz87UE1aXFZbAdFlx7Zeow 1、grafana下载安装12wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.2.2-1.x86_64.rpmsudo yum localinstall grafana-5.2.2-1.x86_64.rpm 启动grafana12service grafana-server startservice grafana-server status 加入开机启动1chkconfig --add grafana-server 查看grafana端口12netstat -plntu | grep grafana-servtcp6 0 0 :::3000 :::* LISTEN 2539/grafana-server 登录web界面，localhost:3000用户：admin密码：admin（默认）登录之后提示修改密码,登录界面如下图所示 2、安装Prometheus下载Prometheus1wget https://github.com/prometheus/prometheus/releases/download/v2.3.2/prometheus-2.3.2.linux-amd64.tar.gz 解压安装1tar -zxvf prometheus-2.3.2.linux-amd64.tar.gz 配置 Prometheus12345678910111213141516171819202122232425262728293031cd prometheus-2.3.2.linux-amd64/cat prometheus.yml # my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093# Load rules once and periodically evaluate them according to the global &apos;evaluation_interval&apos;.rule_files: # - &quot;first_rules.yml&quot; # - &quot;second_rules.yml&quot;# A scrape configuration containing exactly one endpoint to scrape:# Here it&apos;s Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: &apos;prometheus&apos; # metrics_path defaults to &apos;/metrics&apos; # scheme defaults to &apos;http&apos;. static_configs: - targets: [&apos;0.0.0.0:9090&apos;] 启动prometheus12345678910111213141516./prometheus --config.file=prometheus.yml----level=info ts=2018-08-28T03:42:08.966212621Z caller=main.go:222 msg=&quot;Starting Prometheus&quot; version=&quot;(version=2.3.2, branch=HEAD, revision=71af5e29e815795e9dd14742ee7725682fa14b7b)&quot;level=info ts=2018-08-28T03:42:08.966876421Z caller=main.go:223 build_context=&quot;(go=go1.10.3, user=root@5258e0bd9cc1, date=20180712-14:02:52)&quot;level=info ts=2018-08-28T03:42:08.966953021Z caller=main.go:224 host_details=&quot;(Linux 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018 x86_64 dax-mysql-mha (none))&quot;level=info ts=2018-08-28T03:42:08.966990021Z caller=main.go:225 fd_limits=&quot;(soft=1024, hard=4096)&quot;level=info ts=2018-08-28T03:42:08.968101421Z caller=main.go:533 msg=&quot;Starting TSDB ...&quot;level=info ts=2018-08-28T03:42:08.970314221Z caller=web.go:415 component=web msg=&quot;Start listening for connections&quot; address=0.0.0.0:9090level=info ts=2018-08-28T03:42:08.977918922Z caller=main.go:543 msg=&quot;TSDB started&quot;level=info ts=2018-08-28T03:42:08.977966022Z caller=main.go:603 msg=&quot;Loading configuration file&quot; filename=prometheus.ymllevel=info ts=2018-08-28T03:42:08.979126022Z caller=main.go:629 msg=&quot;Completed loading of configuration file&quot; filename=prometheus.ymllevel=info ts=2018-08-28T03:42:08.979171222Z caller=main.go:502 msg=&quot;Server is ready to receive web requests.&quot;level=info ts=2018-08-28T05:00:04.558075427Z caller=compact.go:398 component=tsdb msg=&quot;write block&quot; mint=1535421600000 maxt=1535428800000 ulid=01CNZEEBGJ237RVH965D3VWT57level=info ts=2018-08-28T05:00:04.566214242Z caller=head.go:348 component=tsdb msg=&quot;head GC completed&quot; duration=1.539902mslevel=info ts=2018-08-28T05:00:04.575381958Z caller=head.go:357 component=tsdb msg=&quot;WAL truncation completed&quot; duration=9.095816ms----- 登录控制台 localhost:9090,登录界面如下： 3、mysql_exporter部署123wget https://github.com/prometheus/mysqld_exporter/releases/download/v0.11.0/mysqld_exporter-0.11.0.linux-amd64.tar.gztar -zxvf mysqld_exporter-0.11.0.linux-amd64.tar.gz cd mysqld_exporter-0.11.0.linux-amd64/ 配置连接数据库文件12345cat .my.cnf[client]host = 127.0.0.1user=rootpassword=12345678 启动mysqld_exporter1234567891011./mysqld_exporter --config.my-cnf=&quot;/data/soft/mysqld_exporter-0.11.0.linux-amd64/.my.cnf&quot; ----INFO[0000] Starting mysqld_exporter (version=0.11.0, branch=HEAD, revision=5d7179615695a61ecc3b5bf90a2a7c76a9592cdd) source=&quot;mysqld_exporter.go:206&quot;INFO[0000] Build context (go=go1.10.3, user=root@3d3ff666b0e4, date=20180629-15:00:35) source=&quot;mysqld_exporter.go:207&quot;INFO[0000] Enabled scrapers: source=&quot;mysqld_exporter.go:218&quot;INFO[0000] --collect.global_status source=&quot;mysqld_exporter.go:222&quot;INFO[0000] --collect.global_variables source=&quot;mysqld_exporter.go:222&quot;INFO[0000] --collect.slave_status source=&quot;mysqld_exporter.go:222&quot;INFO[0000] --collect.info_schema.tables source=&quot;mysqld_exporter.go:222&quot;INFO[0000] Listening on :9104 source=&quot;mysqld_exporter.go:232&quot;---- 查看进程是否启动正常：1netstat -tunlp|grep 9104 登录web界面,http://localhost:9104/修改vim prometheus.yml配置文件12345- job_name: mysql static_configs: - targets: [&apos;127.0.0.1:9104&apos;] labels: instance: test 备注:job_name,当前执行job的名字targets，连接的主机instance，数据库实例的标签明 重新启动prometheus，进入prometheus控制台，点击status-&gt;target 4、添加数据源保存测试 5、下载仪表盘https://github.com/percona/grafana-dashboards/tree/master/dashboards选择仪表盘类型，导入到grafana点击仪表盘查看 6、mongodb_exporter部署用于监控mongodbprometheus监控mongodb数据库，安装之前需要配置go环境下载配置go环境12wget https://dl.google.com/go/go1.11.linux-amd64.tar.gztar -C /usr/local -xzf go1.11.linux-amd64.tar.gz 添加环境变量123echo &quot;GOPATH=$HOME/goexport PATH=$PATH:$GOPATH/bin&quot; &gt;&gt; ~/.bash_profile 使配置的环境变量生效1source ~/.bash_profile 创建配置mognodb所需要的目录1mkdir -p $HOME/go/&#123;dir,src,pkg&#125; 安装glide1curl https://glide.sh/get | sh 安装mongodb-exporter123git clone git@github.com:dcu/mongodb_exporter.git $GOPATH/src/github.com/dcu/mongodb_exportercd $GOPATH/src/github.com/dcu/mongodb_exportermake build 查看mongdob_exporter使用的参数1./mongodb_exporter -h 启动mongodb_exporter,获取数据库的数据1./mongodb_exporter -mongodb.uri mongodb://localhost:30000 启动端口默认为9001,登录http://localhost:9001，查看是否正常修改prometheus.yml，添加mongodb参数123- job_name: mongodb static_configs: - targets: [&apos;127.0.0.1:9001&apos;] 重启premetheus,登录premetheus控制台查看获取mognodb是否正常登录grafana添加模板，可以添加模板id或者导入代码点击链接，跳转到mongodb_exporter模板添加完成模板如下图所示： 7、redis_exporter部署用于监控redis,安装redis-exporter模板123go get github.com/oliver006/redis_exportercd $GOPATH/src/github.com/oliver006/redis_exportergo build 启动redis_exporter进程获取redis数据(默认进程端口号位9121)1./redis_exporter -redis.addr redis://127.0.0.1:6379 -redis.password test 启动端口默认为9001,登录http://localhost:9121，查看是否正常修改prometheus.yml，添加redis参数123- job_name: redis static_configs: - targets: [&apos;127.0.0.1:9121&apos;] 重启premetheus,登录premetheus控制台查看获取redis是否正常登录grafana添加模板，可以添加模板id或者导入代码,点击此处跳转到redis_exporter链接，导入完成之后界面如下图所示：]]></content>
      <tags>
        <tag>mysql</tag>
        <tag>mongodb</tag>
        <tag>redis</tag>
        <tag>grafana</tag>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Zabbix] 解决zabbix中文乱码问题]]></title>
    <url>%2F2018%2F08%2F28%2F%E8%A7%A3%E5%86%B3zabbix%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[1、在windows机器上查找simkai.ttf文件，在windows下拷贝simkai字体到zabbix server端，并拷贝到相应目录可在c盘路径下搜索simkai.ttf，本地机器的路径为‘C:\Windows\WinSxS\amd64_microsoft-windows-font-truetype-simkai_31bf3856ad364e35_10.0.17134.1_none_d7e1e1a5de638405’，每台机器对应的目录会有差异，但‘C:\Windows\WinSxS’路径一致。 2、将找到的simkai文件上传到zabbix-server端nginx的安装目录下1mv simkai.ttf /data/nginx-1.9.15/html/zabbix/fonts/ 3、修改nginx配置文件1sed -i &apos;s/DejaVuSans/simkai/g&apos; /data/nginx-1.9.15/html/zabbix/include/defines.inc.php 4、重启php-fpm、nginx1234nginx -s stopservice php-fpm stopservice php-fpm startnginx]]></content>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Zabbix] zabbix监控磁盘io总吞吐量批量安装配置]]></title>
    <url>%2F2018%2F08%2F27%2Fzabbix%E7%9B%91%E6%8E%A7%E7%A3%81%E7%9B%98io%E6%80%BB%E5%90%9E%E5%90%90%E9%87%8F%2F</url>
    <content type="text"><![CDATA[0、编辑监控磁盘io总吞吐量脚本cat /home/zabbix/disk_io_check.sh1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253sudo bash#安装iostat命令yum install -y sysstat #创建监控目录及数据文件mkdir -p /usr/local/zabbix/scripttouch /usr/local/zabbix/script/iostat_data.txtchown zabbix.zabbix /usr/local/zabbix/script/iostat_data.txtchmod 777 /usr/local/zabbix/script/iostat_data.txttouch /usr/local/zabbix/script/iostat_collect_data.shchown zabbix.zabbix /usr/local/zabbix/script/iostat_collect_data.shchmod 777 /usr/local/zabbix/script/iostat_collect_data.sh#编辑收集数据脚本echo &quot;#!/bin/bashiostat -dxkt 1 2|grep -E &apos;^sd|^xvda|^vd&apos; &gt; /usr/local/zabbix/script/iostat_data.txt&quot; &gt; /usr/local/zabbix/script/iostat_collect_data.sh#添加到定时任务，每分钟收集一次echo &quot;* * * * * root sh /usr/local/zabbix/script/iostat_collect_data.sh&quot; &gt;&gt; /etc/crontab#添加参数到zabbix配置文件echo &quot;#每秒进行 merge 的读操作数目。即 rmerge/sUserParameter=disk.status.rmerge,cat /usr/local/zabbix/script/iostat_data.txt|tail -n \`iostat -dxk 1 1|grep -E &apos;^sd|^xvda|^vd&apos;|wc -l\` |awk &apos;&#123;print \$2&#125;&apos;|awk &apos;&#123;sum+=\$1&#125; END &#123;print sum&#125;&apos; #每秒进行 merge 的写操作数目。即 wmerge/sUserParameter=disk.status.wmerge,cat /usr/local/zabbix/script/iostat_data.txt |tail -n \`iostat -dxk 1 1|grep -E &apos;^sd|^xvda|^vd&apos;|wc -l\` |awk &apos;&#123;print \$3&#125;&apos;|awk &apos;&#123;sum+=\$1&#125; END &#123;print sum&#125;&apos;#每秒完成的读 I/O 设备次数。即 rio/sUserParameter=disk.status.rio,cat /usr/local/zabbix/script/iostat_data.txt |tail -n \`iostat -dxk 1 1|grep -E &apos;^sd|^xvda|^vd&apos;|wc -l\` |awk &apos;&#123;print \$4&#125;&apos;|awk &apos;&#123;sum+=\$1&#125; END &#123;print sum&#125;&apos;#每秒完成的写 I/O 设备次数。即 wio/sUserParameter=disk.status.wio,cat /usr/local/zabbix/script/iostat_data.txt |tail -n \`iostat -dxk 1 1|grep -E &apos;^sd|^xvda|^vd&apos;|wc -l\` |awk &apos;&#123;print \$5&#125;&apos;|awk &apos;&#123;sum+=\$1&#125; END &#123;print sum&#125;&apos;#每秒读K字节数。是 rsect/s 的一半，因为每扇区大小为512字节。UserParameter=disk.status.rsect,cat /usr/local/zabbix/script/iostat_data.txt |tail -n \`iostat -dxk 1 1|grep -E &apos;^sd|^xvda|^vd&apos;|wc -l\` |awk &apos;&#123;print \$6&#125;&apos;|awk &apos;&#123;sum+=\$1&#125; END &#123;print sum&#125;&apos;#每秒写K字节数。是 wsect/s 的一半。UserParameter=disk.status.wsect,cat /usr/local/zabbix/script/iostat_data.txt |tail -n \`iostat -dxk 1 1|grep -E &apos;^sd|^xvda|^vd&apos;|wc -l\` |awk &apos;&#123;print \$7&#125;&apos;|awk &apos;&#123;sum+=\$1&#125; END &#123;print sum&#125;&apos;#平均每次设备I/O操作的数据大小 (扇区)。UserParameter=disk.status.sectsize,cat /usr/local/zabbix/script/iostat_data.txt |tail -n \`iostat -dxk 1 1|grep -E &apos;^sd|^xvda|^vd&apos;|wc -l\` |awk &apos;&#123;print \$8&#125;&apos;|awk &apos;&#123;sum+=\$1&#125; END &#123;print sum&#125;&apos;#平均I/O队列长度。UserParameter=disk.status.queuelength,cat /usr/local/zabbix/script/iostat_data.txt |tail -n \`iostat -dxk 1 1|grep -E &apos;^sd|^xvda|^vd&apos;|wc -l\` |awk &apos;&#123;print \$9&#125;&apos;|awk &apos;&#123;sum+=\$1&#125; END &#123;print sum&#125;&apos;#平均每次设备I/O操作的等待时间 (毫秒)。UserParameter=disk.status.avgiowait,cat /usr/local/zabbix/script/iostat_data.txt |tail -n \`iostat -dxk 1 1|grep -E &apos;^sd|^xvda|^vd&apos;|wc -l\` |awk &apos;&#123;print \$10&#125;&apos;|awk &apos;&#123;sum+=\$1&#125; END &#123;print sum&#125;&apos;#平均每次设备I/O读操作的等待时间 (毫秒)。UserParameter=disk.status.avgreadiowait,cat /usr/local/zabbix/script/iostat_data.txt |tail -n \`iostat -dxk 1 1|grep -E &apos;^sd|^xvda|^vd&apos;|wc -l\` |awk &apos;&#123;print \$11&#125;&apos;|awk &apos;&#123;sum+=\$1&#125; END &#123;print sum&#125;&apos;#平均每次设备I/O写操作的等待时间 (毫秒)。UserParameter=disk.status.avgwriteiowait,cat /usr/local/zabbix/script/iostat_data.txt |tail -n \`iostat -dxk 1 1|grep -E &apos;^sd|^xvda|^vd&apos;|wc -l\` |awk &apos;&#123;print \$12&#125;&apos;|awk &apos;&#123;sum+=\$1&#125; END &#123;print sum&#125;&apos;#平均每次设备I/O操作的服务时间 (毫秒)。UserParameter=disk.status.svctm,cat /usr/local/zabbix/script/iostat_data.txt |tail -n \`iostat -dxk 1 1|grep -E &apos;^sd|^xvda|^vd&apos;|wc -l\` |awk &apos;&#123;print \$13&#125;&apos;|awk &apos;&#123;sum+=\$1&#125; END &#123;print sum&#125;&apos;# 一秒中有百分之多少的时间用于 I/O 操作，即被io消耗的cpu百分比UserParameter=disk.status.util,cat /usr/local/zabbix/script/iostat_data.txt |tail -n \`iostat -dxk 1 1|grep -E &apos;^sd|^xvda|^vd&apos;|wc -l\` |awk &apos;&#123;print \$14&#125;&apos;|awk &apos;&#123;sum+=\$1&#125; END &#123;print sum&#125;&apos;&quot; &gt; /usr/local/zabbix/etc/zabbix_agentd.conf.d/iostat_data.confecho &quot;Include=/usr/local/zabbix/etc/zabbix_agentd.conf.d&quot; &gt;&gt; /usr/local/zabbix/etc/zabbix_agentd.conf#重新启动zabbix_agent客户端/etc/init.d/zabbix_agentd restart#查看监控进程是否启动netstat -tunlp|grep 10050 1、如果需要批量安装，在ansible-server端配置批量上传脚本cat /etc/ansible/scopy-diskio-zabbix.sh12345- hosts: test remote_user: centos tasks: - name: scopy zabbix-agent config to all hosts copy: src=&quot;/home/zabbix/disk_io_check.sh&quot; dest=&quot;/home/zabbix/disk_io_check.sh&quot; 执行批量上传脚本1ansible-playbook /etc/ansible/scopy-diskio-zabbix.sh 执行脚本批量配置监控磁盘io总吞吐量1ansible test -m command -a &apos;sudo sh /home/centos/execute-zabbix.sh&apos; 2、web端配置监控模板，亦可自行编译，点击此处下载]]></content>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Zabbix] zabbix-agent批量安装配置]]></title>
    <url>%2F2018%2F08%2F27%2Fzabbix-agent%E6%89%B9%E9%87%8F%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[0、本文批量安装使用ansible加shell脚本的方法，首先安装ansible1yum -y install ansible 1、修改ansible-server端免交互登录cat /etc/ssh/ssh_config12StrictHostKeyChecking noUserKnownHostsFile /dev/null 修改好配置后，重新启动sshd服务即可，命令为：/etc/init.d/sshd restart （或 service sshd restart 或/bin/systemctl restart sshd.service） 2、编辑/etc/ansible/hosts主机配置文件cat /etc/ansible/hosts1234[test]172.29.25.196 ansible_ssh_private_key_file=/root/test ansible_ssh_user=centos 172.29.16.6 ansible_ssh_private_key_file=/root/test ansible_ssh_user=centos172.29.20.97 ansible_ssh_private_key_file=/root/test ansible_ssh_user=centos hosts文件使用参数详解1234567891011ansible_ssh_host #用于指定被管理的主机的真实IPansible_ssh_port #用于指定连接到被管理主机的ssh端口号，默认是22ansible_ssh_user #ssh连接时默认使用的用户名ansible_ssh_pass #ssh连接时的密码ansible_sudo_pass #使用sudo连接用户时的密码ansible_sudo_exec #如果sudo命令不在默认路径，需要指定sudo命令路径 ansible_ssh_private_key_file #秘钥文件路径，秘钥文件如果不想使用ssh-agent管理时可以使用此选项 ansible_shell_type #目标系统的shell的类型，默认sh ansible_connection #SSH 连接的类型： local , ssh , paramiko，在 ansible 1.2 之前默认是 paramiko ，后来智能选择，优先使用基于 ControlPersist 的 ssh(支持的前提)ansible_python_interpreter #用来指定python解释器的路径，默认为/usr/bin/python 同样可以指定ruby 、perl 的路径 ansible_*_interpreter #其他解释器路径，用法和ansible_python_interpreter类似，这里&quot;*&quot;可以是ruby或才perl等其他语言 上例中使用的是私钥和用户名，如果想要使用执行端口，密码，用户名可参考上面详解参数。 3、编辑批量上传脚本cat /etc/ansible/scopy-test-zabbix.sh1234567- hosts: test remote_user: centos tasks: - name: scopy zabbix to all hosts copy: src=&quot;/root/zabbix-3.4.2.tar.gz&quot; dest=&quot;/home/centos/zabbix-3.4.2.tar.gz&quot; - name: scopy zabbix-agent config to all hosts copy: src=&quot;/home/centos/execute-zabbix.sh&quot; dest=&quot;/home/centos/execute-zabbix.sh&quot; src:代表本机的源文件，dest代表目标端的路径hosts：要与/etc/ansible/hosts下的[test]保持一致。 execute-zabbix.sh脚本为安装zabbix-agent脚本cat /home/centos/execute-zabbix.sh12345678910111213141516171819202122232425#!/bin/sh mkdir /home/zabbix useradd -r -s /sbin/nologin zabbix yum -y install gcc gcc-c++ pcre-devel openssl-devel mv /home/centos/zabbix-3.4.2.tar.gz /home/zabbix/cd /home/zabbix tar -zxvf zabbix-3.4.2.tar.gzcd zabbix-3.4.2 /home/zabbix/zabbix-3.4.2/configure --prefix=/usr/local/zabbix --enable-agent --with-openssl make make install#host1=`ifconfig eth0 |awk -F &apos;[ :]+&apos; &apos;NR==2 &#123;print $3&#125;&apos;`host1=`ip addr |grep dynamic |awk &apos;&#123;print $2&#125;&apos;|awk -F&apos;/&apos; &apos;&#123;print $1&#125;&apos;`echo &quot;LogFile=/tmp/zabbix_agentd.logServer=172.29.12.85ServerActive=172.29.12.85Hostname=$host1&quot;&gt; /usr/local/zabbix/etc/zabbix_agentd.confcp /home/zabbix/zabbix-3.4.2/misc/init.d/fedora/core/zabbix_agentd /etc/init.d/ ln -s /usr/local/zabbix/etc/zabbix_agentd.conf /usr/local/etc/zabbix_agentd.confsed -i &apos;s/BASEDIR=\/usr\/local/BASEDIR=\/usr\/local\/zabbix/g&apos; /etc/init.d/zabbix_agentdservice zabbix_agentd startchkconfig --add zabbix_agentdchkconfig --level 35 zabbix_agentd on netstat -lnpt | grep zabbix_agent server端的ip地址根据自己的zabbix-proxy或者zabbix-server自行定义。 执行批量脚本将文件上传到目标端：1ansible-playbook /etc/ansible/scopy-test-zabbix.sh 在ansible-server端执行批量安装脚本1ansible test -m command -a &apos;sudo sh /home/centos/execute-zabbix.sh&apos; 4、编辑查看zabbix-agent进程的脚本，并批量上传到被监控端编辑检查zabbix-agent进程脚本cat /home/centos/check-zabbix-port.sh12#!/bin/bashnetstat -tunlp |grep 10050 编辑批量上传脚本cat /etc/ansible/scopy-test-check-zabbixport.sh12345- hosts: test remote_user: centos tasks: - name: scopy zabbix-check-port to all hosts copy: src=&quot;/home/centos/check-zabbix-port.sh&quot; dest=&quot;/home/centos/check-zabbix-port.sh&quot; 执行批量上传脚本1ansible-playbook /etc/ansible/scopy-test-check-zabbixport.sh 在ansible-server服务端执行检查脚本1ansible test -m command -a &apos;sudo sh /home/centos/check-zabbix-port.sh&apos;]]></content>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Zabbix] zabbix-server安装配置]]></title>
    <url>%2F2018%2F08%2F27%2Fzabbix-server%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[0、安装zabbix-server之前需要把server依赖环境lnmp安装完成，首先安装依赖包：12yum groupinstall &quot;Development tools&quot; yum -y install gcc gcc-c++ cmake autoconf libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel zlib zlib-devel glibc glibc-devel glib2 glib2-devel bzip2 bzip2-devel ncurses ncurses-devel curl curl-devel e2fsprogs e2fsprogs-devel krb5 krb5-devel libidn libidn-devel openssl openssl-devel openldap openldap-devel nss_ldap openldap-clients openldap-servers pcre-devel libaio 1、上传nginx软件包到服务器，如果没有可点击此处下载创建nginx用户，创建nginx安装目录12useradd nginx -s /sbin/nologin -M mkdir /application 解压安装包、编译安装1234tar -zxvf nginx-1.9.15.tar.gzcd nginx-1.9.15./configure --prefix=/application/nginx-1.9.15 --user=nginx --group=nginx --with-http_stub_status_module --with-http_ssl_module make &amp;&amp; make install 创建软链接1ln -s /application/nginx-1.9.15 /application/nginx 检查语法1/application/nginx/sbin/nginx -t 启动nginx1/application/nginx/sbin/nginx 查看进程是否正常启动1netstat -tunlp|grep nginx 关闭nginx服务1/application/nginx/sbin/nginx -s stop 2、Mysql数据库安装下载mysql数据库(可自行下载自己喜欢的版本）1wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.18-linux-glibc2.5-x86_64.tar.gz 添加用户1useradd mysql -s /sbin/nologin -M 解压、重命名12tar -zxvf mysql-5.7.18-linux-glibc2.5-x86_64.tar.gz mv mysql-5.7.18-linux-glibc2.5-x86_64 /application/mysql 改变权限权限、初始化,初始化过程中会默认生成随机密码，记录以便后面更改：123chown -R mysql.mysql /application/mysql/ /application/mysql/bin/mysqld --initialize --user=mysql --basedir=/application/mysql/ --datadir=/application/mysql/data//application/mysql/bin/mysql_ssl_rsa_setup --datadir=/application/mysql/data/ 修改配置文件cat /application/mysql/support-files/my_default.cnf12345678910[mysqld]user = mysqlport = 3306server_id = 1socket=/tmp/mysql.sockbasedir =/application/mysql/datadir =/application/mysql/data/character-set-server=utf8[client]socket=/tmp/mysql.sock 配合mysql启动命令：123cp my_default.cnf /etc/my.cnfcp mysql.server /etc/init.d/mysqldchmod 755 /etc/init.d/mysqld 修改/etc/init.d/mysqld，文件数据和安装路径12basedir=/application/mysql/datadir=/application/mysql/data/ 启动mysql1/etc/init.d/mysqld start 修改mysql密码1234mysql -u root -pset password=password(&apos;root&apos;);grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;root&apos;;flush privileges; 3、安装php服务安装依赖包1yum install -y zlib libxml2 libjpeg freetype libpng gd curl libiconv zlib-devel libxml2-devel libjpeg-devel freetype-devel libpng-devel gd-devel curl-devel openssl-devel libxslt-devel libxslt* 安装libiconv,下载链接123456tar -zxvf libiconv-1.14.tar.gz cd libiconv-1.14./configure --prefix=/usr/local/libiconv make &amp;&amp; make install``` 安装Libmcrypt,下载[链接](https://pan.baidu.com/s/1SH5TujhLgN9EkjjWI97gQQ) tar -zxvf libmcrypt-2.5.8.tar.gzcd libmcrypt-2.5.8./configuremake &amp;&amp; make install /sbin/ldconfigcd libltdl/./configure –enable-ltdl-installmake &amp;&amp; make installcd ../../1安装mhash,下载[链接](https://pan.baidu.com/s/1SH5TujhLgN9EkjjWI97gQQ) tar -zxvf mhash-0.9.9.9.tar.gzcd mhash-0.9.9.9./configuremake &amp;&amp; make installcd ../rm -f /usr/lib64/libmcrypt.rm -f /usr/lib64/libmhashln -s /usr/local/lib/libmcrypt.la /usr/lib64/libmcrypt.laln -s /usr/local/lib/libmcrypt.so /usr/lib64/libmcrypt.soln -s /usr/local/lib/libmcrypt.so.4 /usr/lib64/libmcrypt.so.4ln -s /usr/local/lib/libmcrypt.so.4.4.8 /usr/lib64/libmcrypt.so.4.4.8ln -s /usr/local/lib/libmhash.a /usr/lib64/libmhash.aln -s /usr/local/lib/libmhash.la /usr/lib64/libmhash.laln -s /usr/local/lib/libmhash.so /usr/lib64/libmhash.soln -s /usr/local/lib/libmhash.so.2 /usr/lib64/libmhash.so.2ln -s /usr/local/lib/libmhash.so.2.0.1 /usr/lib64/libmhash.so.2.0.1ln -s /usr/local/lib/libmcrypt-config /usr/lib/libmcrypt-config1安装mcrypt,下载[链接](https://pan.baidu.com/s/1SH5TujhLgN9EkjjWI97gQQ) tar -xf mcrypt-2.6.8.tar.gzcd mcrypt-2.6.8//sbin/ldconfig./configure LD_LIBRARY=/usr/local/lib &amp;&amp; make &amp;&amp;make installcd ..1安装php,下载[链接](https://pan.baidu.com/s/1SH5TujhLgN9EkjjWI97gQQ) tar -zxvf php-5.6.11.tar.gzcd php-5.6.11./configure –prefix=/application/php-5.6.11 –with-mysql=/application/mysql –with-iconv-dir=/usr/local/libiconv –with-freetype-dir –with-jpeg-dir –with-png-dir –with-zlib –with-libxml-dir=/usr –enable-xml –disable-rpath –enable-safe-mode –enable-bcmath –enable-shmop –enable-sysvsem –enable-inline-optimization–with-curl–with-curlwrappers –enable-mbregex –enable-fpm –enable-mbstring –with-mcrypt –with-gd –enable-gd-native-ttf –with-openssl –with-mhash –enable-pcntl –enable-sockets –with-xmlrpc–enable-zip –enable-soap –enable-short-tags –enable-zend-multibyte –enable-static –with-xsl –with-fpm-user=nginx –with-fpm-group=nginx –enable-ftp –enable-pdo–with-pdo-mysqlmake &amp;&amp; make installcd ..1安装过程提示缺少lib包的问题，根据版本差异执行以下命令： ln -s /application/mysql/lib/libmysqlclient.so.18 /usr/lib64/或者ln -s /application/mysql/lib/libmysqlclient.so.20 /usr/lib64/1配置文件（php.ini和php-fpm） ln -s /application/php-5.6.11 /application/phpcp ./php-5.6.11/php.ini-production /application/php/lib/php.inicat /application/php/etc/php-fpm.conf.default&gt;/application/php/etc/php-fpm.conf1检查语法、启动 /application/php/sbin/php-fpm -t/application/php/sbin/php-fpmcp /root/php-5.6.11/sapi/fpm/init.d.php-fpm /etc/init.d/php-fpmchmod a+x /etc/init.d/php-fpm 加入开机启动chkconfig –add php-fpmchkconfig –level 35 php-fpm on 启动 php-fpmservice php-fpm startnetstat -anpt | grep php-fpm 停止 php-fpmservice php-fpm stop12## 4、配置nginx使其支持phpcat /application/nginx/conf/nginx.conf server {listen 80;server_name localhost; location / {root html;index index.php index.html index.htm; # 加入 index.php} error_page 500 502 503 504 /50x.html;location = /50x.html {root html;} location ~ .php$ { # 整段注释去掉root html;fastcgi_pass 127.0.0.1:9000;fastcgi_index index.php;fastcgi_param SCRIPT_FILENAME /application/nginx/html$fastcgi_script_name; # 修改为自己的根目录include fastcgi_params;}}123创建测试页面```bashecho &quot;&lt;?php phpinfo(); ?&gt;&quot; &gt; /application/nginx/html/info.php 重启 Nginx 、php-fpm1234service php-fpm stopservice php-fpm start/application/nginx/sbin/nginx -s stop/application/nginx/sbin/nginx 浏览器访问测试页面1localhost/info.php 5、安装zabbix-server，下载链接安装依赖包1yum -y install ntpdate net-snmp net-snmp-devel libcurl-devel libevent-devel 设置zabbix用户，编译安装zabbix1234567891011useradd -r -s /sbin/nologin zabbixtar -zxvf zabbix-3.4.2.tar.gzcd zabbix-3.4.2./configure --prefix=/usr/local/zabbix --enable-server --enable-agent --with-mysql --with-net-snmp --with-libcurl --with-libxml2 --with-openssl # --prefix 指定安装路径# --enable-server 安装 Server 端# --enable-agent 安装 Agent 端# --with-mysql 使用 Mysql 数据库# --with-net-snmp 支持 SNMP 协议# --with-libcurl 支持 libcurl URL 监控make &amp;&amp; make install mysql数据库添加zabbix初始化数据123456789shell &gt; mysql -uroot -prootmysql&gt; create database zabbixDB character set utf8; # 创建 zabbixDB 并设置编码为 utf8mysql&gt; grant all on zabbixDB.* to &apos;zabbix&apos;@&apos;%&apos; identified by &apos;zabbix&apos;; # 建立授权用户mysql&gt; flush privileges; # 刷新授权表 ( 虽然 grant 操作是不需要刷新授权表的，但那又如何 ? )mysql&gt; use zabbixDB;mysql&gt; source /root/zabbix-3.4.2/database/mysql/schema.sql # 导入数据mysql&gt; source /root/zabbix-3.4.2/database/mysql/images.sqlmysql&gt; source /root/zabbix-3.4.2/database/mysql/data.sqlmysql&gt; quit 配置 Zabbix 服务123456789# 服务端启动脚本shell &gt; cp /root/zabbix-3.4.2/misc/init.d/fedora/core/zabbix_server /etc/init.d/# 客户端启动脚本shell &gt; cp /root/zabbix-3.4.2/misc/init.d/fedora/core/zabbix_agentd /etc/init.d/# 网页文件shell &gt; cp -R /root/zabbix-3.4.2/frontends/php/ /data/nginx/html/zabbix ``` 服务端配置文件vim /usr/local/zabbix/etc/zabbix_server.conf LogFile=/tmp/zabbix_server.log # 日志文件存放位置DBName=zabbixDB # 数据库名DBUser=zabbix # 连接用户DBPassword=zabbix # 连接密码1服务端启动脚本 shell &gt; vim /etc/init.d/zabbix_serverBASEDIR=/usr/local/zabbix # 修改后的位置 ( 原：/usr/local )1客户端启动脚本 shell &gt;vim /etc/init.d/zabbix_agentdBASEDIR=/usr/local/zabbix # 修改后的位置 ( 原：/usr/local )1配置zabbix开机启动，并启动Zabbix服务 shell &gt; chkconfig –add zabbix_servershell &gt; chkconfig –add zabbix_agentdshell &gt; chkconfig –level 35 zabbix_server onshell &gt; chkconfig –level 35 zabbix_agentd onshell &gt; service zabbix_server startStarting zabbix_server: [确定]shell &gt; service zabbix_agentd startStarting zabbix_agentd: [确定]1查看进程启动是否正常 shell &gt; netstat -anpt | grep zabbix ( 注意：要来确认一下到底有没有启动成功，因为当授权用户无法连接数据库时，zabbix_server 是无法启动的，但是启动过程显示成功 )tcp 0 0 0.0.0.0:10050 0.0.0.0: LISTEN 75399/zabbix_agentdtcp 0 0 0.0.0.0:10051 0.0.0.0: LISTEN 75321/zabbix_server 123456## 6、登陆网页进行配置 Zabbix ( http://your-domain/zabbix )（初始用户名密码为admin/zabbix）&gt; 第一个页面是欢迎页面，直接 Next&gt; 第二个页面大多会有多处检测失败，也是出问题最多的位置,修改php文件一下参数：vim /data/php-5.6.11/lib/php.ini post_max_size = 16Mmax_execution_time = 300max_input_time = 300date.timezone = Asia/Shanghaialways_populate_raw_post_data = -11根据页面提示报错还需要修改以下配置： cd /root/php-5.6.11/ext/openssl/cp ./config0.m4 ./config.m4 #重新编译php，添加mysqli模块cd /root/php-5.6.11/ext/mysqli/data/php/bin/phpize./configure –prefix=/data/mysqli –with-php-config=/data/php-5.6.11/bin/php-config –with-mysqli=/data/mysql/bin/mysql_configmakemake testmake install12配置完成后在php.ini下加入下来一行vim /data/php-5.6.11/lib/php.ini extension=mysqli.so12安装过程中的其他报错： fatal error: ext/mysqlnd/mysql_float_to_double.h: No such file or directory1vim /root/php-5.6.11/ext/mysqli/mysqli_api.c 把第36行的 #include “ext/mysqlnd/mysql_float_to_double.h”修改为 #include “/root/php-5.6.11/ext/mysqlnd/mysql_float_to_double.h”123## 7、zabbix-proxy代理端安装依赖包安装 yum -y install ntpdate net-snmp net-snmp-devel libcurl-devel libevent-devel pcre-devel1添加zabbix用户，并编译安装zabbix-proxy useradd -r -s /sbin/nologin zabbixtar -zxvf zabbix-3.4.2.tar.gzcd zabbix-3.4.2yum install pcre-devel./configure –prefix=/usr/local/zabbix –enable-agent –enable-proxy –with-mysql # –enable-agent 不是必须的 ( 如果不想监控 Proxy 的话 )make &amp;&amp; make install1复制 Agent 启动脚本 cp /usr/local/src/zabbix-2.4.5/misc/init.d/fedora/core/zabbix_agentd /etc/init.d/1修改 Zabbix 安装路径 sed -i ‘s#BASEDIR=/usr/local#BASEDIR=/usr/local/zabbix#’ /etc/init.d/zabbix_agentd12将proxy代理所需要的初始化数据导入到mysql数据库： mysql -u root -pmysql&gt; create database proxydb character set utf8; # 创建数据库mysql&gt; grant all on proxydb.* to ‘proxy‘@’%’ identified by ‘proxydb’; # 创建授权用户mysql&gt; flush privileges; # 刷新授权表，虽然不需要~mysql&gt; use proxydb;mysql&gt; source /root/zabbix-3.4.2/database/mysql/schema.sql # 导入数据，只导入这一个就可以了12修改zabbix_proxy.conf配置文件 [root@node1 ~]# grep -vP ‘^$|#’ /usr/local/zabbix/etc/zabbix_proxy.confProxyMode=0Server=192.168.197.135ServerPort=10051Hostname=node1LogFile=/tmp/zabbix_proxy.logDBHost=localhostDBName=proxydbDBUser=proxyDBPassword=proxydbConfigFrequency=60DataSenderFrequency=60Timeout=4LogSlowQueries=30001参数详解： ProxyMode=0 # 0 代表 Proxy 处于主动模式，即：Proxy 主动去请求 Zabbix Server 获取监控项；1 代表被动模式Server=192.168.214.40 # Zabbix Server 地址，当 Proxy 处于被动模式时，不需要设置该项 ( 想想也能明白 )ServerPort=10051 # Zabbix Server 监听端口，同上只在 Proxy 为主动模式时生效Hostname=my_proxy # 这个很重要啦，跟 Agent 的 Hostname 一样重要，待会要用LogFile=/tmp/zabbix_proxy.log # Proxy 日志文件位置DBHost=localhost # 连接哪里的数据库DBName=proxydb # 数据库名DBUser=proxy # 连接用户DBPassword=proxypass # 用户密码ConfigFrequency=60 # Proxy 向 Zabbix Server 请求监控项间隔，单位为 秒DataSenderFrequency=60 # Proxy 向 Zabbix Server 发送监控数据间隔，单位为 秒1启动 zabbix_proxy /usr/local/zabbix/sbin/zabbix_proxy1启动报错如下 /usr/local/zabbix/sbin/zabbix_proxy: error while loading shared libraries: libmysqlclient.so.20: cannot open shared object file: No such file or directory1解决方法 ln -s /application/mysql/lib/libmysqlclient.so.20 /usr/lib64/ 1查看是否启动成功： netstat -lnpt | grep zabbix_proxytcp 0 0 0.0.0.0:10051 0.0.0.0:* LISTEN 80154/zabbix_proxy1加入开机启动 echo “/usr/local/zabbix/sbin/zabbix_proxy” &gt;&gt; /etc/rc.local`]]></content>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Zabbix] zabbix监控cassandra数据库]]></title>
    <url>%2F2018%2F08%2F27%2Fzabbix%E7%9B%91%E6%8E%A7cassandra%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1、安装java_gate_way,如果当前监控架构为client-server将zabbix-java-gate-way安装在server端口，如果当前架构为client-proxy-server,将zabbix-java-gate-way安装在proxy端，安装步骤如下（下面以client-proxy-server架构为例）：123456789101112131415创建zabbix用户，安装依赖包：sudo bashmkdir /home/zabbixuseradd -r -s /sbin/nologin zabbixyum -y install gcc gcc-c++ pcre-devel openssl-develyum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel上传zabbix安装包，并安装java-gate-way： mv /data/download/zabbix-3.4.2.tar.gz /home/zabbix/ cd /home/zabbix/tar -zxvf zabbix-3.4.2.tar.gzcd zabbix-3.4.2./configure --prefix=/data/zabbix --enable-java(如果zabbix-porxy或zabbix-server已安装，可指定一个新路径，gate-way与proxy、server并没有直接关系，可不必一起安装；如果都没有安装，为了管理方便，亦可放到同一目录）make &amp;&amp; make install 安装完成之后，会在指定目录下存在以下文件：1234567ll /data/zabbix/sbin/zabbix_java/total 12drwxr-xr-x. 2 root root 65 Aug 23 02:41 bindrwxr-xr-x. 2 root root 177 Aug 23 02:36 lib-rw-r--r--. 1 root root 791 Aug 23 02:36 settings.sh-rwxr-xr-x. 1 root root 545 Aug 23 02:36 shutdown.sh-rwxr-xr-x. 1 root root 2025 Aug 23 02:36 startup.sh 包括启动脚本，关闭脚本，配置文件，依赖库，运行命令等文件。启动java_gate_way：1./startup.sh 2、安装完成之后，修改zabbix_proxy.conf配置文件，添加以下参数：123JavaGateway=127.0.0.1JavaGatewayPort=10052StartJavaPollers=5 默认端口为10052，因为zabbix-proxy与gate-way在同一台服务器上，因此使用127.0.0.1即可。必须指定java轮询器的预分叉实例数，默认情况下启动不会启动与jmx监控相关的进程。修改完成之后重新启动zabbix-proxy服务，是修改参数生效。 3、修改cassandra-java启动参数，使其jmx监控进程允许其他服务器访问cat /etc/cassandra/conf/cassandra-env.sh |grep -v ‘^#’ |grep -v ‘^$’123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172calculate_heap_sizes()&#123; case &quot;`uname`&quot; in Linux) system_memory_in_mb=`free -m | awk &apos;/:/ &#123;print $2;exit&#125;&apos;` system_cpu_cores=`egrep -c &apos;processor([[:space:]]+):.*&apos; /proc/cpuinfo` ;; FreeBSD) system_memory_in_bytes=`sysctl hw.physmem | awk &apos;&#123;print $2&#125;&apos;` system_memory_in_mb=`expr $system_memory_in_bytes / 1024 / 1024` system_cpu_cores=`sysctl hw.ncpu | awk &apos;&#123;print $2&#125;&apos;` ;; SunOS) system_memory_in_mb=`prtconf | awk &apos;/Memory size:/ &#123;print $3&#125;&apos;` system_cpu_cores=`psrinfo | wc -l` ;; Darwin) system_memory_in_bytes=`sysctl hw.memsize | awk &apos;&#123;print $2&#125;&apos;` system_memory_in_mb=`expr $system_memory_in_bytes / 1024 / 1024` system_cpu_cores=`sysctl hw.ncpu | awk &apos;&#123;print $2&#125;&apos;` ;; *) # assume reasonable defaults for e.g. a modern desktop or # cheap server system_memory_in_mb=&quot;2048&quot; system_cpu_cores=&quot;2&quot; ;; esac # some systems like the raspberry pi don&apos;t report cores, use at least 1 if [ &quot;$system_cpu_cores&quot; -lt &quot;1&quot; ] then system_cpu_cores=&quot;1&quot; fi # set max heap size based on the following # max(min(1/2 ram, 1024MB), min(1/4 ram, 8GB)) # calculate 1/2 ram and cap to 1024MB # calculate 1/4 ram and cap to 8192MB # pick the max half_system_memory_in_mb=`expr $system_memory_in_mb / 2` quarter_system_memory_in_mb=`expr $half_system_memory_in_mb / 2` if [ &quot;$half_system_memory_in_mb&quot; -gt &quot;1024&quot; ] then half_system_memory_in_mb=&quot;1024&quot; fi if [ &quot;$quarter_system_memory_in_mb&quot; -gt &quot;8192&quot; ] then quarter_system_memory_in_mb=&quot;8192&quot; fi if [ &quot;$half_system_memory_in_mb&quot; -gt &quot;$quarter_system_memory_in_mb&quot; ] then max_heap_size_in_mb=&quot;$half_system_memory_in_mb&quot; else max_heap_size_in_mb=&quot;$quarter_system_memory_in_mb&quot; fi MAX_HEAP_SIZE=&quot;$&#123;max_heap_size_in_mb&#125;M&quot; # Young gen: min(max_sensible_per_modern_cpu_core * num_cores, 1/4 * heap size) max_sensible_yg_per_core_in_mb=&quot;100&quot; max_sensible_yg_in_mb=`expr $max_sensible_yg_per_core_in_mb &quot;*&quot; $system_cpu_cores` desired_yg_in_mb=`expr $max_heap_size_in_mb / 4` if [ &quot;$desired_yg_in_mb&quot; -gt &quot;$max_sensible_yg_in_mb&quot; ] then HEAP_NEWSIZE=&quot;$&#123;max_sensible_yg_in_mb&#125;M&quot; else HEAP_NEWSIZE=&quot;$&#123;desired_yg_in_mb&#125;M&quot; fi&#125;java_ver_output=`&quot;$&#123;JAVA:-java&#125;&quot; -version 2&gt;&amp;1`jvmver=`echo &quot;$java_ver_output&quot; | grep &apos;[openjdk|java] version&apos; | awk -F&apos;&quot;&apos; &apos;NR==1 &#123;print $2&#125;&apos; | cut -d\- -f1`JVM_VERSION=$&#123;jvmver%_*&#125;JVM_PATCH_VERSION=$&#123;jvmver#*_&#125;if [ &quot;$JVM_VERSION&quot; \&lt; &quot;1.8&quot; ] ; then echo &quot;Cassandra 3.0 and later require Java 8u40 or later.&quot; exit 1;fiif [ &quot;$JVM_VERSION&quot; \&lt; &quot;1.8&quot; ] &amp;&amp; [ &quot;$JVM_PATCH_VERSION&quot; -lt 40 ] ; then echo &quot;Cassandra 3.0 and later require Java 8u40 or later.&quot; exit 1;fijvm=`echo &quot;$java_ver_output&quot; | grep -A 1 &apos;java version&apos; | awk &apos;NR==2 &#123;print $1&#125;&apos;`case &quot;$jvm&quot; in OpenJDK) JVM_VENDOR=OpenJDK # this will be &quot;64-Bit&quot; or &quot;32-Bit&quot; JVM_ARCH=`echo &quot;$java_ver_output&quot; | awk &apos;NR==3 &#123;print $2&#125;&apos;` ;; &quot;Java(TM)&quot;) JVM_VENDOR=Oracle # this will be &quot;64-Bit&quot; or &quot;32-Bit&quot; JVM_ARCH=`echo &quot;$java_ver_output&quot; | awk &apos;NR==3 &#123;print $3&#125;&apos;` ;; *) # Help fill in other JVM values JVM_VENDOR=other JVM_ARCH=unknown ;;esacJVM_OPTS=&quot;$JVM_OPTS -Xloggc:/var/log/cassandra/gc.log&quot;JVM_OPTS_FILE=$CASSANDRA_CONF/jvm.optionsfor opt in `grep &quot;^-&quot; $JVM_OPTS_FILE`do JVM_OPTS=&quot;$JVM_OPTS $opt&quot;doneecho $JVM_OPTS | grep -q XmnDEFINED_XMN=$?echo $JVM_OPTS | grep -q XmxDEFINED_XMX=$?echo $JVM_OPTS | grep -q XmsDEFINED_XMS=$?echo $JVM_OPTS | grep -q UseConcMarkSweepGCUSING_CMS=$?echo $JVM_OPTS | grep -q UseG1GCUSING_G1=$?if [ &quot;x$MAX_HEAP_SIZE&quot; = &quot;x&quot; ] &amp;&amp; [ &quot;x$HEAP_NEWSIZE&quot; = &quot;x&quot; -o $USING_G1 -eq 0 ]; then calculate_heap_sizeselif [ &quot;x$MAX_HEAP_SIZE&quot; = &quot;x&quot; ] || [ &quot;x$HEAP_NEWSIZE&quot; = &quot;x&quot; -a $USING_G1 -ne 0 ]; then echo &quot;please set or unset MAX_HEAP_SIZE and HEAP_NEWSIZE in pairs when using CMS GC (see cassandra-env.sh)&quot; exit 1fiif [ &quot;x$MALLOC_ARENA_MAX&quot; = &quot;x&quot; ] ; then export MALLOC_ARENA_MAX=4fiif [ $DEFINED_XMX -ne 0 ] &amp;&amp; [ $DEFINED_XMS -ne 0 ]; then JVM_OPTS=&quot;$JVM_OPTS -Xms$&#123;MAX_HEAP_SIZE&#125;&quot; JVM_OPTS=&quot;$JVM_OPTS -Xmx$&#123;MAX_HEAP_SIZE&#125;&quot;elif [ $DEFINED_XMX -ne 0 ] || [ $DEFINED_XMS -ne 0 ]; then echo &quot;Please set or unset -Xmx and -Xms flags in pairs on jvm.options file.&quot; exit 1fiif [ $DEFINED_XMN -eq 0 ] &amp;&amp; [ $DEFINED_XMX -ne 0 ]; then echo &quot;Please set or unset -Xmx and -Xmn flags in pairs on jvm.options file.&quot; exit 1elif [ $DEFINED_XMN -ne 0 ] &amp;&amp; [ $USING_CMS -eq 0 ]; then JVM_OPTS=&quot;$JVM_OPTS -Xmn$&#123;HEAP_NEWSIZE&#125;&quot;fiif [ &quot;$JVM_ARCH&quot; = &quot;64-Bit&quot; ] &amp;&amp; [ $USING_CMS -eq 0 ]; then JVM_OPTS=&quot;$JVM_OPTS -XX:+UseCondCardMark&quot;fiJVM_OPTS=&quot;$JVM_OPTS -XX:CompileCommandFile=$CASSANDRA_CONF/hotspot_compiler&quot;JVM_OPTS=&quot;$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/jamm-0.3.0.jar&quot;if [ &quot;x$CASSANDRA_HEAPDUMP_DIR&quot; != &quot;x&quot; ]; then JVM_OPTS=&quot;$JVM_OPTS -XX:HeapDumpPath=$CASSANDRA_HEAPDUMP_DIR/cassandra-`date +%s`-pid$$.hprof&quot;fiJVM_ON_OUT_OF_MEMORY_ERROR_OPT=&quot;-XX:OnOutOfMemoryError=kill -9 %p&quot;JVM_OPTS=&quot;$JVM_OPTS -Djava.rmi.server.hostname=10.0.7.50&quot;if [ &quot;x$LOCAL_JMX&quot; = &quot;x&quot; ]; then LOCAL_JMX=nofiJMX_PORT=&quot;7199&quot;if [ &quot;$LOCAL_JMX&quot; = &quot;yes&quot; ]; then JVM_OPTS=&quot;$JVM_OPTS -Dcassandra.jmx.local.port=$JMX_PORT&quot; JVM_OPTS=&quot;$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=false&quot;else JVM_OPTS=&quot;$JVM_OPTS -Dcassandra.jmx.remote.port=$JMX_PORT&quot; # if ssl is enabled the same port cannot be used for both jmx and rmi so either # pick another value for this property or comment out to use a random port (though see CASSANDRA-7087 for origins) JVM_OPTS=&quot;$JVM_OPTS -Dcom.sun.management.jmxremote.rmi.port=$JMX_PORT&quot; # turn on JMX authentication. See below for further options JVM_OPTS=&quot;$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=false&quot; # jmx ssl options JVM_OPTS=&quot;$JVM_OPTS -Dcom.sun.management.jmxremote.ssl=false&quot; #JVM_OPTS=&quot;$JVM_OPTS -Dcom.sun.management.jmxremote.ssl.need.client.auth=true&quot; #JVM_OPTS=&quot;$JVM_OPTS -Dcom.sun.management.jmxremote.ssl.enabled.protocols=&lt;enabled-protocols&gt;&quot; #JVM_OPTS=&quot;$JVM_OPTS -Dcom.sun.management.jmxremote.ssl.enabled.cipher.suites=&lt;enabled-cipher-suites&gt;&quot; #JVM_OPTS=&quot;$JVM_OPTS -Djavax.net.ssl.keyStore=/path/to/keystore&quot; #JVM_OPTS=&quot;$JVM_OPTS -Djavax.net.ssl.keyStorePassword=&lt;keystore-password&gt;&quot; #JVM_OPTS=&quot;$JVM_OPTS -Djavax.net.ssl.trustStore=/path/to/truststore&quot; #JVM_OPTS=&quot;$JVM_OPTS -Djavax.net.ssl.trustStorePassword=&lt;truststore-password&gt;&quot;fiJVM_OPTS=&quot;$JVM_OPTS -Djava.library.path=$CASSANDRA_HOME/lib/sigar-bin&quot;JVM_OPTS=&quot;$JVM_OPTS $MX4J_ADDRESS&quot;JVM_OPTS=&quot;$JVM_OPTS $MX4J_PORT&quot;JVM_OPTS=&quot;$JVM_OPTS $JVM_EXTRA_OPTS&quot; 主要修改一下内容，默认LOCAL_JMX=yes123if [ &quot;x$LOCAL_JMX&quot; = &quot;x&quot; ]; then LOCAL_JMX=nofi 将true改为false,修改远程访问认证无需密码：12345678910if [ &quot;$LOCAL_JMX&quot; = &quot;yes&quot; ]; then JVM_OPTS=&quot;$JVM_OPTS -Dcassandra.jmx.local.port=$JMX_PORT&quot; JVM_OPTS=&quot;$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=false&quot;else JVM_OPTS=&quot;$JVM_OPTS -Dcassandra.jmx.remote.port=$JMX_PORT&quot; # if ssl is enabled the same port cannot be used for both jmx and rmi so either # pick another value for this property or comment out to use a random port (though see CASSANDRA-7087 for origins) JVM_OPTS=&quot;$JVM_OPTS -Dcom.sun.management.jmxremote.rmi.port=$JMX_PORT&quot; # turn on JMX authentication. See below for further options JVM_OPTS=&quot;$JVM_OPTS -Dcom.sun.management.jmxremote.authenticate=false&quot; 4、在zabbix-proxy端测试，获取被监控端的数据是否正常，监控脚本如下：cat /data/zabbix/sbin/zabbix_java/bin/zabbix_get_jmx12345678910111213141516171819202122232425262728#!/usr/bin/env bashif [ $# != 5 ]then echo &quot;Usage: $0 &lt;JAVA_GATEWAY_HOST&gt; &lt;JAVA_GATEWAY_PORT&gt; &lt;JMX_SERVER&gt; &lt;JMX_PORT&gt; &lt;KEY&gt;&quot; exit;fi# create connectionexec 3&lt;&gt;/dev/tcp/$1/$2# compose messageMSG=&quot;&#123;\&quot;request\&quot;: \&quot;java gateway jmx\&quot;,\&quot;jmx_endpoint\&quot;:\&quot;service:jmx:rmi:///jndi/rmi://$3:$4/jmxrmi\&quot;,\&quot;keys\&quot;: [\&quot;$5\&quot;]&#125;&quot;# write message length as zero-padded 16-digit hexadecimal numberprintf -v LEN &apos;%016x&apos; &quot;$&#123;#MSG&#125;&quot;# prepare message length in little endian representationBYTES=&quot;&quot;for i in &#123;0..14..2&#125;do BYTES=&quot;\\x$&#123;LEN:$i:2&#125;$BYTES&quot;done# prepend protocol header and message lengthprintf &quot;ZBXD\\1$BYTES%s&quot; &quot;$MSG&quot; &gt;&amp;3# output the result skipping 5 bytes of &quot;ZBXD\\1&quot; header and 8 bytes of message lengthtail -c+13 &lt;&amp;3 该脚本可自行存放，没有规定，测试命令如下：1./zabbix_get_jmx 127.0.0.1 10052 10.0.7.50 7199 &apos;jmx[\&quot;org.apache.cassandra.metrics:type=ThreadPools,path=internal,scope=MemtablePostFlush,name=ActiveTasks\&quot;,\&quot;Value\&quot;]&apos; 127.0.0.1:为java-gate-way地址；10052：为java-gate-way端口；10.0.7.50：为被监控端的ip地址；7199：为被监控端jmx的端口；org.apache**:为要获取的参数，需要获取哪些参数可查看官方文档 5、zabbix官网提供了对cassandra的监控模板，其中有部分参数已失效，如需下载官方模板，点击此处，该模板为纯英文，且里面参数没有详细描述，下面为根据个人理解翻译后的模板，点击此处下载]]></content>
      <tags>
        <tag>zabbix</tag>
        <tag>cassandra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Linux] centos7.2 服务器 使用df -h卡死问题]]></title>
    <url>%2F2018%2F08%2F23%2Fcentos7.2%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8df%20-h%E5%8D%A1%E6%AD%BB%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1、服务器使用df -h命令卡死，crtl+c没有反应，ps -ef|grep df 命令杀掉df -h进程也无效。2、重新启动会话，使用strace跟踪df -h执行状态strace df -h12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182execve(&quot;/usr/bin/df&quot;, [&quot;df&quot;, &quot;-h&quot;], [/* 24 vars */]) = 0brk(NULL) = 0x1e9c000mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efe77097000access(&quot;/etc/ld.so.preload&quot;, R_OK) = -1 ENOENT (No such file or directory)open(&quot;/etc/ld.so.cache&quot;, O_RDONLY|O_CLOEXEC) = 3fstat(3, &#123;st_mode=S_IFREG|0644, st_size=19527, ...&#125;) = 0mmap(NULL, 19527, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7efe77092000close(3) = 0open(&quot;/lib64/libc.so.6&quot;, O_RDONLY|O_CLOEXEC) = 3read(3, &quot;\177ELF\2\1\1\3\0\0\0\0\0\0\0\0\3\0&gt;\0\1\0\0\0 \34\2\0\0\0\0\0&quot;..., 832) = 832fstat(3, &#123;st_mode=S_IFREG|0755, st_size=2107816, ...&#125;) = 0mmap(NULL, 3932736, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7efe76ab6000mprotect(0x7efe76c6c000, 2097152, PROT_NONE) = 0mmap(0x7efe76e6c000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1b6000) = 0x7efe76e6c000mmap(0x7efe76e72000, 16960, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7efe76e72000close(3) = 0mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efe77091000mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efe7708f000arch_prctl(ARCH_SET_FS, 0x7efe7708f740) = 0mprotect(0x7efe76e6c000, 16384, PROT_READ) = 0mprotect(0x616000, 4096, PROT_READ) = 0mprotect(0x7efe77098000, 4096, PROT_READ) = 0munmap(0x7efe77092000, 19527) = 0brk(NULL) = 0x1e9c000brk(0x1ebd000) = 0x1ebd000brk(NULL) = 0x1ebd000open(&quot;/usr/lib/locale/locale-archive&quot;, O_RDONLY|O_CLOEXEC) = 3fstat(3, &#123;st_mode=S_IFREG|0644, st_size=106065056, ...&#125;) = 0mmap(NULL, 106065056, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7efe7058f000close(3) = 0open(&quot;/usr/share/locale/locale.alias&quot;, O_RDONLY|O_CLOEXEC) = 3fstat(3, &#123;st_mode=S_IFREG|0644, st_size=2502, ...&#125;) = 0mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efe77096000read(3, &quot;# Locale name alias data base.\n#&quot;..., 4096) = 2502read(3, &quot;&quot;, 4096) = 0close(3) = 0munmap(0x7efe77096000, 4096) = 0open(&quot;/usr/share/locale/zh_CN.UTF-8/LC_MESSAGES/coreutils.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)open(&quot;/usr/share/locale/zh_CN.utf8/LC_MESSAGES/coreutils.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)open(&quot;/usr/share/locale/zh_CN/LC_MESSAGES/coreutils.mo&quot;, O_RDONLY) = 3fstat(3, &#123;st_mode=S_IFREG|0644, st_size=190751, ...&#125;) = 0mmap(NULL, 190751, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7efe77060000close(3) = 0open(&quot;/usr/share/locale/zh.UTF-8/LC_MESSAGES/coreutils.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)open(&quot;/usr/share/locale/zh.utf8/LC_MESSAGES/coreutils.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)open(&quot;/usr/share/locale/zh/LC_MESSAGES/coreutils.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)open(&quot;/etc/mtab&quot;, O_RDONLY|O_CLOEXEC) = 3fstat(3, &#123;st_mode=S_IFREG|0444, st_size=0, ...&#125;) = 0mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efe77096000read(3, &quot;rootfs / rootfs rw 0 0\nsysfs /sy&quot;..., 1024) = 1024read(3, &quot;ev,noexec,relatime,freezer 0 0\nc&quot;..., 1024) = 1024read(3, &quot;sd rw,relatime 0 0\n&quot;, 1024) = 19read(3, &quot;&quot;, 1024) = 0close(3) = 0munmap(0x7efe77096000, 4096) = 0open(&quot;/usr/lib64/gconv/gconv-modules.cache&quot;, O_RDONLY) = 3fstat(3, &#123;st_mode=S_IFREG|0644, st_size=26254, ...&#125;) = 0mmap(NULL, 26254, PROT_READ, MAP_SHARED, 3, 0) = 0x7efe77059000close(3) = 0stat(&quot;/&quot;, &#123;st_mode=S_IFDIR|0555, st_size=4096, ...&#125;) = 0stat(&quot;/sys&quot;, &#123;st_mode=S_IFDIR|0555, st_size=0, ...&#125;) = 0stat(&quot;/proc&quot;, &#123;st_mode=S_IFDIR|0555, st_size=0, ...&#125;) = 0stat(&quot;/dev&quot;, &#123;st_mode=S_IFDIR|0755, st_size=3140, ...&#125;) = 0stat(&quot;/sys/kernel/security&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/dev/shm&quot;, &#123;st_mode=S_IFDIR|S_ISVTX|0777, st_size=40, ...&#125;) = 0stat(&quot;/dev/pts&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/run&quot;, &#123;st_mode=S_IFDIR|0755, st_size=740, ...&#125;) = 0stat(&quot;/sys/fs/cgroup&quot;, &#123;st_mode=S_IFDIR|0755, st_size=280, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/systemd&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/pstore&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/memory&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/cpu,cpuacct&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/net_cls&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/freezer&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/devices&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/blkio&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/perf_event&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/cpuset&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/hugetlb&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/kernel/config&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/&quot;, &#123;st_mode=S_IFDIR|0555, st_size=4096, ...&#125;) = 0stat(&quot;/proc/sys/fs/binfmt_misc&quot; df -h进程在”/proc/sys/fs/binfmt_misc”这个位置卡主了 3、启动另一个会话，重新挂在这个目录1systemctl restart proc-sys-fs-binfmt_misc.automount 4、strace df -h 跟踪的进程内容有新的输出，完整信息输出如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157execve(&quot;/usr/bin/df&quot;, [&quot;df&quot;, &quot;-h&quot;], [/* 24 vars */]) = 0brk(NULL) = 0x1e9c000mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efe77097000access(&quot;/etc/ld.so.preload&quot;, R_OK) = -1 ENOENT (No such file or directory)open(&quot;/etc/ld.so.cache&quot;, O_RDONLY|O_CLOEXEC) = 3fstat(3, &#123;st_mode=S_IFREG|0644, st_size=19527, ...&#125;) = 0mmap(NULL, 19527, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7efe77092000close(3) = 0open(&quot;/lib64/libc.so.6&quot;, O_RDONLY|O_CLOEXEC) = 3read(3, &quot;\177ELF\2\1\1\3\0\0\0\0\0\0\0\0\3\0&gt;\0\1\0\0\0 \34\2\0\0\0\0\0&quot;..., 832) = 832fstat(3, &#123;st_mode=S_IFREG|0755, st_size=2107816, ...&#125;) = 0mmap(NULL, 3932736, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7efe76ab6000mprotect(0x7efe76c6c000, 2097152, PROT_NONE) = 0mmap(0x7efe76e6c000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1b6000) = 0x7efe76e6c000mmap(0x7efe76e72000, 16960, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7efe76e72000close(3) = 0mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efe77091000mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efe7708f000arch_prctl(ARCH_SET_FS, 0x7efe7708f740) = 0mprotect(0x7efe76e6c000, 16384, PROT_READ) = 0mprotect(0x616000, 4096, PROT_READ) = 0mprotect(0x7efe77098000, 4096, PROT_READ) = 0munmap(0x7efe77092000, 19527) = 0brk(NULL) = 0x1e9c000brk(0x1ebd000) = 0x1ebd000brk(NULL) = 0x1ebd000open(&quot;/usr/lib/locale/locale-archive&quot;, O_RDONLY|O_CLOEXEC) = 3fstat(3, &#123;st_mode=S_IFREG|0644, st_size=106065056, ...&#125;) = 0mmap(NULL, 106065056, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7efe7058f000close(3) = 0open(&quot;/usr/share/locale/locale.alias&quot;, O_RDONLY|O_CLOEXEC) = 3fstat(3, &#123;st_mode=S_IFREG|0644, st_size=2502, ...&#125;) = 0mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efe77096000read(3, &quot;# Locale name alias data base.\n#&quot;..., 4096) = 2502read(3, &quot;&quot;, 4096) = 0close(3) = 0munmap(0x7efe77096000, 4096) = 0open(&quot;/usr/share/locale/zh_CN.UTF-8/LC_MESSAGES/coreutils.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)open(&quot;/usr/share/locale/zh_CN.utf8/LC_MESSAGES/coreutils.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)open(&quot;/usr/share/locale/zh_CN/LC_MESSAGES/coreutils.mo&quot;, O_RDONLY) = 3fstat(3, &#123;st_mode=S_IFREG|0644, st_size=190751, ...&#125;) = 0mmap(NULL, 190751, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7efe77060000close(3) = 0open(&quot;/usr/share/locale/zh.UTF-8/LC_MESSAGES/coreutils.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)open(&quot;/usr/share/locale/zh.utf8/LC_MESSAGES/coreutils.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)open(&quot;/usr/share/locale/zh/LC_MESSAGES/coreutils.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)open(&quot;/etc/mtab&quot;, O_RDONLY|O_CLOEXEC) = 3fstat(3, &#123;st_mode=S_IFREG|0444, st_size=0, ...&#125;) = 0mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efe77096000read(3, &quot;rootfs / rootfs rw 0 0\nsysfs /sy&quot;..., 1024) = 1024read(3, &quot;ev,noexec,relatime,freezer 0 0\nc&quot;..., 1024) = 1024read(3, &quot;sd rw,relatime 0 0\n&quot;, 1024) = 19read(3, &quot;&quot;, 1024) = 0close(3) = 0munmap(0x7efe77096000, 4096) = 0open(&quot;/usr/lib64/gconv/gconv-modules.cache&quot;, O_RDONLY) = 3fstat(3, &#123;st_mode=S_IFREG|0644, st_size=26254, ...&#125;) = 0mmap(NULL, 26254, PROT_READ, MAP_SHARED, 3, 0) = 0x7efe77059000close(3) = 0stat(&quot;/&quot;, &#123;st_mode=S_IFDIR|0555, st_size=4096, ...&#125;) = 0stat(&quot;/sys&quot;, &#123;st_mode=S_IFDIR|0555, st_size=0, ...&#125;) = 0stat(&quot;/proc&quot;, &#123;st_mode=S_IFDIR|0555, st_size=0, ...&#125;) = 0stat(&quot;/dev&quot;, &#123;st_mode=S_IFDIR|0755, st_size=3140, ...&#125;) = 0stat(&quot;/sys/kernel/security&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/dev/shm&quot;, &#123;st_mode=S_IFDIR|S_ISVTX|0777, st_size=40, ...&#125;) = 0stat(&quot;/dev/pts&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/run&quot;, &#123;st_mode=S_IFDIR|0755, st_size=740, ...&#125;) = 0stat(&quot;/sys/fs/cgroup&quot;, &#123;st_mode=S_IFDIR|0755, st_size=280, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/systemd&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/pstore&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/memory&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/cpu,cpuacct&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/net_cls&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/freezer&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/devices&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/blkio&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/perf_event&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/cpuset&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/fs/cgroup/hugetlb&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/kernel/config&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/&quot;, &#123;st_mode=S_IFDIR|0555, st_size=4096, ...&#125;) = 0stat(&quot;/proc/sys/fs/binfmt_misc&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/dev/hugepages&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0stat(&quot;/sys/kernel/debug&quot;, &#123;st_mode=S_IFDIR|0700, st_size=0, ...&#125;) = 0stat(&quot;/dev/mqueue&quot;, &#123;st_mode=S_IFDIR|S_ISVTX|0777, st_size=40, ...&#125;) = 0stat(&quot;/boot&quot;, &#123;st_mode=S_IFDIR|0555, st_size=4096, ...&#125;) = 0stat(&quot;/run/user/0&quot;, &#123;st_mode=S_IFDIR|0700, st_size=40, ...&#125;) = 0stat(&quot;/var/lib/nfs/rpc_pipefs&quot;, &#123;st_mode=S_IFDIR|0555, st_size=0, ...&#125;) = 0stat(&quot;/proc/fs/nfsd&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0uname(&#123;sysname=&quot;Linux&quot;, nodename=&quot;xkey-boss01-8035-184&quot;, ...&#125;) = 0statfs(&quot;/&quot;, &#123;f_type=0x58465342, f_bsize=4096, f_blocks=9293380, f_bfree=5378782, f_bavail=5378782, f_files=37191680, f_ffree=37150656, f_fsid=&#123;64768, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_RELATIME&#125;) = 0stat(&quot;/&quot;, &#123;st_mode=S_IFDIR|0555, st_size=4096, ...&#125;) = 0statfs(&quot;/dev&quot;, &#123;f_type=TMPFS_MAGIC, f_bsize=4096, f_blocks=494791, f_bfree=494791, f_bavail=494791, f_files=494791, f_ffree=494430, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID&#125;) = 0stat(&quot;/dev&quot;, &#123;st_mode=S_IFDIR|0755, st_size=3140, ...&#125;) = 0statfs(&quot;/sys/kernel/security&quot;, &#123;f_type=SECURITYFS_MAGIC, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV|ST_NOEXEC|ST_RELATIME&#125;) = 0stat(&quot;/sys/kernel/security&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/dev/shm&quot;, &#123;f_type=TMPFS_MAGIC, f_bsize=4096, f_blocks=497337, f_bfree=497337, f_bavail=497337, f_files=497337, f_ffree=497336, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV&#125;) = 0stat(&quot;/dev/shm&quot;, &#123;st_mode=S_IFDIR|S_ISVTX|0777, st_size=40, ...&#125;) = 0statfs(&quot;/run&quot;, &#123;f_type=TMPFS_MAGIC, f_bsize=4096, f_blocks=497337, f_bfree=458142, f_bavail=458142, f_files=497337, f_ffree=496689, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV&#125;) = 0stat(&quot;/run&quot;, &#123;st_mode=S_IFDIR|0755, st_size=740, ...&#125;) = 0statfs(&quot;/sys/fs/cgroup&quot;, &#123;f_type=TMPFS_MAGIC, f_bsize=4096, f_blocks=497337, f_bfree=497337, f_bavail=497337, f_files=497337, f_ffree=497324, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_RDONLY|ST_NOSUID|ST_NODEV|ST_NOEXEC&#125;) = 0stat(&quot;/sys/fs/cgroup&quot;, &#123;st_mode=S_IFDIR|0755, st_size=280, ...&#125;) = 0statfs(&quot;/sys/fs/cgroup/systemd&quot;, &#123;f_type=CGROUP_SUPER_MAGIC, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV|ST_NOEXEC|ST_RELATIME&#125;) = 0stat(&quot;/sys/fs/cgroup/systemd&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/sys/fs/pstore&quot;, &#123;f_type=PSTOREFS_MAGIC, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV|ST_NOEXEC|ST_RELATIME&#125;) = 0stat(&quot;/sys/fs/pstore&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/sys/fs/cgroup/memory&quot;, &#123;f_type=CGROUP_SUPER_MAGIC, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV|ST_NOEXEC|ST_RELATIME&#125;) = 0stat(&quot;/sys/fs/cgroup/memory&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/sys/fs/cgroup/cpu,cpuacct&quot;, &#123;f_type=CGROUP_SUPER_MAGIC, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV|ST_NOEXEC|ST_RELATIME&#125;) = 0stat(&quot;/sys/fs/cgroup/cpu,cpuacct&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/sys/fs/cgroup/net_cls&quot;, &#123;f_type=CGROUP_SUPER_MAGIC, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV|ST_NOEXEC|ST_RELATIME&#125;) = 0stat(&quot;/sys/fs/cgroup/net_cls&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/sys/fs/cgroup/freezer&quot;, &#123;f_type=CGROUP_SUPER_MAGIC, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV|ST_NOEXEC|ST_RELATIME&#125;) = 0stat(&quot;/sys/fs/cgroup/freezer&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/sys/fs/cgroup/devices&quot;, &#123;f_type=CGROUP_SUPER_MAGIC, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV|ST_NOEXEC|ST_RELATIME&#125;) = 0stat(&quot;/sys/fs/cgroup/devices&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/sys/fs/cgroup/blkio&quot;, &#123;f_type=CGROUP_SUPER_MAGIC, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV|ST_NOEXEC|ST_RELATIME&#125;) = 0stat(&quot;/sys/fs/cgroup/blkio&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/sys/fs/cgroup/perf_event&quot;, &#123;f_type=CGROUP_SUPER_MAGIC, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV|ST_NOEXEC|ST_RELATIME&#125;) = 0stat(&quot;/sys/fs/cgroup/perf_event&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/sys/fs/cgroup/cpuset&quot;, &#123;f_type=CGROUP_SUPER_MAGIC, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV|ST_NOEXEC|ST_RELATIME&#125;) = 0stat(&quot;/sys/fs/cgroup/cpuset&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/sys/fs/cgroup/hugetlb&quot;, &#123;f_type=CGROUP_SUPER_MAGIC, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV|ST_NOEXEC|ST_RELATIME&#125;) = 0stat(&quot;/sys/fs/cgroup/hugetlb&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/sys/kernel/config&quot;, &#123;f_type=0x62656570, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_RELATIME&#125;) = 0stat(&quot;/sys/kernel/config&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/dev/hugepages&quot;, &#123;f_type=HUGETLBFS_MAGIC, f_bsize=2097152, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=2097152, f_flags=ST_VALID|ST_RELATIME&#125;) = 0stat(&quot;/dev/hugepages&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0statfs(&quot;/boot&quot;, &#123;f_type=0x58465342, f_bsize=4096, f_blocks=127147, f_bfree=95769, f_bavail=95769, f_files=512000, f_ffree=511670, f_fsid=&#123;64513, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_RELATIME&#125;) = 0stat(&quot;/boot&quot;, &#123;st_mode=S_IFDIR|0555, st_size=4096, ...&#125;) = 0statfs(&quot;/run/user/0&quot;, &#123;f_type=TMPFS_MAGIC, f_bsize=4096, f_blocks=99468, f_bfree=99468, f_bavail=99468, f_files=497337, f_ffree=497336, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_NOSUID|ST_NODEV|ST_RELATIME&#125;) = 0stat(&quot;/run/user/0&quot;, &#123;st_mode=S_IFDIR|0700, st_size=40, ...&#125;) = 0statfs(&quot;/proc/fs/nfsd&quot;, &#123;f_type=0x6e667364, f_bsize=4096, f_blocks=0, f_bfree=0, f_bavail=0, f_files=0, f_ffree=0, f_fsid=&#123;0, 0&#125;, f_namelen=255, f_frsize=4096, f_flags=ST_VALID|ST_RELATIME&#125;) = 0stat(&quot;/proc/fs/nfsd&quot;, &#123;st_mode=S_IFDIR|0755, st_size=0, ...&#125;) = 0fstat(1, &#123;st_mode=S_IFCHR|0620, st_rdev=makedev(136, 7), ...&#125;) = 0mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efe77096000write(1, &quot;\346\226\207\344\273\266\347\263\273\347\273\237 \345\256\271&quot;..., 70文件系统 容量 已用 可用 已用% 挂载点) = 70write(1, &quot;/dev/mapper/centos-root 36G &quot;..., 50/dev/mapper/centos-root 36G 15G 21G 43% /) = 50write(1, &quot;devtmpfs 1.9G &quot;..., 53devtmpfs 1.9G 0 1.9G 0% /dev) = 53write(1, &quot;tmpfs 1.9G &quot;..., 57tmpfs 1.9G 0 1.9G 0% /dev/shm) = 57write(1, &quot;tmpfs 1.9G 1&quot;..., 53tmpfs 1.9G 154M 1.8G 8% /run) = 53write(1, &quot;tmpfs 1.9G &quot;..., 63tmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup) = 63write(1, &quot;/dev/vda1 497M 1&quot;..., 54/dev/vda1 497M 123M 375M 25% /boot) = 54write(1, &quot;tmpfs 389M &quot;..., 60tmpfs 389M 0 389M 0% /run/user/0) = 60close(1) = 0munmap(0x7efe77096000, 4096) = 0close(2) = 0exit_group(0) = ?+++ exited with 0 +++ 5、重新使用df -h命令，恢复正常。]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] InnoDB：page_cleaner：1000ms intended loop took xxxms]]></title>
    <url>%2F2018%2F08%2F21%2FInnoDB-page_cleaner-1000ms%20intended%20loop%20took%20xxxms%2F</url>
    <content type="text"><![CDATA[1、在查看mysqllog日志的时候不经意间发现一条这个提示：1[Note] InnoDB: page_cleaner: 1000ms intended loop took 16111ms. The settings might not be optimal. (flushed=7 and evicted=0, during the time.) 造成该问题的主要原因：1234567page_cleaner_thread:脏页清理线程负责将脏页从内存写到磁盘。出现该问题的原因：上面提示的信息的含义是,有大量脏页需要刷新，理论上应该在1s内完成，但实际却用了16s的时间将脏页刷新到磁盘，它接受脏页的数量远远大于它每秒能够处理脏页的能力，因此为了避免该问题，可降低每秒循环期间搜索脏页的深度（innodb_lru_scan_depth）。如果数据库存在很多的buffer pool instance 将会引起更多的刷新工作，因此如果只是增大 buffer pool instances 而没有降低lru_scan_depth,很可能会引起性能瓶颈。如果只是临时对数据库进行大量导入操作造成的这类问题,可以忽略这个问题不需关注。如果数据库一直存在大量更新操作，快速创建大量的脏页，该问题一直存在需要关注。 下面是官方文档对lru_scan_depth参数的解释，中文为自己对官方文档的理解翻译，错误之处望大家指正。12345A setting smaller than the default is generally suitable for most workloads. A value that is much higher than necessary may impact performance. Only consider increasing the value if you have spare I/O capacity under a typical workload. Conversely, if a write-intensive workload saturates your I/O capacity, decrease the value, especially in the case of a large buffer pool.When tuning innodb_lru_scan_depth, start with a low value and configure the setting upward with the goal of rarely seeing zero free pages. Also, consider adjusting innodb_lru_scan_depth when changing the number of buffer pool instances, since innodb_lru_scan_depth * innodb_buffer_pool_instances defines the amount of work performed by the page cleaner thread each second.比默认值小适合于大多数工作负载，如果设置比默认值大可能会影响性能。只有当有额外的磁盘io容量的时候才考虑需不需要增加该值。相反，如果在写密集度负载已经使io容量饱和，需要降低该值，尤其是buffer pool的值设置特别大的时候。如果lru_scan_depth值特别低，而且存在0空闲页，可以考虑调高该值。如果调整buffer pool instances时，需要考虑是否调整innodb_lru_scan_depth大小，因为innodb_lru_scan_depth * innodb_buffer_pool_instances决定了每秒page cleaner thread处理的工作量。 2、解决方法1SET GLOBAL innodb_lru_scan_depth=256; 该参数默认只为1024。这只是数据库级别调整，出现该问题还需要查看服务器硬件问题，磁盘io是否达到瓶颈扥问题。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql-mgr集群节点主节点与两个从节点网络异常，造成的集群异常问题]]></title>
    <url>%2F2018%2F08%2F20%2Fmysql-mgr%E9%9B%86%E7%BE%A4%E8%8A%82%E7%82%B9%E4%B8%BB%E8%8A%82%E7%82%B9%E4%B8%8E%E4%B8%A4%E4%B8%AA%E4%BB%8E%E8%8A%82%E7%82%B9%E7%BD%91%E7%BB%9C%E5%BC%82%E5%B8%B8%EF%BC%8C%E9%80%A0%E6%88%90%E7%9A%84%E9%9B%86%E7%BE%A4%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1、mysql-mgr集群节点主节点（A）与两个从节点网络异常，主节点（A）发生切换，从节点（B)切换为主，报错如下：1232018-08-20T05:38:16.071653Z 0 [Warning] Plugin group_replication reported: &apos;Member with address wallet-mysql-2:3306 has become unreachable.&apos;2018-08-20T05:38:16.071730Z 0 [Warning] Plugin group_replication reported: &apos;Member with address wallet-mysql-3:3306 has become unreachable.&apos;2018-08-20T05:38:16.071744Z 0 [ERROR] Plugin group_replication reported: &apos;This server is not able to reach a majority of members in the group. This server will now block all updates. The server will remain blocked until contact with the majority is restored. It is possible to use group_replication_force_members to force a new group membership.&apos; 2、对故障节点A操作，使其重新加入集群A节点：12mysql -u root -pstart group_replication; 查看报错日志：1234567891011121314152018-08-20T07:29:40.796952Z 12 [Note] Plugin group_replication reported: &apos;Terminating existing group replication donor connection and purging the corresponding logs.&apos;2018-08-20T07:29:40.803975Z 26 [Note] Error reading relay log event for channel &apos;group_replication_recovery&apos;: slave SQL thread was killed2018-08-20T07:29:40.855935Z 12 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;wallet-mysql-2&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-08-20T07:29:40.906041Z 12 [Note] Plugin group_replication reported: &apos;Retrying group recovery connection with another donor. Attempt 4/10&apos;2018-08-20T07:29:40.949597Z 12 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;wallet-mysql-3&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;.2018-08-20T07:29:41.001446Z 12 [Note] Plugin group_replication reported: &apos;Establishing connection to a group replication recovery donor 7dd813bf-8e3d-11e8-8d92-000d3aa1c31e at wallet-mysql-3 port: 3306.&apos;2018-08-20T07:29:41.001738Z 29 [Warning] Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the &apos;START SLAVE Syntax&apos; in the MySQL Manual for more information.2018-08-20T07:29:41.005538Z 29 [Note] Slave I/O thread for channel &apos;group_replication_recovery&apos;: connected to master &apos;repl@wallet-mysql-3:3306&apos;,replication started in log &apos;FIRST&apos; at position 42018-08-20T07:29:41.016839Z 30 [Note] Slave SQL thread for channel &apos;group_replication_recovery&apos; initialized, starting replication in log &apos;FIRST&apos; at position 0, relay log &apos;./relay-log-group_replication_recovery.000001&apos; position: 42018-08-20T07:29:41.028408Z 29 [ERROR] Error reading packet from server for channel &apos;group_replication_recovery&apos;: The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires. (server_errno=1236)2018-08-20T07:29:41.028442Z 29 [ERROR] Slave I/O for channel &apos;group_replication_recovery&apos;: Got fatal error 1236 from master when reading data from binary log: &apos;The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires.&apos;, Error_code: 12362018-08-20T07:29:41.028449Z 29 [Note] Slave I/O thread exiting for channel &apos;group_replication_recovery&apos;, read up to log &apos;FIRST&apos;, position 42018-08-20T07:29:41.028493Z 12 [Note] Plugin group_replication reported: &apos;Terminating existing group replication donor connection and purging the corresponding logs.&apos;2018-08-20T07:29:41.028790Z 30 [Note] Error reading relay log event for channel &apos;group_replication_recovery&apos;: slave SQL thread was killed2018-08-20T07:29:41.085012Z 12 [Note] &apos;CHANGE MASTER TO FOR CHANNEL &apos;group_replication_recovery&apos; executed&apos;. Previous state master_host=&apos;wallet-mysql-3&apos;, master_port= 3306, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. New state master_host=&apos;&lt;NULL&gt;&apos;, master_port= 0, master_log_file=&apos;&apos;, master_log_pos= 4, master_bind=&apos;&apos;. 出现该问题的原因是现在的新的主节点B，已经清除了binglog,A节点需要恢复的日志已经找不到了，因此需要对A进行数据恢复。 3、在B节点mysqldump一份最新数据，命令如下：1/data/mysql/bin/mysqldump --all-databases --set-gtid-purged=ON --single-transaction -uroot -p***** &gt; /data/backup/all.sql 4、将dump的数据库拷贝到A节点，对节点A进行恢复：1mysql -u root -p******* &lt; /data/backup/all.sql 执行提示报错12mysql: [Warning] Using a password on the command line interface can be insecure.ERROR 1840 (HY000) at line 24: @@GLOBAL.GTID_PURGED can only be set when @@GLOBAL.GTID_EXECUTED is empty. 解决方法：12345678910提示只有@@GLOBAL.GTID_EXECUTED为空是，才能设置@@GLOBAL.GTID_PURGED的值，清空GLOBAL.GTID_EXECUTED的值root@db 07:53: [(none)]&gt; reset master;ERROR 3190 (HY000): RESET MASTER is not allowed because Group Replication is running.root@db 07:53: [(none)]&gt; stop group_replication;Query OK, 0 rows affected (1.01 sec)root@db 07:53: [(none)]&gt; reset master;Query OK, 0 rows affected (0.27 sec)如果数据库为只读的状态还需要将数据库改为读写模式：root@db 07:54: [performance_schema]&gt; set global read_only=0;Query OK, 0 rows affected (0.00 sec) 5、重新恢复数据12mysql -u root -p******* &lt; /data/backup/all.sqlmysql: [Warning] Using a password on the command line interface can be insecure. 恢复完成 6、重新将该节点加入集群1234567891011121314151617181920mysql -u root -proot@db 07:56: [performance_schema]&gt; select * from replication_group_members;+---------------------------+-----------+-------------+-------------+--------------+| CHANNEL_NAME | MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE |+---------------------------+-----------+-------------+-------------+--------------+| group_replication_applier | | | NULL | OFFLINE |+---------------------------+-----------+-------------+-------------+--------------+1 row in set (0.00 sec)root@db 07:56: [performance_schema]&gt; start group_replication;Query OK, 0 rows affected, 1 warning (3.03 sec)root@db 07:57: [performance_schema]&gt; select * from replication_group_members;+---------------------------+--------------------------------------+----------------+-------------+--------------+| CHANNEL_NAME | MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE |+---------------------------+--------------------------------------+----------------+-------------+--------------+| group_replication_applier | *** | wallet-mysql-2 | 3306 | ONLINE || group_replication_applier | *** | wallet-mysql-1 | 3306 | RECOVERING || group_replication_applier | *** | wallet-mysql-3 | 3306 | ONLINE |+---------------------------+--------------------------------------+----------------+-------------+--------------+3 rows in set (0.00 sec) 此时节点A正在与主节点同步数据，待同步完成之后再查看及节点之间的信息：123456789root@db 07:57: [performance_schema]&gt; select * from replication_group_members;+---------------------------+--------------------------------------+----------------+-------------+--------------+| CHANNEL_NAME | MEMBER_ID | MEMBER_HOST | MEMBER_PORT | MEMBER_STATE |+---------------------------+--------------------------------------+----------------+-------------+--------------+| group_replication_applier | *** | wallet-mysql-2 | 3306 | ONLINE || group_replication_applier | *** | wallet-mysql-1 | 3306 | ONLINE || group_replication_applier | *** | wallet-mysql-3 | 3306 | ONLINE |+---------------------------+--------------------------------------+----------------+-------------+--------------+3 rows in set (0.00 sec) 此时集群恢复完成。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Zabbix] zabbix监控redis数据库]]></title>
    <url>%2F2018%2F08%2F17%2Fzabbix%E7%9B%91%E6%8E%A7redis%2F</url>
    <content type="text"><![CDATA[1、编写定期收集redis信息脚本，并指定到对应目录cat /usr/local/zabbix/script/redis_get_data.sh1234567891011121314151617#!/bin/bash#配置文件路径dbconfigfile=/data/redis/etc/redis.conf#主机ip地址dbhost=`cat $&#123;dbconfigfile&#125; |grep bind|awk '&#123;print $2&#125;'`#redis启动端口dbport=`cat $&#123;dbconfigfile&#125; |grep port|awk '&#123;print $2&#125;'`#数据库命令路径dbcmdpath=/data/redis/src#数据库密码dbpass=`cat $&#123;dbconfigfile&#125; |grep requirepass|awk '&#123;print $2&#125;'`#指定生成的log路径dblogpath=/tmp/redis.log#如果有没密码，使用以下命令#$&#123;dbcmdpath&#125;/redis-cli -h $&#123;dbhost&#125; -a $&#123;dbpass&#125; -p $&#123;dbport&#125; 'info' &gt; $&#123;dblogpath&#125;#如果没有密码，使用以下命令$&#123;dbcmdpath&#125;/redis-cli -h $&#123;dbhost&#125; -p $&#123;dbport&#125; 'info' &gt; $&#123;dblogpath&#125; 修改redis配置文件目录，生成的log目录，修改完成之后授权脚本执行权限：1chmod +x /usr/local/zabbix/script/redis_get_data.sh 2、将脚本添加到定时任务，每分钟执行一次crontab -l1* * * * * sh /usr/local/zabbix/script/redis_get_data.sh 3、编写zabbix监控项文件,通过/usr/local/zabbix/script/redis_get_data.sh脚本生成的log文件，根据关键字过滤，获取对应监控的数值vim /usr/local/zabbix/etc/zabbix_agentd.conf.d/redis_monitor_parameter.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#数据库进程是否存在UserParameter=redis_exist[*],ps -ef|grep redis-server|grep -v grep|wc -l#客户端连接数，包括从库的连接UserParameter=connected_clients[*],cat /tmp/redis.log |grep &apos;connected_clients&apos; |cut -d&apos;:&apos; -f2#当前客户端最长输出列表UserParameter=client_longest_output_list[*],cat /tmp/redis.log |grep client_longest_output_list |cut -d&apos;:&apos; -f2#当前客户端最大输入缓冲区UserParameter=client_biggest_input_buf[*],cat /tmp/redis.log |grep client_biggest_input_buf |cut -d&apos;:&apos; -f2#阻塞会话数UserParameter=blocked_clients[*],cat /tmp/redis.log |grep blocked_clients |cut -d&apos;:&apos; -f2#redis通过分配器分配的总内存UserParameter=used_memory[*],cat /tmp/redis.log |grep used_memory |cut -d&apos;:&apos; -f2|head -1#操作系统通过top和ps命令查看到redis分配的内存UserParameter=used_memory_rss[*],cat /tmp/redis.log |grep used_memory_rss |cut -d&apos;:&apos; -f2|head -1#redis消耗内存的峰值UserParameter=used_memory_peak[*],cat /tmp/redis.log |grep used_memory_peak |cut -d&apos;:&apos; -f2|head -1#系统为管理内部数据架构造成的所有内存开销UserParameter=used_memory_overhead[*],cat /tmp/redis.log |grep used_memory_overhead |cut -d&apos;:&apos; -f2|head -1#redis启动初始化消耗的内存UserParameter=used_memory_startup[*],cat /tmp/redis.log |grep used_memory_startup |cut -d&apos;:&apos; -f2|head -1#数据集的大小（used_memory减去used_memory_overhead）UserParameter=used_memory_dataset[*],cat /tmp/redis.log |grep used_memory_dataset |cut -d&apos;:&apos; -f2|head -1#数据集内存占净内存的百分比（used_memory_dataset/(used_memory-used_memory_startup))UserParameter=used_memory_dataset_perc[*],cat /tmp/redis.log |grep used_memory_dataset_perc |cut -d&apos;:&apos; -f2|head -1|cut -d&apos;%&apos; -f1#lua引擎使用的内存UserParameter=used_memory_lua[*],cat /tmp/redis.log |grep used_memory_lua |cut -d&apos;:&apos; -f2|head -1#系统配置文件配置的内存UserParameter=maxmemory[*],cat /tmp/redis.log |grep maxmemory |cut -d&apos;:&apos; -f2|head -1#内存碎片率（需要重点关注）远大于1过高说明内存碎片化严重，原小于1过低说明有大量内存交换，建议增加内存。UserParameter=mem_fragmentation_ratio[*],cat /tmp/redis.log |grep mem_fragmentation_ratio |cut -d&apos;:&apos; -f2|head -1#自上次转存以来redis发生的更改数UserParameter=rdb_changes_since_last_save[*],cat /tmp/redis.log |grep rdb_changes_since_last_save |cut -d&apos;:&apos; -f2|head -1#服务端接受的总连接数UserParameter=total_connections_received[*],cat /tmp/redis.log |grep total_connections_received |cut -d&apos;:&apos; -f2|head -1#服务端处理命令总数UserParameter=total_commands_processed[*],cat /tmp/redis.log |grep total_commands_processed |cut -d&apos;:&apos; -f2|head -1#服务端每秒处理命令数UserParameter=instantaneous_ops_per_sec[*],cat /tmp/redis.log |grep instantaneous_ops_per_sec |cut -d&apos;:&apos; -f2|head -1#网络读取的总流量UserParameter=total_net_input_bytes[*],cat /tmp/redis.log |grep total_net_input_bytes |cut -d&apos;:&apos; -f2|head -1#网络写入的总流量UserParameter=total_net_output_bytes[*],cat /tmp/redis.log |grep total_net_output_bytes |cut -d&apos;:&apos; -f2|head -1#每秒网络读的流量UserParameter=instantaneous_input_kbps[*],cat /tmp/redis.log |grep instantaneous_input_kbps |cut -d&apos;:&apos; -f2|head -1#每秒网络写的流量UserParameter=instantaneous_output_kbps[*],cat /tmp/redis.log |grep instantaneous_output_kbps |cut -d&apos;:&apos; -f2|head -1#因最大连接数达到上限，而被拒绝的连接数，maxclient默认值为10000UserParameter=rejected_connections[*],cat /tmp/redis.log |grep rejected_connections |cut -d&apos;:&apos; -f2|head -1#键值到期总数UserParameter=expired_keys[*],cat /tmp/redis.log |grep expired_keys |cut -d&apos;:&apos; -f2|head -1#因maxmemory限制，被驱逐的键值UserParameter=evicted_keys[*],cat /tmp/redis.log |grep evicted_keys |cut -d&apos;:&apos; -f2|head -1#在主字典里面键值匹配成功的次数UserParameter=keyspace_hits[*],cat /tmp/redis.log |grep keyspace_hits |cut -d&apos;:&apos; -f2|head -1#在主字典里面键值匹配失败的次数UserParameter=keyspace_misses[*],cat /tmp/redis.log |grep keyspace_misses |cut -d&apos;:&apos; -f2|head -1#db0数据库键值的数量UserParameter=db0_keys[*], cat /tmp/redis.log |grep db0 |cat /tmp/redis.log |grep db0|cut -d &apos;,&apos; -f1|cut -d &apos;=&apos; -f2#db0数据库键值过期的数量UserParameter=db0_keys_expired[*],cat /tmp/redis.log |cat /tmp/redis.log |grep db0|cut -d &apos;,&apos; -f2|cut -d &apos;=&apos; -f2 4、修改/usr/local/zabbix/etc/zabbix_agentd.conf配置文件,添加该行参数：1Include=/usr/local/zabbix/etc/zabbix_agentd.conf.d/ 添加完成之后，需要重启zabbix_agent客户端1/etc/init.d/zabbix_agentd restart 5、通过zabbix-web控制台，自定义一个模板，新建监控项、触发器、图形等内容，下面链接是我创建的比较简单的redis模板，上传到百度云，仅供参考，点击下载]]></content>
      <tags>
        <tag>zabbix</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[linux] suse_11_sp4安装samba-server]]></title>
    <url>%2F2018%2F08%2F17%2Fsamba%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[0、当前系统版本12cat /etc/issueWelcome to SUSE Linux Enterprise Server 11 SP4 (x86_64) - Kernel \r (\l). 1、查看samba是否安装12localhost:~ # rpm -q sambasamba-3.6.3-0.58.1 如果没有安装，使用以下命令安装：1localhost:~ # zypper install samba 2、新建共享目录1localhost:~ # mkdir -p /data/share 3、开始配置共享目录yast进入控制台,network services-samba server第一次启用samba server会做一些初始化，操作如下（也可根据自己需求自定义）：初始化完成之后，再一次进入samba server,会显示如下：点击add,开始新增共享目录修改完成之后保存，会显示新的share目录 4、添加smbuser系统用户1234567localhost:~ # useradd smbuserlocalhost:~ # passwd smbuserChanging password for smbuser.New Password: Bad password: too simpleReenter New Password: Password changed. 5、为samba服务添加访问用户，设置访问用的密码：1234localhost:~ # smbpasswd -a smbuserNew SMB password:Retype new SMB password:Added user smbuser. 6、修改共享目录权限12chown smbuser /data/sharechmod 777 /data/share **如果不修改目录权限，访问预览都是正常的，但是没有写权限，创建目录会报错 7、修改配置配置文件,添加读写权限vim /etc/samba/smb.conf12345678[share] comment = share inherit acls = Yes path = /data/share read only = No Valid users =smbuser Writable = Yes Browsable = Yes 8、重启samba服务器/etc/rc.d/smb restart 9、suse默认防火墙是开启的，客户端要访问samba,需要开启139和445端口vim /etc/sysconfig/SuSEfirewall21FW_SERVICES_EXT_TCP=&quot;22 1521 139 445&quot; 重新启动防火墙1rcSuSEfirewall2 restart 10、windows10之后微软默认已放弃安装smbv1客户端，使用windows访问会报如下错误：因此修改sambaserver服务器端的协议，启用smbv2协议vim /etc/samba/smb.conf12345[global]min protocol = SMB2max protocol = SMB2client min protocol = SMB2client max protocol = SMB2 重启samba服务器1/etc/rc.d/smb restart 11、在sambaserver本地服务器测试的时候，哪怕指定了smb2协议，也会报如下错误1234567smbclient -U smbuser //10.0.30.180/share -m smb2Unrecognised protocol level smb2WARNING: Ignoring invalid value &apos;SMB3&apos; for parameter &apos;max protocol&apos;Unknown parameter encountered: &quot;client min protocol&quot;Ignoring unknown parameter &quot;client min protocol&quot;Unknown parameter encountered: &quot;client max protocol&quot;Ignoring unknown parameter &quot;client max protocol&quot; 该问题为suse 11版本的问题，默认只支持smbv1协议，suse12版本之后才开始支持smbv2.]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql lock table && unlock tables实验]]></title>
    <url>%2F2018%2F08%2F17%2Fmysql%20lock%20table%20%26%26%20unlock%20tables%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[0、mysql版本1234567root@db 04:12: [aaaa]&gt; select @@version;+------------+| @@version |+------------+| 5.7.22-log |+------------+1 row in set (0.00 sec) 1、创建实验表，内容如下：123456789101112131415161718192021222324252627root@db 15:11: [aaaa]&gt; show tables;+----------------+| Tables_in_aaaa |+----------------+| aaa || bbb |+----------------+2 rows in set (0.00 sec)root@db 15:15: [aaaa]&gt; select * from aaa;+----+------+-------------+| id | name | telephone |+----+------+-------------+| 1 | a | 11111111111 || 2 | b | 22222222222 || 3 | c | 33333333333 || 4 | d | 44444444 |+----+------+-------------+4 rows in set (0.00 sec)root@db 15:15: [aaaa]&gt; select * from bbb;+----+------+-------------+| id | name | telephone |+----+------+-------------+| 1 | a | 11111111111 || 2 | b | 22222222222 || 3 | c | 33333333333 |+----+------+-------------+3 rows in set (0.00 sec) 2、开启两个会话，session1、session2,对表aaa进行read表锁session11234567891011121314151617root@db 15:16: [aaaa]&gt; lock table aaa read;Query OK, 0 rows affected (0.00 sec)root@db 15:17: [aaaa]&gt; select * from aaa;+----+------+-------------+| id | name | telephone |+----+------+-------------+| 1 | a | 11111111111 || 2 | b | 22222222222 || 3 | c | 33333333333 || 4 | d | 44444444 |+----+------+-------------+4 rows in set (0.00 sec)root@db 15:17: [aaaa]&gt; select * from bbb;ERROR 1100 (HY000): Table &apos;bbb&apos; was not locked with LOCK TABLESroot@db 15:17: [aaaa]&gt; root@db 15:18: [aaaa]&gt; update aaa set name=&apos;e&apos; where id=1;ERROR 1099 (HY000): Table &apos;aaa&apos; was locked with a READ lock and can&apos;t be updated session 212345678910111213141516171819root@db 15:18: [aaaa]&gt; select * from aaa;+----+------+-------------+| id | name | telephone |+----+------+-------------+| 1 | a | 11111111111 || 2 | b | 22222222222 || 3 | c | 33333333333 || 4 | d | 44444444 |+----+------+-------------+4 rows in set (0.00 sec)root@db 15:18: [aaaa]&gt; update aaa set name=&apos;e&apos; where id=1;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionroot@db 15:20: [aaaa]&gt; show OPEN TABLES where In_use &gt; 0;+----------+-------+--------+-------------+| Database | Table | In_use | Name_locked |+----------+-------+--------+-------------+| aaaa | aaa | 1 | 0 |+----------+-------+--------+-------------+1 row in set (0.00 sec) session 112root@db 15:21: [aaaa]&gt; unlock tables;Query OK, 0 rows affected (0.00 sec) 结论：在session1对表aaa进行read锁表，session1只能对表aaa进行读操作，对其他表没有任何操作权限，session2对表aaa有读权限，没有写权限。 3、开启两个会话，session1、session2,对表aaa进行write表锁session 112345678910111213141516171819root@db 15:21: [aaaa]&gt; lock table aaa write;Query OK, 0 rows affected (0.00 sec)root@db 15:26: [aaaa]&gt; select * from aaa;+----+------+-------------+| id | name | telephone |+----+------+-------------+| 1 | a | 11111111111 || 2 | b | 22222222222 || 3 | c | 33333333333 || 4 | d | 44444444 |+----+------+-------------+4 rows in set (0.00 sec)root@db 15:26: [aaaa]&gt; select * from bbb;ERROR 1100 (HY000): Table &apos;bbb&apos; was not locked with LOCK TABLESroot@db 15:27: [aaaa]&gt; update aaa set name=&apos;e&apos; where id=1;Query OK, 1 row affected (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 0root@db 15:29: [aaaa]&gt; update bbb set name=&apos;e&apos; where id=1;ERROR 1100 (HY000): Table &apos;bbb&apos; was not locked with LOCK TABLES session 21234root@db 15:20: [aaaa]&gt; select * from aaa;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionroot@db 15:28: [aaaa]&gt; update aaa set name=&apos;e&apos; where id=1;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction session 112root@db 15:31: [aaaa]&gt; unlock tables;Query OK, 0 rows affected (0.00 sec) 结论：在session1对表aaa进行write锁表，session1对表aaa有读写权限，对其他表没有任何操作权限，session2对表aaa即没有读权限，又没有写权限。 4、开启两个会话，session1、session2,对表aaa进行write&amp;read表锁session112345678910111213141516171819202122232425262728293031323334root@db 15:32: [aaaa]&gt; lock table aaa write , aaa as t1 read;Query OK, 0 rows affected (0.00 sec)root@db 15:32root@db 04:01: [aaaa]&gt; select * from aaa;+----+------+-------------+| id | name | telephone |+----+------+-------------+| 1 | e | 11111111111 || 2 | e | 22222222222 || 3 | c | 33333333333 || 4 | d | 44444444 |+----+------+-------------+4 rows in set (0.00 sec)root@db 04:01: [aaaa]&gt; select * from aaa as t1;+----+------+-------------+| id | name | telephone |+----+------+-------------+| 1 | e | 11111111111 || 2 | e | 22222222222 || 3 | c | 33333333333 || 4 | d | 44444444 |+----+------+-------------+4 rows in set (0.00 sec)root@db 04:02: [aaaa]&gt; update aaa as t1 set name=&apos;e&apos; where id =1;ERROR 1099 (HY000): Table &apos;t1&apos; was locked with a READ lock and can&apos;t be updatedroot@db 04:04: [aaaa]&gt; update aaa set name=&apos;e&apos; where id =1;Query OK, 0 rows affected (0.00 sec)Rows matched: 1 Changed: 0 Warnings: 0root@db 04:04: [aaaa]&gt; insert into aaa select * from aaa;ERROR 1100 (HY000): Table &apos;aaa&apos; was not locked with LOCK TABLESroot@db 04:04: [aaaa]&gt; insert into aaa select * from aaa as t1;ERROR 1062 (23000): Duplicate entry &apos;1&apos; for key &apos;PRIMARY&apos; session21234root@db 04:05: [aaaa]&gt; select * from aaa;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionroot@db 04:06: [aaaa]&gt; update aaa set name=&apos;e&apos; where id=2;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction session112root@db 04:07: [aaaa]&gt; unlock tables;Query OK, 0 rows affected (0.00 sec) 结论：session1对表aaa同时进行read，write锁，需要使用别名。对表进行select,update操作正常，如果使用insert into aaa select * from aaa as t1;需要加上别名。session 2对表aaa即没有读权限，又没有写权限。 5、对表aaa进行read表锁，并使用别名session 112345678910111213141516root@db 03:47: [aaaa]&gt; LOCK TABLE aaa AS t1 READ;Query OK, 0 rows affected (0.00 sec)root@db 03:47: [aaaa]&gt; select * from aaa;ERROR 1100 (HY000): Table &apos;aaa&apos; was not locked with LOCK TABLESroot@db 03:47: [aaaa]&gt; select * from aaa as t1;+----+------+-------------+| id | name | telephone |+----+------+-------------+| 1 | e | 11111111111 || 2 | e | 22222222222 || 3 | c | 33333333333 || 4 | d | 44444444 || 5 | f | 5555555 |+----+------+-------------+5 rows in set (0.00 sec) session 21234567891011 select * from aaa;+----+------+-------------+| id | name | telephone |+----+------+-------------+| 1 | e | 11111111111 || 2 | e | 22222222222 || 3 | c | 33333333333 || 4 | d | 44444444 || 5 | f | 5555555 |+----+------+-------------+5 rows in set (0.00 sec) session 112root@db 03:48: [aaaa]&gt; unlock tables;Query OK, 0 rows affected (0.00 sec) 结论：session 1对表aaa加别名read表锁，session1查询需要使用别名，直接查询无效，session2对表aaa有读权限，无写权限 6、对表aaa使用write表锁，并添加别名session 11234567891011121314151617181920212223root@db 03:51: [aaaa]&gt; LOCK TABLE aaa AS t1 write;Query OK, 0 rows affected (0.00 sec)root@db 03:53: [aaaa]&gt; root@db 03:53: [aaaa]&gt; select * from aaa;ERROR 1100 (HY000): Table &apos;aaa&apos; was not locked with LOCK TABLESroot@db 03:53: [aaaa]&gt; select * from aaa as t1;+----+------+-------------+| id | name | telephone |+----+------+-------------+| 1 | e | 11111111111 || 2 | e | 22222222222 || 3 | c | 33333333333 || 4 | d | 44444444 || 5 | f | 5555555 |+----+------+-------------+5 rows in set (0.00 sec)root@db 03:53: [aaaa]&gt; update aaa set name=&apos;e&apos; where id =1;ERROR 1100 (HY000): Table &apos;aaa&apos; was not locked with LOCK TABLESroot@db 03:54: [aaaa]&gt; update aaa as t1 set name=&apos;e&apos; where id =1;Query OK, 0 rows affected (0.00 sec)Rows matched: 1 Changed: 0 Warnings: 0 session 212root@db 03:55: [aaaa]&gt; select * from aaa;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction session 112root@db 03:55: [aaaa]&gt; unlock tables;Query OK, 0 rows affected (0.00 sec) 结论：session 1对表aaa加别名write表锁，session1查询和更改需要使用别名，直接查询和更改无效，session2对表aaa无读权限，无写权限 ##7、额外提示：1LOCK TABLES或者UNLOCK TABLES，当应用于分区表时，始终锁定或解锁整个表; 这些语句不支持分区锁定修剪]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Zabbix] zabbix监控mongodb数据库]]></title>
    <url>%2F2018%2F08%2F15%2Fzabbix%E7%9B%91%E6%8E%A7mongodb%2F</url>
    <content type="text"><![CDATA[1、创建监控脚本，用于连接mongodb数据库，可根据自身数据库配置修改该脚本mkdir -p /usr/local/zabbix/scriptvim /usr/local/zabbix/script/zabbix_monitor_mongodb.sh123456789101112#!/bin/bash#mongodb管理员用户authuser=admin1#mongodb管理员密码authpass=admin123#Mongodb指定验证数据库authdb=admin#mongodb指定端口dbport=30000#mongodb安装路径dbpath=/data/mongodb/bin$&#123;dbpath&#125;/mongo --port $&#123;dbport&#125; -u $&#123;authuser&#125; -p $&#123;authpass&#125; --authenticationDatabase $&#123;authdb&#125; 授权脚本执行权限1chmod +x /usr/local/zabbix/script/zabbix_monitor_mongodb.sh 2、修改zabbix监控项脚本，用于获取mongodb参数vim /usr/local/zabbix/etc/zabbix_agentd.conf.d/zabbix_mongodb.conf1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#当前连接数，包括当前的shell会话，副本集成员连接，mongos实例连接#(4.0version)connections.current[*],echo &quot;db.serverStatus().connections.current&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 4p#3.0versionUserParameter=connections.current[*],echo &quot;db.serverStatus().connections.current&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p#当前可用的连接数，数据库上的连接负载的值#(4.0version)connections.available[*],echo &quot;db.serverStatus().connections.available&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 4p#3.0versionUserParameter=connections.available[*],echo &quot;db.serverStatus().connections.available&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p#服务器所有的连接，包括已经关闭的连接#(4.0version)connections.totalCreated[*],echo &quot;db.serverStatus().connections.totalCreated&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 4p#3.0versionUserParameter=connections.totalCreated[*],echo &quot;db.serverStatus().connections.totalCreated&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p|cut -d &apos;(&apos; -f2|cut -d &apos;)&apos; -f1#因锁而造成排队等待的总数#(4.0version)UserParameter=globalLock.currentQueue.total[*],echo &quot;db.serverStatus().globalLock.currentQueue.total&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 4p#3.0versionUserParameter=globalLock.currentQueue.total[*],echo &quot;db.serverStatus().globalLock.currentQueue.total&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p#因读锁而造成排队等待的数量#(4.0version)UserParameter=globalLock.currentQueue.readers[*],echo &quot;db.serverStatus().globalLock.currentQueue.readers&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 4p#3.0versionUserParameter=globalLock.currentQueue.readers[*],echo &quot;db.serverStatus().globalLock.currentQueue.readers&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p#因写锁而造成排队等待的数量#(4.0version)UserParameter=globalLock.currentQueue.writers[*],echo &quot;db.serverStatus().globalLock.currentQueue.writers&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p#3.0versionUserParameter=globalLock.currentQueue.writers[*],echo &quot;db.serverStatus().globalLock.currentQueue.writers&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p#当前数据库进程占用内存情况#(4.0version)mem.resident[*],echo &quot;db.serverStatus().mem.resident&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 4p#3.0versionUserParameter=mem.resident[*],echo &quot;db.serverStatus().mem.resident&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p#当前数据库进程占用虚拟内存的大小#(4.0version)mem.virtual[*],echo &quot;db.serverStatus().mem.virtual&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 4p#3.0versionUserParameter=mem.virtual[*],echo &quot;db.serverStatus().mem.virtual&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p#流入mongodb数据库的总量#(4.0version)network.bytesIn[*],echo &quot;db.serverStatus().network.bytesIn&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh| grep NumberLong |cut -d &apos;(&apos; -f2|cut -d &apos;)&apos; -f1#3.0versionUserParameter=network.bytesIn[*],echo &quot;db.serverStatus().network.bytesIn&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p#数据库流出总量#(4.0version)network.bytesOut[*],echo &quot;db.serverStatus().network.bytesOut&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh| grep NumberLong |cut -d &apos;(&apos; -f2|cut -d &apos;)&apos; -f1|cut -d &apos;&quot;&apos; -f2|cut -d &apos;&quot;&apos; -f1#3.0versionUserParameter=network.bytesOut[*],echo &quot;db.serverStatus().network.bytesOut&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p#数据库总请求数#(4.0version)network.numRequests[*],echo &quot;db.serverStatus().network.numRequests&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh| grep NumberLong |cut -d &apos;(&apos; -f2|cut -d &apos;)&apos; -f1#3.0versionUserParameter=network.numRequests[*],echo &quot;db.serverStatus().network.numRequests&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p#当前副本集状态，为1代表为主节点，为2代表为从节点#(4.0version)rs.status.myState[*],echo &quot;rs.status().myState&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 4p#3.0versionUserParameter=rs.status.myState[*],echo &quot;rs.status().myState&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p#页错误总数，当数据库性能不佳、内存限制、或者数据库较大会导致该值增加#(4.0version)extra_info.page_faults[*],echo &quot;db.serverStatus().extra_info.page_faults&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 4p#3.0versionUserParameter=extra_info.page_faults[*],echo &quot;db.serverStatus().extra_info.page_faults&quot;|sh /usr/local/zabbix/script/zabbix_monitor_mongodb.sh|sed -n 3p 3、修改zabbix-agent配置文件，添加zabbix_agentd.conf.d目录，用于加载该目录下文件vim /usr/local/zabbix/etc/zabbix_agentd.conf1Include=/usr/local/zabbix/etc/zabbix_agentd.conf.d/ 4、重新启动agent客户端1/etc/init.d/zabbix_agentd restart 5、通过zabbix-web端，添加配置模板，参考模板，点击下载]]></content>
      <tags>
        <tag>zabbix</tag>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Zabbix] zabbix监控mysql数据库]]></title>
    <url>%2F2018%2F08%2F14%2Fzabbix%E7%9B%91%E6%8E%A7mysql%2F</url>
    <content type="text"><![CDATA[1、编写定期收集mysql信息脚本，并指定到对应目录cat /usr/local/zabbix/script/mysql_monitor.sh123456789101112131415161718192021222324#!/bin/bashdbpath='/data/mysql/bin/mysql'#mysql的用户dbuser='root'#mysql的密码dbpass='123456'#mysql的端口dbport='3306'#mysql的socket文件dbsocket='/data/mysql/tmp/mysql.sock'#mysql-status日志路径dbstatuspath=/tmp/mysql_status_monitor.log#mysql-engine-status日志路径dbenginestatuspath=/tmp/mysql_engine_innodb_status.log#self-define-scriptdbselfpath=/tmp/mysql_self_status.log#查询mysql-status信息$&#123;dbpath&#125; -u$&#123;dbuser&#125; -p$&#123;dbpass&#125; -S$&#123;dbsocket&#125; -P$&#123;dbport&#125; -BNe "show global status;" &gt; $&#123;dbstatuspath&#125;#查询mysql-engine-innodb信息$&#123;dbpath&#125; -u$&#123;dbuser&#125; -p$&#123;dbpass&#125; -S$&#123;dbsocket&#125; -P$&#123;dbport&#125; -BNe "show engine innodb status\G" &gt; $&#123;dbenginestatuspath&#125;#检查集群节点nodecount=`export MYSQL_PWD=$&#123;dbpass&#125;;$&#123;dbpath&#125; -u$&#123;dbuser&#125; --socket=/data/mysql/data/mysql.sock -P$&#123;dbport&#125; -BNe "select count(*) from performance_schema.replication_group_members where MEMBER_STATE != 'ONLINE'"`echo -e "nodecount $&#123;nodecount&#125;" &gt; $&#123;dbselfpath&#125; 根据自己的数据库配置，修改相应的参数，修改完成之后授权脚本执行权限：1chmod +x /usr/local/zabbix/script/mysql_monitor.sh 2、将脚本添加到定时任务，每分钟执行一次crontab -l1* * * * * sh /usr/local/zabbix/script/mysql_monitor.sh 3、编写获取数据脚本，对定时生成的log文件进行信息筛选cat /usr/local/zabbix/script/mysql_get_data.sh12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394case $1 in nodecount) #查询集群节点存活状态 cat /tmp/mysql_self_status.log |awk '/nodecount/ &#123;print $2&#125;' |head -1 ;; uptime) cat /tmp/mysql_status_monitor.log | awk '/Uptime/ &#123;print $2&#125;' |head -1 ;; com_select) cat /tmp/mysql_status_monitor.log | awk '/Com_select/ &#123;print $2&#125;' ;; com_insert) cat /tmp/mysql_status_monitor.log | awk '/Com_insert/ &#123;print $2&#125;'|head -1 ;; com_update) cat /tmp/mysql_status_monitor.log | awk '/Com_update/ &#123;print $2&#125;'|head -1 ;; com_delete) cat /tmp/mysql_status_monitor.log | awk '/Com_delete/ &#123;print $2&#125;'|head -1 ;; connections) cat /tmp/mysql_status_monitor.log | awk '/Connections/ &#123;print $2&#125;'|head -1 ;; thread_cached) cat /tmp/mysql_status_monitor.log | awk '/Threads_cached/ &#123;print $2&#125;' ;; threads_connected) cat /tmp/mysql_status_monitor.log | awk '/Threads_connected/ &#123;print $2&#125;' ;; thread_created) cat /tmp/mysql_status_monitor.log | awk '/Threads_created/ &#123;print $2&#125;' ;; Threads_running) cat /tmp/mysql_status_monitor.log | awk '/Threads_running/ &#123;print $2&#125;' ;; table_locks_immediate) cat /tmp/mysql_status_monitor.log | awk '/Table_locks_immediate/ &#123;print $2&#125;' ;; table_locks_waited) cat /tmp/mysql_status_monitor.log | awk '/Table_locks_waited/ &#123;print $2&#125;' ;; slow_launch_threads) cat /tmp/mysql_status_monitor.log | awk '/Slow_launch_threads/ &#123;print $2&#125;' ;; slow_queries) cat /tmp/mysql_status_monitor.log | awk '/Slow_queries/ &#123;print $2&#125;' ;; qps) uptime=`cat /tmp/mysql_status_monitor.log | awk '/Uptime/ &#123;print $2&#125;' |head -1` questions=`cat /tmp/mysql_status_monitor.log | awk '/Questions/ &#123;print $2&#125;'` echo $(printf "%.2f" `echo "scale=2;$&#123;questions&#125;/$&#123;uptime&#125;"|bc`) ;; tps) uptime=`cat /tmp/mysql_status_monitor.log | awk '/Uptime/ &#123;print $2&#125;' |head -1` com_commit=`cat /tmp/mysql_status_monitor.log | awk '/Com_commit/ &#123;print $2&#125;'` com_rollback=`cat /tmp/mysql_status_monitor.log | awk '/Com_rollback/ &#123;print $2&#125;'|head -1` com_sum=$(($&#123;com_commit&#125;+$&#123;com_rollback&#125;)) echo $(printf "%.2f" `echo "scale=2;$&#123;com_sum&#125;/$&#123;uptime&#125;"|bc`) ;; innodb_buffer_read_hits) innodb_buffer_pool_reads=`cat /tmp/mysql_status_monitor.log | awk '/Innodb_buffer_pool_reads/ &#123;print $2&#125;'|head -1` innodb_buffer_pool_read_requests=`cat /tmp/mysql_status_monitor.log | awk '/Innodb_buffer_pool_read_requests/ &#123;print $2&#125;'|head -1` innodb_buffer_read_diff=$(($&#123;innodb_buffer_pool_read_requests&#125;-$&#123;innodb_buffer_pool_reads&#125;)) echo $(printf "%.2f" `echo "scale=2;$&#123;innodb_buffer_read_diff&#125;/$&#123;innodb_buffer_pool_read_requests&#125;"|bc`) ;; table_cache_hit) cat /tmp/mysql_status_monitor.log | awk '/Opened_tables/ &#123;print $2&#125;' ;; thread_cache_hits) thread_created=`cat /tmp/mysql_status_monitor.log | awk '/Threads_created/ &#123;print $2&#125;'` connections=`cat /tmp/mysql_status_monitor.log | awk '/Connections/ &#123;print $2&#125;'|head -1` thread_cache_diff=$(($&#123;connections&#125;-$&#123;thread_created&#125;)) echo $(printf "%.2f" `echo "scale=2;$&#123;thread_cache_diff&#125;/$&#123;connections&#125;"|bc`) ;; create_tmp_tables_hits) created_tmp_tables=`cat /tmp/mysql_status_monitor.log | awk '/Created_tmp_tables/ &#123;print $2&#125;'` created_tmp_disk_tables=`cat /tmp/mysql_status_monitor.log | awk '/Created_tmp_disk_tables/ &#123;print $2&#125;'` echo $(printf "%.2f" `echo "scale=2;$&#123;created_tmp_disk_tables&#125;/$&#123;created_tmp_tables&#125;"|bc`) ;; binlog_cache_disk_use) cat /tmp/mysql_status_monitor.log | awk '/Binlog_cache_disk_use/ &#123;print $2&#125;' ;; table_locks_immediate) cat /tmp/mysql_status_monitor.log | awk '/Table_locks_immediate/ &#123;print $2&#125;' ;; table_locks_waited) cat /tmp/mysql_status_monitor.log | awk '/Table_locks_waited/ &#123;print $2&#125;' ;; innodb_row_lock_waits) cat /tmp/mysql_status_monitor.log | awk '/Innodb_row_lock_waits/ &#123;print $2&#125;' ;; *) ;;esac 4、编写zabbix监控项文件,通过/usr/local/zabbix/script/mysql_get_data.sh脚本去获取对应监控的数值cat /usr/local/zabbix/etc/zabbix_agentd.conf.d/mysql_monitor_parameter.conf1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#集群节点数量UserParameter=nodecount[*],sh /usr/local/zabbix/script/mysql_get_data.sh nodecount#系统运行时间UserParameter=uptime[*],sh /usr/local/zabbix/script/mysql_get_data.sh uptime#查看select语句的执行数UserParameter=com_select[*],sh /usr/local/zabbix/script/mysql_get_data.sh com_select#查看insert语句的执行数UserParameter=com_insert[*],sh /usr/local/zabbix/script/mysql_get_data.sh com_insert#查看update语句的执行数UserParameter=com_update[*],sh /usr/local/zabbix/script/mysql_get_data.sh com_update#查看delete语句的执行数UserParameter=com_delete[*],sh /usr/local/zabbix/script/mysql_get_data.sh com_delete#查看试图连接到MySQL(不管是否连接成功)的连接数UserParameter=connections[*],sh /usr/local/zabbix/script/mysql_get_data.sh connections#查看线程缓存内的线程的数量UserParameter=thread_cached[*],sh /usr/local/zabbix/script/mysql_get_data.sh thread_cached#当线程打开连接数UserParameter=threads_connected[*],sh /usr/local/zabbix/script/mysql_get_data.sh threads_connected#查看创建用来处理连接的线程数。如果Threads_created较大，你可能要增加thread_cache_size值UserParameter=thread_created[*],sh /usr/local/zabbix/script/mysql_get_data.sh thread_created#查看激活的(非睡眠状态)线程数UserParameter=threads_running[*],sh /usr/local/zabbix/script/mysql_get_data.sh threads_running#查看创建时间超过slow_launch_time秒的线程数。UserParameter=slow_launch_threads[*],sh /usr/local/zabbix/script/mysql_get_data.sh slow_launch_threads#查看查询时间超过long_query_time秒的查询的个数。UserParameter=slow_queries[*],sh /usr/local/zabbix/script/mysql_get_data.sh slow_queries#在服务器上执行语句的数量，只包括客户端发送给服务器的语句，并不包括存储项目执行的语句UserParameter=qps[*],sh /usr/local/zabbix/script/mysql_get_data.sh qps#事物提交数量com_commit#事物回滚数量com_rollbackUserParameter=tps[*],sh /usr/local/zabbix/script/mysql_get_data.sh tps#在buffer pool满足不了逻辑读的请求，而必须直接从硬盘读取的数量innodb_buffer_pool_reads#逻辑读的请求数innodb_buffer_pool_read_requests#buffer_read命中率UserParameter=innodb_buffer_read_hits[*],sh /usr/local/zabbix/script/mysql_get_data.sh innodb_buffer_read_hits#打开表的数量open_tables#已经打开表的数量opened_tables#table_cache命中率UserParameter=table_cache_hit[*],sh /usr/local/zabbix/script/mysql_get_data.sh table_cache_hit#thread_cache命中率UserParameter=thread_cache_hits[*],sh /usr/local/zabbix/script/mysql_get_data.sh thread_cache_hits#执行语句过程中创建的临时表created_tmp_tables#当创建的内部临时表过大，mysql会自动将表从内存中转换为磁盘上的表created_tmp_disk_tables#在磁盘上创建临时表的比例UserParameter=create_tmp_tables_hits[*],sh /usr/local/zabbix/script/mysql_get_data.sh create_tmp_tables_hits#事物使用binlog但是临时二进制日志超过binlog_cache_size，使用临时文件存储事物的语句，如果该值不为0，需要调整binlog_cache_sizeUserParameter=binlog_cache_disk_use[*],sh /usr/local/zabbix/script/mysql_get_data.sh binlog_cache_disk_use#可以立即授予表锁请求的次数UserParameter=table_locks_immediate[*],sh /usr/local/zabbix/script/mysql_get_data.sh table_locks_immediate#一个表锁的请求不能立即被授予，需要等待的次数,如果该值过高，说明数据库有性能问题，应该优化sql语句，表分区或者读写分离UserParameter=table_locks_waited[*],sh /usr/local/zabbix/script/mysql_get_data.sh table_locks_waited#操作innodb表等待行数的次数UserParameter=innodb_row_lock_waits[*],sh /usr/local/zabbix/script/mysql_get_data.sh innodb_row_lock_waits 5、修改/usr/local/zabbix/etc/zabbix_agentd.conf配置文件,添加该行参数：1Include=/usr/local/zabbix/etc/zabbix_agentd.conf.d/ 添加完成之后，需要重启zabbix_agent客户端 6、通过zabbix-web控制台，自定义一个模板，新建监控项、触发器、图形等内容，下面链接是我创建的比较简单的mysql模板，上传到百度云，仅供参考，点击下载7、上面模式中tps和qps的算法是按照自数据库启动获取的参数值与uptime相除获取的每秒内执行数，下面计算另一个取值方法：取单位时间内前后值得差值，得到的差值除以时间间隔，获取单位时间内的qps和tps值，修改配置文件/usr/local/zabbix/script/mysql_get_data.sh：1234567891011121314qps) #uptime=`cat /tmp/mysql_status_monitor.log | awk &apos;/Uptime/ &#123;print $2&#125;&apos; |head -1` #questions=`cat /tmp/mysql_status_monitor.log | awk &apos;/Questions/ &#123;print $2&#125;&apos;` #echo $(printf &quot;%.2f&quot; `echo &quot;scale=2;$&#123;questions&#125;/$&#123;uptime&#125;&quot;|bc`) cat /tmp/mysql_status_monitor.log | awk &apos;/Questions/ &#123;print $2&#125;&apos; ;;tps) #uptime=`cat /tmp/mysql_status_monitor.log | awk &apos;/Uptime/ &#123;print $2&#125;&apos; |head -1` com_commit=`cat /tmp/mysql_status_monitor.log | awk &apos;/Com_commit/ &#123;print $2&#125;&apos;` com_rollback=`cat /tmp/mysql_status_monitor.log | awk &apos;/Com_rollback/ &#123;print $2&#125;&apos;|head -1` com_sum=$(($&#123;com_commit&#125;+$&#123;com_rollback&#125;)) echo $&#123;com_sum&#125; #echo $(printf &quot;%.2f&quot; `echo &quot;scale=2;$&#123;com_sum&#125;/$&#123;uptime&#125;&quot;|bc`) ;; 修改zabbix模板监控项，将tps、qps的监控项下添加如下图所示进程：亦可点击此处下载，获取该模板，并导入zabbix。]]></content>
      <tags>
        <tag>mysql</tag>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] 去掉查询结果中带的mysql: [Warning] Using a password on the command line interface can be insecure]]></title>
    <url>%2F2018%2F08%2F14%2F%E5%8E%BB%E6%8E%89%E6%9F%A5%E8%AF%A2%E7%BB%93%E6%9E%9C%E4%B8%AD%E5%B8%A6%E7%9A%84mysql%20%5BWarning%5D%20Using%20a%20password%20on%20the%20command%20line%20interface%20can%20be%20insecure%2F</url>
    <content type="text"><![CDATA[1、一般情况为了方便，在使用mysql命令查询的时候会在-p后面直接加上密码,输出结果如下：1mysql -u root -p12345678 -BNe "select count(*) from performance_schema.replication_group_members where MEMBER_STATE='ONLINE'" 结果如下：12mysql: [Warning] Using a password on the command line interface can be insecure.3 2、为了消除结果中的[Warning]信息，只显示查询结果，可使用环境变量存储密码的方法：1export MYSQL_PWD=12345678;mysql -u root -BNe "select count(*) from performance_schema.replication_group_members where MEMBER_STATE='ONLINE'" 结果如下：13]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql-innodb buffer pool size调整]]></title>
    <url>%2F2018%2F08%2F14%2Fmysql-innodb%20buffer%20pool%20size%E8%B0%83%E6%95%B4%2F</url>
    <content type="text"><![CDATA[1、查看当前数据库innodb_buffer_pool参数1show global variables like 'innodb_buffer_pool_size'; ##2、查看page_size大小1show variables like 'innodb_page_size'; 官方文档参数详解123Innodb_page_sizeInnoDB page size (default 16KB). Many values are counted in pages; the page size enables them to beeasily converted to bytes 3、查看当前内存innodb页的总数量和包含数据的页的数量12show global status like 'Innodb_buffer_pool_pages_data';show global status like 'Innodb_buffer_pool_pages_total'; 官方文档参数详解：123456Innodb_buffer_pool_pages_dataThe number of pages in the InnoDB buffer pool containing data. The number includes both dirty andclean pages.Innodb_buffer_pool_pages_totalThe total size of the InnoDB buffer pool, in pages. 4、调优参考计算方法：123val = Innodb_buffer_pool_pages_data / Innodb_buffer_pool_pages_total * 100%val &gt; 95% 则考虑增大 innodb_buffer_pool_size， 建议使用物理内存的75%val &lt; 95% 则考虑减小 innodb_buffer_pool_size， 建议设置为：Innodb_buffer_pool_pages_data * Innodb_page_size * 1.05 / (1024*1024*1024) 5、设置innodb_buffer_pool_siz大小设置命令：1set global innodb_buffer_pool_size = 17179869184; 缓冲池字节大小，单位kb，如果不设置，默认为128M5.7版本以后可以动态修改参数，但是也要修改配置文件参数，防止重启之后，参数又变成配置文件内的参数,5.7以下的版本为静态参数，需要修改配置文件，并重新启动mysql1234cat /etc/my.cnf---------innodb_buffer_pool_size = 17179869184 #设置16G--------- 6、配置参数也可使用M、G等参数，内容如下：12345/etc/my.cnf：-------------------innodb_buffer_pool_size = 16G #设置16Ginnodb_buffer_pool_size = 500M #设置500M--------------------------]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mongodb] mongodb4.0定时备份并邮件告知备份情况]]></title>
    <url>%2F2018%2F08%2F12%2Fmongodb4.0%E5%AE%9A%E6%97%B6%E5%A4%87%E4%BB%BD%E5%B9%B6%E9%82%AE%E4%BB%B6%E5%91%8A%E7%9F%A5%E5%A4%87%E4%BB%BD%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[1、mongodb全量备份脚本，内容如下：cat /data/soft/mongodb_backup.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141#!/bin/bash#认证用户名authuser=admin1#认证用户对应密码authpass=admin123#备份的服务器地址servername=testrepl/127.0.0.1:30000#备份的数据库实例名称instancename=test#认证数据库实例名称authdbname=admin#备份脚本运行获取到的时间戳stamp=`date +"%Y-%m-%d"`#项目名称programname=bulingbuling#备份文件目录backuppath=/data/backup#备份文件绝对路径名称backname=$&#123;backuppath&#125;/$&#123;instancename&#125;#需要删除的备份文件绝对路径名称oldstamp=`date +"%Y-%m-%d" -d "-5 day"`oldbackname=$&#123;backname&#125;-$&#123;oldstamp&#125;.dump#数据库安装路径dbpath=/data/mongodb/bin#备份操作log目录backuplogname=/tmp/mongodb_bakcup_$&#123;stamp&#125;.log#当前服务器主机名localname=`hostname`#当前服务器ip地址localip=`ip a |grep global| head -1| awk '&#123;print $2&#125;'|awk -F'/' '&#123;print $1&#125;'`#localip='65.52.165.104'#目标邮箱dest_mail='test@qq.com'#判断指定log文件是否存在，如果存在将其删除if [ -e $&#123;backuplogname&#125; ];then sudo rm -rf $&#123;backuplogname&#125;else :fi#备份执行失败发送错误邮件send_fail_mail()&#123; echo "$&#123;programname&#125;备份失败,登录服务器$&#123;localname&#125;-$&#123;localip&#125;的日志$&#123;backuplogname&#125;下查看报错！！！" |mail -s $&#123;programname&#125;数据库备份情况 $&#123;dest_mail&#125;&#125;#备份执行成功发送成功邮件 send_success_mail()&#123; echo "$&#123;programname&#125;备份成功,可登录服务器$&#123;localname&#125;-$&#123;localip&#125;的日志$&#123;backuplogname&#125;下查看备份信息。" |mail -s $&#123;programname&#125;数据库备份情况 $&#123;dest_mail&#125;&#125;#创建备份时间函数date_func()&#123; dateresult=$(($&#123;afterstamp&#125;-$&#123;beforstamp&#125;)) hour=$(($&#123;dateresult&#125;/3600)) min=$((($&#123;dateresult&#125;-$&#123;hour&#125;*3600)/60)) sec=$(($&#123;dateresult&#125;-$&#123;hour&#125;*3600-$&#123;min&#125;*60)) echo "6、本次备份运行时间为:"$&#123;hour&#125;时$&#123;min&#125;分$&#123;sec&#125;秒 &gt;&gt; $&#123;backuplogname&#125;&#125;#创建一个删除五天前的备份数据的函数del_old_bakdata()&#123; if [ -e $&#123;oldbackname&#125; ];then sudo echo "7、五天前的备份数据$&#123;oldbackname&#125;存在，进行删除" &gt;&gt; $&#123;backuplogname&#125; rm -rf $&#123;oldbackname&#125; if [ $? == 0 ];then sudo echo "7.1、备份数据删除成功" &gt;&gt; $&#123;backuplogname&#125; else sudo echo "7.1、备份数据删除失败，注意查看失败原因" &gt;&gt; $&#123;backuplogname&#125; fi else sudo echo "7、五天前的备份数据$&#123;oldbackname&#125;不存在，不需要再删除。" &gt;&gt; $&#123;backuplogname&#125; fi &#125;#因为mongodbdump只能指定路径，不能自定义备份文件名称，因此需要对备份完成的目录重命名rename_backname_func()&#123; if [ -e $&#123;backname&#125;-$&#123;stamp&#125; ];then mv $&#123;backname&#125;-$&#123;stamp&#125; $&#123;backname&#125;-$&#123;stamp&#125;-old mv $&#123;backname&#125; $&#123;backname&#125;-$&#123;stamp&#125; if [ $? == 0 ];then echo "5、备份目录重命名成功" &gt;&gt; $&#123;backuplogname&#125; else echo "5、备份目录重命名失败,查看命名失败原因！" &gt;&gt; $&#123;backuplogname&#125; fi else mv $&#123;backname&#125; $&#123;backname&#125;-$&#123;stamp&#125; if [ $? == 0 ];then echo "5、备份目录重命名成功" &gt;&gt; $&#123;backuplogname&#125; else echo "5、备份目录重命名失败,查看命名失败原因！" &gt;&gt; $&#123;backuplogname&#125; fi fi&#125;#备份之前判断备份目录是否存在if [ -d $&#123;backuppath&#125; ];then sudo echo "1、备份目录存在,可执行备份操作" &gt;&gt; $&#123;backuplogname&#125;else sudo echo "1、备份目录不存在,手动创建" &gt;&gt; $&#123;backuplogname&#125; sudo mkdir -p $&#123;backuppath&#125; if [ $? == 0 ];then sudo echo "1.1、备份目录创建成功，可进行下面操作" &gt;&gt; $&#123;backuplogname&#125; else sudo echo "1.1、备份目录不存在，且创建失败，无法进行下面操作，退出备份" &gt;&gt; $&#123;backuplogname&#125; exit fifi#备份之前判断备份文件是否已经存在if [ -d $&#123;backname&#125; ];then sudo echo "2、指定的备份实例目录$&#123;backname&#125;已存在，对其进行重命名" &gt;&gt; $&#123;backuplogname&#125; mv $&#123;backname&#125; $&#123;backuppath&#125;/bak-$&#123;fullbackupname&#125;-$&#123;stamp&#125;.dump if [ $? == 0 ];then sudo echo "2.1、文件已重新命名为$&#123;backuppath&#125;/bak-$&#123;fullbackupname&#125;-$&#123;stamp&#125;.dump,可继续备份" &gt;&gt; $&#123;backuplogname&#125; else sudo echo "2.1、文件重命名失败，退出备份操作" &gt;&gt; $&#123;backuplogname&#125; exit fielse sudo echo "2、指定的备份文件不存在，开始进行备份" &gt;&gt; $&#123;backuplogname&#125;fi#备份之前时间戳beforstamp=`date +%s`#开始全量备份数据库sudo echo "3、开始全量备份数据库" &gt;&gt; $&#123;backuplogname&#125;$&#123;dbpath&#125;/mongodump -h $&#123;servername&#125; -u $&#123;authuser&#125; -p $&#123;authpass&#125; -d $&#123;instancename&#125; -o $&#123;backuppath&#125; --authenticationDatabase $&#123;authdbname&#125;backupresult=$?afterstamp=`date +%s`if [ $&#123;backupresult&#125; == 0 ];then sudo echo "4、$&#123;programname&#125;-$&#123;instancename&#125;数据库备份成功,文件名称为$&#123;backname&#125;，重命名备份文件" &gt;&gt; $&#123;backuplogname&#125; rename_backname_func date_func del_old_bakdata send_success_mailelse sudo echo "4、$&#123;programname&#125;-$&#123;instancename&#125;数据库备份失败,注意查看备份失败原因并重新备份" &gt;&gt; $&#123;backuplogname&#125; send_fail_mail exitfi 使用该脚本需要修改自己数据库对应的用户名authuser,密码authpass,邮箱dest_mail,项目名称programname,服务器名称servername,数据库实例名称instancename,mongodb数据库安装路径dbpath。其他参数可使用脚本内默认的，也可自行修改 2、配置邮件发送功能2.1、安装mailx软件1yum install -y mailx 2.2、修改配置文件/etc/mail.rc,在行尾添加一下信息：12set from=test@qq.com smtp=smtp.qq.comset smtp-auth-user=test@qq.com smtp-auth-password=testpassword smtp-auth=login 参数详解：1234from:指定发送邮件的发件人smtp:指定smtp服务器信息smtp-auth-user:允许第三方登录的用户名smtp-auth-password：允许第三方登录的密码 2.3、发送测试邮件：1echo 'test email' |mail -s 'title' test@qq.com title:指定发送文件的标题，可自行定义 3、修改备份脚本权限1chmod +x /data/soft/mongodb_backup.sh 4、将脚本添加到定时任务如：每天凌晨一点备份1shell &gt; crontab -e 10 1 * * * sh /data/soft/mongodb_backup.sh]]></content>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mongodb] mongodb4.0备份与恢复]]></title>
    <url>%2F2018%2F08%2F12%2Fmongdob4.0%20%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[1、mongodb备份命令1.1、备份之前查看备份实例的相关数据：12345678MongoDB Enterprise testrepl:PRIMARY&gt; use adminswitched to db adminMongoDB Enterprise testrepl:PRIMARY&gt; db.auth(&apos;admin1&apos;,&apos;admin123&apos;);1MongoDB Enterprise testrepl:PRIMARY&gt; use test;switched to db testMongoDB Enterprise testrepl:PRIMARY&gt; db.test_collection.find().count();844703 1.2、对于replica set集群模式命令如下：1/data/mongodb/bin/mongodump -h "testrepl/127.0.0.1:30000" -u admin1 -p admin123 -d test -o /data/backup/ --authenticationDatabase admin 结果如下：122018-08-12T03:40:01.489+0000 writing test.test_collection to 2018-08-12T03:40:03.823+0000 done dumping test.test_collection (844703 documents) 会在指定的备份目录下，生成备份实例名对应的文件夹，文件夹下有以下文件：1test_collection.bson test_collection.metadata.json 命令参数详解：1234567-h:指定当前备份主机ip--port:指定当前mongodb的启动端口-u:指定验证的用户名-d:需要备份的数据库实例-p:指定用户名对应的密码-o:指定备份的路径--authenticationDatabase:认证数据库 备份过程没有加上指定认证数据库“–authenticationDatabase admin”,会报一下错误1Failed: error connecting to db server: server returned error on SASL authentication step: Authentication failed. 1.3、对于单节点备份命令如下：1/data/mongodb/bin/mongodump -h 127.0.0.1 --port 30000 -u admin1 -p admin123 -d test -o /data/backup --authenticationDatabase admin 2、拷贝备份出来的文件，新建一台新节点(10.0.7.36)mongodb,并做恢复测试,恢复命令如下(因为测试环境节点有限，该恢复操作是将一个集群模式的备份，恢复到一个单点上的操作)：1/data/mongodb/bin/mongorestore -h 10.0.7.36 --port 30000 -u admin1 -p admin123 -d test /data/backup/test --authenticationDatabase admin 恢复输出结果如下：1234567892018-08-11T14:19:48.860+0000 the --db and --collection args should only be used when restoring from a BSON file. Other uses are deprecated and will not exist in the future; use --nsInclude instead2018-08-11T14:19:48.860+0000 building a list of collections to restore from /data/backup/test dir2018-08-11T14:19:48.885+0000 reading metadata for test.test_collection from /data/backup/test/test_collection.metadata.json2018-08-11T14:19:48.972+0000 restoring test.test_collection from /data/backup/test/test_collection.bson2018-08-11T14:19:51.781+0000 [###########.............] test.test_collection 28.4MB/57.1MB (49.8%)2018-08-11T14:19:54.490+0000 [########################] test.test_collection 57.1MB/57.1MB (100.0%)2018-08-11T14:19:54.490+0000 restoring indexes for collection test.test_collection from metadata2018-08-11T14:19:57.737+0000 finished restoring test.test_collection (844703 documents)2018-08-11T14:19:57.737+0000 done 控制台登录，查看数据是否一致：123456789101112131415MongoDB Enterprise &gt; use adminswitched to db adminMongoDB Enterprise &gt; db.auth(&apos;admin1&apos;,&apos;admin123&apos;);1MongoDB Enterprise &gt; show dbs;admin 0.000GBconfig 0.000GBlocal 0.000GBtest 0.031GBMongoDB Enterprise &gt; use testswitched to db testMongoDB Enterprise &gt; show tables;test_collectionMongoDB Enterprise &gt; db.test_collection.find().count();844703 恢复完成]]></content>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] 使用innodb trx和innodb lock信息表查看锁事物]]></title>
    <url>%2F2018%2F08%2F11%2F%E4%BD%BF%E7%94%A8innodb%20trx%E5%92%8Cinnodb%20lock%E4%BF%A1%E6%81%AF%E8%A1%A8%E6%9F%A5%E7%9C%8B%E9%94%81%E4%BA%8B%E7%89%A9%2F</url>
    <content type="text"><![CDATA[1、插入表测试数据12345678select * from aaa;+----+------+-------------+| id | name | telephone |+----+------+-------------+| 1 | a | 11111111111 || 2 | b | 22222222222 || 3 | c | 33333333333 |+----+------+-------------+ 2、创建三个会话，造成锁事物sessin 1:123BEGIN;SELECT id FROM aaa FOR UPDATE;SELECT SLEEP(60); session 2:1SELECT name FROM aaa FOR UPDATE; sesion 3:1SELECT telephone FROM aaa FOR UPDATE; 3、使用以下查询来查看正在等待的事务以及阻止它们的事务：session 41234567891011121314151617181920SELECT r.trx_id waiting_trx_id, r.trx_mysql_thread_id waiting_thread, r.trx_query waiting_query, b.trx_id blocking_trx_id, b.trx_mysql_thread_id blocking_thread, b.trx_query blocking_queryFROM information_schema.innodb_lock_waits wINNER JOIN information_schema.innodb_trx b ON b.trx_id = w.blocking_trx_idINNER JOIN information_schema.innodb_trx r ON r.trx_id = w.requesting_trx_id;查询结果如下：+----------------+----------------+--------------------------------------+-----------------+-----------------+---------------------------------+| waiting_trx_id | waiting_thread | waiting_query | blocking_trx_id | blocking_thread | blocking_query |+----------------+----------------+--------------------------------------+-----------------+-----------------+---------------------------------+| 2488 | 58 | SELECT telephone FROM aaa FOR UPDATE | 2487 | 57 | SELECT name FROM aaa FOR UPDATE || 2488 | 58 | SELECT telephone FROM aaa FOR UPDATE | 2486 | 53 | SELECT SLEEP(100) || 2487 | 57 | SELECT name FROM aaa FOR UPDATE | 2486 | 53 | SELECT SLEEP(100) |+----------------+----------------+--------------------------------------+-----------------+-----------------+---------------------------------+ 上述sql语句可能太繁琐，也可使用下面语句查询12345678SELECT waiting_trx_id, waiting_pid, waiting_query, blocking_trx_id, blocking_pid, blocking_queryFROM sys.innodb_lock_waits; 4、如果查询造成锁事物的会话已经变成空闲，上面查询出来的数据会变为空，可以通过process_id,来查询琐事物12show full processlist;| 57 | root | localhost | aaaa | Sleep | 566 | | NULL | 查询出process_id为57 5、通过processlist_id查询出thread_id123456SELECT THREAD_ID FROM performance_schema.threads WHERE PROCESSLIST_ID = 57;+-----------+| THREAD_ID |+-----------+| 79 |+-----------+ 6、根据thread_id查询出来造成锁事物的sql语句123456SELECT THREAD_ID, SQL_TEXT FROM performance_schema.events_statements_current WHERE THREAD_ID = 79\G*************************** 1. row ***************************THREAD_ID: 79 SQL_TEXT: SELECT name FROM aaa FOR UPDATE1 row in set (0.00 sec) 7、如果线程执行的最后一个查询不足以确定锁定的原因，则可以查询Performance Schema events_statements_history 表以查看该线程执行的最后10个语句。1234567SELECT THREAD_ID, SQL_TEXT FROM performance_schema.events_statements_history WHERE THREAD_ID = 79 ORDER BY EVENT_ID;+-----------+---------------------------------+| THREAD_ID | SQL_TEXT |+-----------+---------------------------------+| 79 | SELECT name FROM aaa FOR UPDATE |+-----------+---------------------------------+]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction]]></title>
    <url>%2F2018%2F08%2F11%2Fmysql-%20Lock%20wait%20timeout%20exceeded%3B%20try%20restarting%20transaction%2F</url>
    <content type="text"><![CDATA[1、对一个表进行ddl操作123mysql&gt;ALTER TABLE deposit_coin_order_user add `txid` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NOT NULL COMMENT '区块链id' ;提示报错：ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 查看该表数据只有39行数据：123456select count(*) from deposit_coin_order_user;+----------+| count(*) |+----------+| 39 |+----------+ 2、查看事物表Innodb_trx是否记录相关事物，如果有找到该事物的‘trx_mysql_thread_id’123456789101112131415161718192021222324252627SELECT * FROM information_schema.INNODB_TRX\G*************************** 1. row *************************** trx_id: 421462261342800 trx_state: RUNNING trx_started: 2018-08-10 08:24:58 trx_requested_lock_id: NULL trx_wait_started: NULL trx_weight: 0 trx_mysql_thread_id: 12989053 trx_query: NULL trx_operation_state: NULL trx_tables_in_use: 0 trx_tables_locked: 0 trx_lock_structs: 0 trx_lock_memory_bytes: 1136 trx_rows_locked: 0 trx_rows_modified: 0 trx_concurrency_tickets: 0 trx_isolation_level: REPEATABLE READ trx_unique_checks: 1 trx_foreign_key_checks: 1trx_last_foreign_key_error: NULL trx_adaptive_hash_latched: 0 trx_adaptive_hash_timeout: 0 trx_is_read_only: 0trx_autocommit_non_locking: 01 row in set (0.00 sec) 使用show full processlist查看是否有‘trx_mysql_thread_id’对应的进程，如果有就证明这个sleep的线程事务一直没有commit或者rollback而是卡住了，我们需要手动kill掉。没有的话看看有没有正在执行的很慢SQL记录线程。12mysql &gt; show full processlist;| 12989053 | cwreadonly | 10.1.10.9:51650 | exchange_market | Sleep | 1189 | | NULL 3、发现有id为12989053的sql，需要手动kill掉mysql &gt; KILL 12989053; 4、再次执行ddl语句，正常执行ALTER TABLE deposit_coin_order_user add txid varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NOT NULL COMMENT ‘区块链id’ ;]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql定时备份及邮件告知备份情况]]></title>
    <url>%2F2018%2F08%2F10%2Fmysql%E5%AE%9A%E6%97%B6%E5%A4%87%E4%BB%BD%E5%8F%8A%E9%82%AE%E4%BB%B6%E5%91%8A%E7%9F%A5%E5%A4%87%E4%BB%BD%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[1、mysql全量备份脚本，内容如下：cat /data/soft/mysql_backup.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118#!/bin/bash#数据库用户名dbuser=test#数据库密码dbpass=test#备份脚本运行获取到的时间戳stamp=`date +"%Y-%m-%d"`#项目名称programname=mw#备份文件名称前缀fullbackupname=$&#123;programname&#125;-dbfullbak#备份文件目录backuppath=/data/backup#备份文件绝对路径名称backname=$backuppath/$&#123;fullbackupname&#125;-$&#123;stamp&#125;.dump#需要删除的备份文件绝对路径名称oldstamp=`date +"%Y-%m-%d" -d "-5 day"`oldbackname=$backuppath/$&#123;fullbackupname&#125;-$&#123;oldstamp&#125;.dump#数据库安装路径dbpath=/data/mysql/bin#备份操作log目录backuplogname=/tmp/backup_$&#123;stamp&#125;.log#当前服务器主机名localname=`hostname`#当前服务器ip地址localip=`ip a |grep global| head -1| awk '&#123;print $2&#125;'|awk -F'/' '&#123;print $1&#125;'`#localip='65.52.165.104'#目标邮箱dest_mail='test@qq.com'#判断指定log文件是否存在，如果存在将其删除if [ -e $&#123;backuplogname&#125; ];then sudo rm -rf $&#123;backuplogname&#125;else :fi#备份执行失败发送错误邮件send_fail_mail()&#123; echo "$&#123;programname&#125;备份失败,登录服务器$&#123;localname&#125;-$&#123;localip&#125;的日志$&#123;backuplogname&#125;下查看报错！！！" |mail -s $&#123;programname&#125;数据库备份情况 $&#123;dest_mail&#125;&#125;#备份执行成功发送成功邮件 send_success_mail()&#123; echo "$&#123;programname&#125;备份成功,可登录服务器$&#123;localname&#125;-$&#123;localip&#125;的日志$&#123;backuplogname&#125;下查看备份信息。" |mail -s $&#123;programname&#125;数据库备份情况 $&#123;dest_mail&#125;&#125;#创建备份时间函数date_func()&#123; dateresult=$(($&#123;afterstamp&#125;-$&#123;beforstamp&#125;)) hour=$(($&#123;dateresult&#125;/3600)) min=$((($&#123;dateresult&#125;-$&#123;hour&#125;*3600)/60)) sec=$(($&#123;dateresult&#125;-$&#123;hour&#125;*3600-$&#123;min&#125;*60)) echo "5、本次备份运行时间为:"$&#123;hour&#125;时$&#123;min&#125;分$&#123;sec&#125;秒 &gt;&gt; $&#123;backuplogname&#125;&#125;#创建一个删除五天前的备份数据的函数del_old_bakdata()&#123; if [ -e $&#123;oldbackname&#125; ];then sudo echo "6、五天前的备份数据$&#123;oldbackname&#125;存在，进行删除" &gt;&gt; $&#123;backuplogname&#125; rm -rf $&#123;oldbackname&#125; if [ $? == 0 ];then sudo echo "6.1、备份数据删除成功" &gt;&gt; $&#123;backuplogname&#125; else sudo echo "6.1、备份数据删除失败，注意查看失败原因" &gt;&gt; $&#123;backuplogname&#125; fi else sudo echo "6、五天前的备份数据$&#123;oldbackname&#125;不存在，不需要再删除。" &gt;&gt; $&#123;backuplogname&#125; fi &#125;#备份之前判断备份目录是否存在if [ -d $&#123;backuppath&#125; ];then sudo echo "1、备份目录存在,可执行备份操作" &gt;&gt; $&#123;backuplogname&#125;else sudo echo "1、备份目录不存在,手动创建" &gt;&gt; $&#123;backuplogname&#125; sudo mkdir -p $&#123;backuppath&#125; if [ $? == 0 ];then sudo echo "1.1、备份目录创建成功，可进行下面操作" &gt;&gt; $&#123;backuplogname&#125; else sudo echo "1.1、备份目录不存在，且创建失败，无法进行下面操作，退出备份" &gt;&gt; $&#123;backuplogname&#125; exit fifi#备份之前判断备份文件是否已经存在if [ -e $&#123;backname&#125; ];then sudo echo "2、指定的备份文件$&#123;backname&#125;已存在，对其进行重命名" &gt;&gt; $&#123;backuplogname&#125; mv $&#123;backname&#125; $&#123;backuppath&#125;/bak-$&#123;fullbackupname&#125;-$&#123;stamp&#125;.dump if [ $? == 0 ];then sudo echo "2.1、文件已重新命名为$&#123;backuppath&#125;/bak-$&#123;fullbackupname&#125;-$&#123;stamp&#125;.dump,可继续备份" &gt;&gt; $&#123;backuplogname&#125; else sudo echo "2.1、文件重命名失败，退出备份操作" &gt;&gt; $&#123;backuplogname&#125; exit fielse sudo echo "2、指定的备份文件不存在，开始进行备份" &gt;&gt; $&#123;backuplogname&#125;fi#备份之前时间戳beforstamp=`date +%s`#开始全量备份数据库sudo echo "3、开始全量备份数据库" &gt;&gt; $&#123;backuplogname&#125;$&#123;dbpath&#125;/mysqldump -u$&#123;dbuser&#125; -p$&#123;dbpass&#125; --all-databases --single-transaction --master-data=2 --quick --set-gtid-purged=OFF &gt; $&#123;backname&#125;backupresult=$?afterstamp=`date +%s`if [ $&#123;backupresult&#125; == 0 ];then sudo echo "4、$&#123;programname&#125;数据库备份成功,文件名称为$&#123;backname&#125;，准备删除五天前的备份数据" &gt;&gt; $&#123;backuplogname&#125; date_func del_old_bakdata send_success_mailelse sudo echo "4、$&#123;programname&#125;数据库备份失败,注意查看备份失败原因并重新备份" &gt;&gt; $&#123;backuplogname&#125; send_fail_mail exitfi 使用该脚本需要修改自己数据库对应的用户名dbuser,密码dbpass,邮箱dest_mail,项目名称programname,mysql数据库安装路径dbpath。其他参数可使用脚本内默认的，也可自行修改其中–single-transaction：该参数在备份前将隔离模式设为repeatable read并启动start transaction语句。该参数只对innodb事物表有作用，当启动start transaction时开始备份数据库一致性状态，并不阻塞其他的应用。对于myisam和memory数据引擎使用这个参数备份的时候状态是实时改变的。另外该参数与–lock-tables参数是相互排斥的，lock-tables会是隐式吧提交的事物产生等待。–master-data=2：使用该参数指备份执行时binglog开始的坐标，在输出结果中属于change master to语句，如下所示：1-- CHANGE MASTER TO MASTER_LOG_FILE=&apos;mysql-binlog.000036&apos;, MASTER_LOG_POS=79549; 如果使用该参数不指定值，默认值为1，语句不会以注释的形式写入，当dump文件被加载时该行参数会影响加载过程：1CHANGE MASTER TO MASTER_LOG_FILE=&apos;mysql-binlog.000036&apos;, MASTER_LOG_POS=79549; 如果指定参数为2，会以注释的形式写入输出文件，dump文件被加载也不会受该行语句影响。 –quick：对于备份大表比较有用，该参数强制mysqldump命令一次性从数据库中获取表的行，而不是先获取行集合，在写出之前把他们缓存到内存。 2、配置邮件发送功能2.1、安装mailx软件1yum install -y mailx 2.2、修改配置文件/etc/mail.rc,在行尾添加一下信息：12set from=test@qq.com smtp=smtp.qq.comset smtp-auth-user=test@qq.com smtp-auth-password=testpassword smtp-auth=login 参数详解：1234from:指定发送邮件的发件人smtp:指定smtp服务器信息smtp-auth-user:允许第三方登录的用户名smtp-auth-password：允许第三方登录的密码 2.3、发送测试邮件：1echo 'test email' |mail -s 'title' test@qq.com title:指定发送文件的标题，可自行定义 3、修改备份脚本权限1chmod +x /data/soft/mysql_backup.sh 4、将脚本添加到定时任务如：每天凌晨一点备份1shell &gt; crontab -e 10 1 * * * sh /data/soft/mysql_backup.sh]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysqldump逻辑备份及恢复]]></title>
    <url>%2F2018%2F08%2F09%2Fmysqldump%E9%80%BB%E8%BE%91%E5%A4%87%E4%BB%BD%E5%8F%8A%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[0、mysql实验版本15.7.22-log 1、mysqldump备份操作详解：123456789101112131415161718-d --no-data No row information 只导出数据结构-t --no-create-info 只导出数据（不包含结构）1）导出所有数据库mysqldump -u root -p --all-databases &gt; all_db.dump2）导出指定数据库mysqldump -u root -p zabbixDB &gt; zabbix.dump3) 导出多个数据库mysqldump -u root -p --databases zabbixDB mysql &gt; zabbix_mysql.dump4）导出一个表mysqldump -u root -p zabbixDB task &gt; zabbixDB_task.dump5)导出一个数据库的多个表mysqldump -u root -p --databases zabbixDB --tables trends users &gt; zabbix_trends_users.dump6)条件导出----导出db1表a1中id=1的数据mysqldump -uroot -p --databases db1 --tables a1 --where=&apos;id=1&apos; &gt; db1_a1_id.dump7)将h1服务器中的db1数据库的所有数据导入到h2中的db2数据库中，db2的数据库必须存在否则会报错mysqldump --host=h1 -uroot -proot --databases db1 |mysql --host=h2 -uroot -proot db28)压缩备份mysqldump -uroot -p --databases zabbixDB 2&gt;/dev/null |gzip &gt; zabbixDB.gz ##开始实验 2、源库插入测试数据1234create database aaaa;use aaaa;create table aa (id int primary key);insert into aa values(&apos;1&apos;); 3、导出备份1mysqldump -u root -p --all-databases --set-gtid-purged=OFF &gt; /tmp/test.dump 4、在目标数据库进行还原，全库导入1mysql -u root -p12345678 &lt; test.dump 因为mysqldump属于逻辑备份，恢复步骤为先检查要恢复的database是否存在，如果不存在就手动创建，如果存在就进入database去检查表是否存在，如果存在，会先删除再手动创建,表创建完成之后插入数据,分析dump文件，具体内容如下： 4.1、创建database：12345---- Current Database: `aaaa`--CREATE DATABASE /*!32312 IF NOT EXISTS*/ `aaaa` /*!40100 DEFAULT CHARACTER SET utf8mb4 */;USE `aaaa`; 4.2、检查并创建表aaa:1234567891011---- Table structure for table `aaa`--DROP TABLE IF EXISTS `aaa`;/*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `aaa` ( `id` int(11) NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;/*!40101 SET character_set_client = @saved_cs_client */; 4.3、对表aaa插入备份出来的数据：12345678---- Dumping data for table `aaa`--LOCK TABLES `aaa` WRITE;/*!40000 ALTER TABLE `aaa` DISABLE KEYS */;INSERT INTO `aaa` VALUES (1);/*!40000 ALTER TABLE `aaa` ENABLE KEYS */;UNLOCK TABLES;]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mongodb] mongodb sharding集群配置]]></title>
    <url>%2F2018%2F08%2F08%2Fmongodb-sharding-cluster%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[0、前提需求为了满足mongodb数据库高可用环境，可以使用配置副本集CSRS（config server replica set)，搭建方法点击此处,但是当业务吞吐量特别高的时刻，单台节点已经不能满足访问需求，因此需要考虑sharding cluster 模式，提高访问性能。 1、将集群架构从csrs调整为sharding cluster,下文按照上面搭建的csrs架构继续调整：1.1、节点信息如下：主：10.0.7.53从：10.0.7.51仲裁：10.0.7.50依次关闭仲裁、从、主节点，修改配置文件mongodb.conf，添加如下信息：12sharding: clusterRole: shardsvr 1.2、修改完三个节点之后，依次启动主，从，仲裁三个节点，进入控制台使用rs.status()，查看集群状态是否正常。为了保证停机时间较短，可使用rs.stepDown()分别进行主从切换，对从库进行配置并重启。 2、新建一个shard1集群节点，操作如下(三个节点都要执行）：2.1、节点信息如下：从：10.0.7.53主：10.0.7.51仲裁：10.0.7.502.2、拷贝一份mongodb.conf文件，并重新命名cp mongodb.conf shard1mongodb.conf2.3、主节点需要在无需验证的模式下，先配置用户名密码。之后修改（三个节点）shard1mongodb.conf配置文件内容如下：cat shard1mongodb.conf123456789101112131415161718192021222324252627282930313233343536373839404142# mongod.conf# for documentation of all options, see:# http://docs.mongodb.org/manual/reference/configuration-options/# where to write logging data.systemLog: destination: file logAppend: true path: /data/mongodb/shard1/log/mongod.log# Where and how to store data.storage: dbPath: /data/mongodb/shard1/data/ journal: enabled: true# engine:# mmapv1:# wiredTiger:# how the process runsprocessManagement: fork: true # fork and run in background pidFilePath: /data/mongodb/shard1/log/mongod.pid # location of pidfile timeZoneInfo: /usr/share/zoneinfo# network interfacesnet: port: 30001 bindIp: 127.0.0.1,10.0.7.53 # Enter 0.0.0.0,:: to bind to all IPv4 and IPv6 addresses or, alternatively, use the net.bindIpAll setting.#security:security: authorization: enabled keyFile: /data/mongodb/shard1/mongodb.keyfile#operationProfiling:#replication:replication: replSetName: "shard1"#sharding:sharding: clusterRole: shardsvr## Enterprise-Only Options#auditLog:#snmp: 注意修改sharding、port、path、replname等参数 2.4、创建相应的数据目录和日志目录：1mkdir -p /data/mongodb/shard1/&#123;log,data&#125; 2.5、生成keyfile文件并拷贝keyfile文件到指定目录：1cp mongodb.keyfile /data/mongodb/shard1/ 2.6、三个节点启动mongodb-shard1:1/data/mongodb/bin/mongod -f /data/mongodb/shard1mongodb.conf 2.7、初始化集群配置： /data/mongodb/bin/mongo –port 30001rs.initiate( { _id : “shard”, members: [ { _id : 0, host : “10.0.7.53:30001” }, { _id : 1, host : “10.0.7.50:30001” }, { _id : 2, host : “10.0.7.51:30001”,”arbiterOnly” : true } ] })2.8、查看集群节点信息：1rs.status(); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117&#123; "set" : "shard", "date" : ISODate("2018-08-09T03:13:34.697Z"), "myState" : 2, "term" : NumberLong(1), "syncingTo" : "10.0.7.50:30001", "syncSourceHost" : "10.0.7.50:30001", "syncSourceId" : 1, "heartbeatIntervalMillis" : NumberLong(2000), "optimes" : &#123; "lastCommittedOpTime" : &#123; "ts" : Timestamp(1533784405, 1), "t" : NumberLong(1) &#125;, "readConcernMajorityOpTime" : &#123; "ts" : Timestamp(1533784405, 1), "t" : NumberLong(1) &#125;, "appliedOpTime" : &#123; "ts" : Timestamp(1533784405, 1), "t" : NumberLong(1) &#125;, "durableOpTime" : &#123; "ts" : Timestamp(1533784405, 1), "t" : NumberLong(1) &#125; &#125;, "lastStableCheckpointTimestamp" : Timestamp(1533784355, 1), "members" : [ &#123; "_id" : 0, "name" : "10.0.7.53:30001", "health" : 1, "state" : 2, "stateStr" : "SECONDARY", "uptime" : 47039, "optime" : &#123; "ts" : Timestamp(1533784405, 1), "t" : NumberLong(1) &#125;, "optimeDate" : ISODate("2018-08-09T03:13:25Z"), "syncingTo" : "10.0.7.50:30001", "syncSourceHost" : "10.0.7.50:30001", "syncSourceId" : 1, "infoMessage" : "", "configVersion" : 1, "self" : true, "lastHeartbeatMessage" : "" &#125;, &#123; "_id" : 1, "name" : "10.0.7.50:30001", "health" : 1, "state" : 1, "stateStr" : "PRIMARY", "uptime" : 46862, "optime" : &#123; "ts" : Timestamp(1533784405, 1), "t" : NumberLong(1) &#125;, "optimeDurable" : &#123; "ts" : Timestamp(1533784405, 1), "t" : NumberLong(1) &#125;, "optimeDate" : ISODate("2018-08-09T03:13:25Z"), "optimeDurableDate" : ISODate("2018-08-09T03:13:25Z"), "lastHeartbeat" : ISODate("2018-08-09T03:13:33.395Z"), "lastHeartbeatRecv" : ISODate("2018-08-09T03:13:32.735Z"), "pingMs" : NumberLong(1), "lastHeartbeatMessage" : "", "syncingTo" : "", "syncSourceHost" : "", "syncSourceId" : -1, "infoMessage" : "", "electionTime" : Timestamp(1533737562, 1), "electionDate" : ISODate("2018-08-08T14:12:42Z"), "configVersion" : 1 &#125;, &#123; "_id" : 2, "name" : "10.0.7.51:30001", "health" : 1, "state" : 7, "stateStr" : "ARBITER", "uptime" : 46862, "lastHeartbeat" : ISODate("2018-08-09T03:13:34.660Z"), "lastHeartbeatRecv" : ISODate("2018-08-09T03:13:34.658Z"), "pingMs" : NumberLong(0), "lastHeartbeatMessage" : "", "syncingTo" : "", "syncSourceHost" : "", "syncSourceId" : -1, "infoMessage" : "", "configVersion" : 1 &#125; ], "ok" : 1, "operationTime" : Timestamp(1533784405, 1), "$gleStats" : &#123; "lastOpTime" : Timestamp(0, 0), "electionId" : ObjectId("000000000000000000000000") &#125;, "lastCommittedOpTime" : Timestamp(1533784405, 1), "$configServerState" : &#123; "opTime" : &#123; "ts" : Timestamp(1533784394, 1), "t" : NumberLong(1) &#125; &#125;, "$clusterTime" : &#123; "clusterTime" : Timestamp(1533784408, 1), "signature" : &#123; "hash" : BinData(0,"dToZVSHbihPkQILJVl0qcSY+8ys="), "keyId" : NumberLong("6587344633552961565") &#125; &#125;&#125; 3、新建一个config-server集群节点，操作如下(三个节点都要执行）：3.1、节点信息如下：123从：10.0.7.53从：10.0.7.51主：10.0.7.50 3.2、拷贝一份mongodb.conf文件，并重新命名1cp shard1mongodb.conf configmongodb.conf 3.3、主节点需要在无需验证的模式下，先配置用户名密码。之后修改（三个节点）configmongodb.conf配置文件内容如下：cat configmongodb.conf123456789101112131415161718192021222324252627282930313233343536373839404142# mongod.conf# for documentation of all options, see:# http://docs.mongodb.org/manual/reference/configuration-options/# where to write logging data.systemLog: destination: file logAppend: true path: /data/mongodb/config/log/mongod.log# Where and how to store data.storage: dbPath: /data/mongodb/config/data/ journal: enabled: true# engine:# mmapv1:# wiredTiger:# how the process runsprocessManagement: fork: true # fork and run in background pidFilePath: /data/mongodb/config/log/mongod.pid # location of pidfile timeZoneInfo: /usr/share/zoneinfo# network interfacesnet: port: 30002 bindIp: 127.0.0.1,10.0.7.53 # Enter 0.0.0.0,:: to bind to all IPv4 and IPv6 addresses or, alternatively, use the net.bindIpAll setting.#security:#security:# authorization: enabled# keyFile: /data/mongodb/config/mongodb.keyfile#operationProfiling:#replication:replication: replSetName: "config"#sharding:sharding: clusterRole: configsvr## Enterprise-Only Options#auditLog:#snmp: 注意修改sharding、repl、port、path3.4、创建相应的数据目录和日志目录：1mkdir -p /data/mongodb/config/&#123;log,data&#125; 3.5、三个节点启动mongodb-shard1:1/data/mongodb/bin/mongod --configsvr -f /data/mongodb/configmongodb.conf 启动config-server服务需要指定参数，否则初始化集群的时候会报错：1"errmsg" : "Nodes being used for config servers must be started with the --configsvr flag" 3.6、初始化集群配置： /data/mongodb/bin/mongo –port 30002rs.initiate( { _id : “config”, configsvr: true, members: [ { _id : 0, host : “10.0.7.53:30002” }, { _id : 1, host : “10.0.7.50:30002” }, { _id : 2, host : “10.0.7.51:30002” } ] })3.7、查看集群信息：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117rs.status();&#123; "set" : "config", "date" : ISODate("2018-08-06T08:29:14.472Z"), "myState" : 1, "term" : NumberLong(1), "syncingTo" : "", "syncSourceHost" : "", "syncSourceId" : -1, "heartbeatIntervalMillis" : NumberLong(2000), "optimes" : &#123; "lastCommittedOpTime" : &#123; "ts" : Timestamp(1533544151, 2), "t" : NumberLong(1) &#125;, "readConcernMajorityOpTime" : &#123; "ts" : Timestamp(1533544151, 2), "t" : NumberLong(1) &#125;, "appliedOpTime" : &#123; "ts" : Timestamp(1533544151, 2), "t" : NumberLong(1) &#125;, "durableOpTime" : &#123; "ts" : Timestamp(1533544151, 2), "t" : NumberLong(1) &#125; &#125;, "lastStableCheckpointTimestamp" : Timestamp(1533544151, 1), "members" : [ &#123; "_id" : 0, "name" : "10.0.7.53:30002", "health" : 1, "state" : 2, "stateStr" : "SECONDARY", "uptime" : 15, "optime" : &#123; "ts" : Timestamp(1533544151, 2), "t" : NumberLong(1) &#125;, "optimeDurable" : &#123; "ts" : Timestamp(1533544151, 2), "t" : NumberLong(1) &#125;, "optimeDate" : ISODate("2018-08-06T08:29:11Z"), "optimeDurableDate" : ISODate("2018-08-06T08:29:11Z"), "lastHeartbeat" : ISODate("2018-08-06T08:29:14.161Z"), "lastHeartbeatRecv" : ISODate("2018-08-06T08:29:12.674Z"), "pingMs" : NumberLong(0), "lastHeartbeatMessage" : "", "syncingTo" : "10.0.7.51:30002", "syncSourceHost" : "10.0.7.51:30002", "syncSourceId" : 2, "infoMessage" : "", "configVersion" : 1 &#125;, &#123; "_id" : 1, "name" : "10.0.7.50:30002", "health" : 1, "state" : 2, "stateStr" : "SECONDARY", "uptime" : 15, "optime" : &#123; "ts" : Timestamp(1533544151, 2), "t" : NumberLong(1) &#125;, "optimeDurable" : &#123; "ts" : Timestamp(1533544151, 2), "t" : NumberLong(1) &#125;, "optimeDate" : ISODate("2018-08-06T08:29:11Z"), "optimeDurableDate" : ISODate("2018-08-06T08:29:11Z"), "lastHeartbeat" : ISODate("2018-08-06T08:29:14.164Z"), "lastHeartbeatRecv" : ISODate("2018-08-06T08:29:12.680Z"), "pingMs" : NumberLong(1), "lastHeartbeatMessage" : "", "syncingTo" : "10.0.7.51:30002", "syncSourceHost" : "10.0.7.51:30002", "syncSourceId" : 2, "infoMessage" : "", "configVersion" : 1 &#125;, &#123; "_id" : 2, "name" : "10.0.7.51:30002", "health" : 1, "state" : 1, "stateStr" : "PRIMARY", "uptime" : 59, "optime" : &#123; "ts" : Timestamp(1533544151, 2), "t" : NumberLong(1) &#125;, "optimeDate" : ISODate("2018-08-06T08:29:11Z"), "syncingTo" : "", "syncSourceHost" : "", "syncSourceId" : -1, "infoMessage" : "could not find member to sync from", "electionTime" : Timestamp(1533544150, 1), "electionDate" : ISODate("2018-08-06T08:29:10Z"), "configVersion" : 1, "self" : true, "lastHeartbeatMessage" : "" &#125; ], "ok" : 1, "operationTime" : Timestamp(1533544151, 2), "$clusterTime" : &#123; "clusterTime" : Timestamp(1533544151, 2), "signature" : &#123; "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="), "keyId" : NumberLong(0) &#125; &#125;&#125; 4、配置mongs(mongodb-router),用于客户端连接到mongodb集群，为防止单点故障，可配置多个mongos节点4.1、拷贝一份配置文件，并重命名1cp configmongodb.conf routermongodb.conf 4.2、修改配置文件参数，内容如下：cat routermongodb.conf12345678910111213141516171819202122232425262728293031323334353637383940414243444546# mongod.conf# for documentation of all options, see:# http://docs.mongodb.org/manual/reference/routeruration-options/# where to write logging data.systemLog: destination: file logAppend: true path: /data/mongodb/router/log/mongod.log# Where and how to store data.#storage:# dbPath: /data/mongodb/router/data/# journal:# enabled: true# engine:# mmapv1:# wiredTiger:# how the process runsprocessManagement: fork: true # fork and run in background pidFilePath: /data/mongodb/router/log/mongod.pid # location of pidfile timeZoneInfo: /usr/share/zoneinfo# network interfacesnet: port: 30004 bindIp: 127.0.0.1,10.0.7.53 # Enter 0.0.0.0,:: to bind to all IPv4 and IPv6 addresses or, alternatively, use the net.bindIpAll setting.#security:#security:# authorization: enabled#operationProfiling:#replication:#sharding:sharding: configDB: config/10.0.7.53:30002,10.0.7.51:30002,10.0.7.50:30002## Enterprise-Only Options#auditLog:#snmp: 注意修改sharding,port参数，注释掉storage参数4.3、创建router日志目录1mkdir -p /data/mongodb/router/log 4.4、启动mongos1/data/mongodb/bin/mongos -f /data/mongodb/routermongodb.conf 4.5、为模拟生产环境数据，对testrepl集群节点，插入部分数据,批量插入脚本如下：12345678910use testvar bulk = db.test_collection.initializeUnorderedBulkOp();people = [&quot;Marc&quot;, &quot;Bill&quot;, &quot;George&quot;, &quot;Eliot&quot;, &quot;Matt&quot;, &quot;Trey&quot;, &quot;Tracy&quot;, &quot;Greg&quot;, &quot;Steve&quot;, &quot;Kristina&quot;, &quot;Katie&quot;, &quot;Jeff&quot;];for(var i=0; i&lt;1000000; i++)&#123; user_id = i; name = people[Math.floor(Math.random()*people.length)]; number = Math.floor(Math.random()*10001); bulk.insert( &#123; &quot;user_id&quot;:user_id, &quot;name&quot;:name, &quot;number&quot;:number &#125;);&#125;bulk.execute(); 4.6、连接到mongos123shell &gt; mongo --host 10.0.7.50 --port 30004mongos&gt; use adminmongos&gt; db.auth('admin1','admin123'); 4.7、将节点添加到集群内：12mongos&gt; sh.addShard(&quot;testrepl/10.0.7.53:30000&quot;)mongos&gt; sh.addShard(&quot;shard1/10.0.7.53:30001&quot;) 4.8、使数据库test支持sharding123456mongos&gt; sh.enableSharding(&quot;test&quot;)mongos&gt; use test#在拆分键上创建索引mongos&gt; db.test_collection.createIndex( &#123; number : 1 &#125; )#拆分集合mongos&gt; sh.shardCollection(&apos;test.mycol2&apos;, &#123;&apos;_id&apos;: 1&#125;) 4.9、查看拆分结果,(可多次重复4.5步骤进行测试）sh.status()或者db.printShardingStatus()123456789101112131415161718192021222324252627282930313233343536373839404142434445--- Sharding Status --- sharding version: &#123; "_id" : 1, "minCompatibleVersion" : 5, "currentVersion" : 6, "clusterId" : ObjectId("5b6af30b5025146bfd45717b") &#125; shards: &#123; "_id" : "shard", "host" : "shard/10.0.7.50:30001,10.0.7.53:30001", "state" : 1 &#125; &#123; "_id" : "testrepl", "host" : "testrepl/10.0.7.50:30000,10.0.7.53:30000", "state" : 1 &#125; active mongoses: "4.0.0" : 1 autosplit: Currently enabled: yes balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: 4 : Success databases: &#123; "_id" : "config", "primary" : "config", "partitioned" : true &#125; config.system.sessions shard key: &#123; "_id" : 1 &#125; unique: false balancing: true chunks: shard 1 &#123; "_id" : &#123; "$minKey" : 1 &#125; &#125; --&gt;&gt; &#123; "_id" : &#123; "$maxKey" : 1 &#125; &#125; on : shard Timestamp(1, 0) &#123; "_id" : "test", "primary" : "testrepl", "partitioned" : true, "version" : &#123; "uuid" : UUID("185a3842-639f-42b0-89a1-afdc8dcdfbe7"), "lastMod" : 1 &#125; &#125; test.test_collection shard key: &#123; "number" : 1 &#125; unique: false balancing: true chunks: shard 4 testrepl 4 &#123; "number" : &#123; "$minKey" : 1 &#125; &#125; --&gt;&gt; &#123; "number" : 2394 &#125; on : shard Timestamp(2, 0) &#123; "number" : 2394 &#125; --&gt;&gt; &#123; "number" : 4791 &#125; on : shard Timestamp(3, 0) &#123; "number" : 4791 &#125; --&gt;&gt; &#123; "number" : 7194 &#125; on : shard Timestamp(4, 0) &#123; "number" : 7194 &#125; --&gt;&gt; &#123; "number" : 7892 &#125; on : shard Timestamp(5, 0) &#123; "number" : 7892 &#125; --&gt;&gt; &#123; "number" : 8591 &#125; on : testrepl Timestamp(5, 1) &#123; "number" : 8591 &#125; --&gt;&gt; &#123; "number" : 9287 &#125; on : testrepl Timestamp(3, 4) &#123; "number" : 9287 &#125; --&gt;&gt; &#123; "number" : 9589 &#125; on : testrepl Timestamp(3, 5) &#123; "number" : 9589 &#125; --&gt;&gt; &#123; "number" : &#123; "$maxKey" : 1 &#125; &#125; on : testrepl Timestamp(4, 1) 5、大功告成，客户端访问mongodb集群，只需连接mongos对应的ip、port即可]]></content>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mysql] mysql-mha主从环境，从库报错Error_code: 1062]]></title>
    <url>%2F2018%2F08%2F07%2F5.7.22-log%E7%89%88%E6%9C%ACmysql%2Cmha%E4%B8%BB%E4%BB%8E%E7%8E%AF%E5%A2%83%EF%BC%8C%E4%BB%8E%E5%BA%93%E6%8A%A5%E9%94%99%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[0、数据库版本信息：15.7.22 1、从库报错信息1234Last_SQL_Error: Could not execute Write_rows event on table test.tb1;Duplicate entry '4' for key 'PRIMARY',Error_code: 1062;handler error HA_ERR_FOUND_DUPP_KEY; the event's master log mysql-binlog.000005, end_log_pos 273273632 2、解决方法2.1、在主库上查询二进制文件信息：1/data/mysql/bin/mysqlbinlog -v --stop-position=273273632 /data/mysql/log/mysql-binlog.000005 &gt; /tmp/f.log 2.2、过滤出报错的pos所对应的行数：1cat /tmp/f.log | awk '/end_log_pos 273273632/ &#123;print NR&#125;' 122556452 2.3、根据查询到的行数，查看其上下文：1cat /tmp/f.log | awk 'NR==22556442,NR==22556492' 2.4、查询到insert的插入语句1234567### INSERT INTO `test`.`tb1`### SET### @1=4### @2='ERC20'### @3='GTO'### @4=1533176390ROLLBACK /* added by mysqlbinlog */ /*!*/; 2.5、在从库上执行下面操作，停止主从同步1stop slave; 2.6、删除报错提示的对应的行：123use test;delete from tb1 where id=4;select * from tb1; 2.7、重新启动主从同步，查看节点信息正常：12start slave;show slave status\G;]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mongodb] mongodb集群配置一主一从一仲裁]]></title>
    <url>%2F2018%2F08%2F05%2Fmongodb-%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[1、首先安装在三个节点安装mongodb,安装方法详见 mongodb单点安装 2、集群节点信息：123主：10.0.7.53从：10.0.7.51仲裁：10.0.7.50 3、启动主节点，进入主节点控制台，3.1、创建管理员用户名、密码：12345678910shell &gt; /data/mongodb/bin/mongo --port 30000MongoDB Enterprise testrepl:PRIMARY&gt;use adminMongoDB Enterprise testrepl:PRIMARY&gt;db.createUser( &#123; user: "admin", pwd: "abc123", roles: [ &#123; role: "userAdminAnyDatabase", db: "admin" &#125; ] &#125;) 使用userAdminAnyDatabase的role权限去查看rs.status();会提示“not authorized on admin to execute command”,建议授予root权限：1234567db.createUser( &#123; user: "admin1", pwd: "admin123", roles: [ &#123; role: "root", db: "admin" &#125; ] &#125;) 3.2、查看创建的管理员信息：1MongoDB Enterprise testrepl:PRIMARY&gt; db.system.users.find() 3.3、退出控制台，修改配置文件mongodb.conf(每个节点都要配置)：12security: authorization: enabled 4、在主节点上，生成秘钥文件4.1、生成密钥文件，用于集群之间互相通信：1openssl rand -base64 756 &gt; /data/mongodb/mongodb.keyfile 4.2、更改文件权限以仅为文件所有者提供读取权限1chmod 400 /data/mongodb/mongodb.keyfile 4.3、将密钥文件复制到每个副本集成员12scp /data/mongodb/mongodb.keyfile root@10.0.7.51:/data/mongodb/scp /data/mongodb/mongodb.keyfile root@10.0.7.50:/data/mongodb/ 4.4、修改配置文件mongodb.conf，添加keyfile参数(每个节点都要配置)：12security: keyFile: /data/mongodb/mongodb.keyfile 5、配置集群5.1、修改配置文件mongodb.conf，添加集群配置参数(每个节点都要配置)：12replication: replSetName: "testrepl" replSetName:设置集群名称，根据自己需求自定义（三个节点要保持一直） 5.2、重新启动mongodb1/data/mongodb/bin/mongod -f /data/mongodb/mongodb.conf 5.3、进入主节点控制台1/data/mongodb/bin/mongo --port 30000 5.4、添加集群节点：1234567891011use admin;db.auth('admin','abc123');rs.initiate( &#123; _id : "testrepl", members: [ &#123; _id: 0, host: "10.0.7.53:30000" &#125;, &#123; _id: 1, host: "10.0.7.50:30000" &#125;, &#123; _id: 2, host: "10.0.7.51:30000","arbiterOnly" : true&#125; ]&#125;) 执行结果如下：1234567891011&#123; "ok" : 1, "operationTime" : Timestamp(1533388128, 1), "$clusterTime" : &#123; "clusterTime" : Timestamp(1533388128, 1), "signature" : &#123; "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="), "keyId" : NumberLong(0) &#125; &#125;&#125; 5.5、查看集群状态123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107MongoDB Enterprise testrepl:PRIMARY&gt; rs.status();&#123; "set" : "testrepl", "date" : ISODate("2018-08-04T13:09:25.894Z"), "myState" : 1, "term" : NumberLong(1), "syncingTo" : "", "syncSourceHost" : "", "syncSourceId" : -1, "heartbeatIntervalMillis" : NumberLong(2000), "optimes" : &#123; "lastCommittedOpTime" : &#123; "ts" : Timestamp(1533388160, 1), "t" : NumberLong(1) &#125;, "readConcernMajorityOpTime" : &#123; "ts" : Timestamp(1533388160, 1), "t" : NumberLong(1) &#125;, "appliedOpTime" : &#123; "ts" : Timestamp(1533388160, 1), "t" : NumberLong(1) &#125;, "durableOpTime" : &#123; "ts" : Timestamp(1533388160, 1), "t" : NumberLong(1) &#125; &#125;, "lastStableCheckpointTimestamp" : Timestamp(1533388140, 1), "members" : [ &#123; "_id" : 0, "name" : "10.0.7.53:30000", "health" : 1, "state" : 1, "stateStr" : "PRIMARY", "uptime" : 289, "optime" : &#123; "ts" : Timestamp(1533388160, 1), "t" : NumberLong(1) &#125;, "optimeDate" : ISODate("2018-08-04T13:09:20Z"), "syncingTo" : "", "syncSourceHost" : "", "syncSourceId" : -1, "infoMessage" : "could not find member to sync from", "electionTime" : Timestamp(1533388139, 1), "electionDate" : ISODate("2018-08-04T13:08:59Z"), "configVersion" : 1, "self" : true, "lastHeartbeatMessage" : "" &#125;, &#123; "_id" : 1, "name" : "10.0.7.50:30000", "health" : 1, "state" : 2, "stateStr" : "SECONDARY", "uptime" : 37, "optime" : &#123; "ts" : Timestamp(1533388160, 1), "t" : NumberLong(1) &#125;, "optimeDurable" : &#123; "ts" : Timestamp(1533388160, 1), "t" : NumberLong(1) &#125;, "optimeDate" : ISODate("2018-08-04T13:09:20Z"), "optimeDurableDate" : ISODate("2018-08-04T13:09:20Z"), "lastHeartbeat" : ISODate("2018-08-04T13:09:25.051Z"), "lastHeartbeatRecv" : ISODate("2018-08-04T13:09:25.677Z"), "pingMs" : NumberLong(3), "lastHeartbeatMessage" : "", "syncingTo" : "10.0.7.53:30000", "syncSourceHost" : "10.0.7.53:30000", "syncSourceId" : 0, "infoMessage" : "", "configVersion" : 1 &#125;, &#123; "_id" : 2, "name" : "10.0.7.51:30000", "health" : 1, "state" : 7, "stateStr" : "ARBITER", "uptime" : 37, "lastHeartbeat" : ISODate("2018-08-04T13:09:25.031Z"), "lastHeartbeatRecv" : ISODate("2018-08-04T13:09:24.265Z"), "pingMs" : NumberLong(0), "lastHeartbeatMessage" : "", "syncingTo" : "", "syncSourceHost" : "", "syncSourceId" : -1, "infoMessage" : "", "configVersion" : 1 &#125; ], "ok" : 1, "operationTime" : Timestamp(1533388160, 1), "$clusterTime" : &#123; "clusterTime" : Timestamp(1533388160, 1), "signature" : &#123; "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="), "keyId" : NumberLong(0) &#125; &#125;&#125; 5.6、从库默认为不能读写模式，如果要启动从库的读模式，进入从库执行以下命令：1rs.slaveOk(); 5.7、增加一个仲裁节点，新增节点，配置文件中密钥文件、repl参数一定要保持一致：1rs.addArb("m1.example.net:27017") 5.8、添加一个新的从节点，添加之前设置该节点优先级为0，表决为0，以防止该节点数据未同步完成之前参与选举，待该节点数据同步完成之后，使用rs.reconfig()参数再修改其优先级和表决：1rs.add( &#123; host: "mongodb3.example.net:27017", priority: 0, votes: 0 &#125; ) rs.reconfig()命令执行如下：1234var cfg = rs.conf();cfg.members[4].priority = 1cfg.members[4].votes = 1rs.reconfig(cfg)]]></content>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Mongodb] mongdob-enterprise-4.0安装]]></title>
    <url>%2F2018%2F08%2F03%2Fmongdob4.0%2F</url>
    <content type="text"><![CDATA[1、Yum方式安装 1.1配置yum源 cat /etc/yum.repos.d/mongodb-enterprise.repo123456[mongodb-enterprise]name=MongoDB Enterprise Repositorybaseurl=https://repo.mongodb.com/yum/redhat/$releasever/mongodb-enterprise/4.0/$basearch/gpgcheck=1enabled=1gpgkey=https://www.mongodb.org/static/pgp/server-4.0.asc 1.2执行安装命令1sudo yum install -y mongodb-enterprise 如果想要指定安装某一个版本的mongodb,需要指定每一个组件的安装版本,如下所示：1sudo yum install -y mongodb-enterprise-4.0.0 mongodb-enterprise-server-4.0.0 mongodb-enterprise-shell-4.0.0 mongodb-enterprise-mongos-4.0.0 mongodb-enterprise-tools-4.0.0 1.3mongodb启动、关闭命令：123sudo service mongod startsudo service mongod stopsudo service mongod restart 1.4通过yum方式安装完之后，配置文件所在为/etc/mongod.conf 2、tar包方式安装 2.1安装之前需要，安装依赖包： linux-6:1yum install -y cyrus-sasl cyrus-sasl-plain cyrus-sasl-gssapi krb5-libs libcurl libpcap net-snmp openldap openssl linux-71yum install -y cyrus-sasl cyrus-sasl-gssapi cyrus-sasl-plain krb5-libs libcurl libpcap lm_sensors-libs net-snmp net-snmp-agent-libs openldap openssl rpm-libs tcp_wrappers-libs 2.2下载mongodb-enterprise软件包1wget https://downloads.mongodb.com/linux/mongodb-linux-x86_64-enterprise-rhel70-4.0.0.tgz 2.3解压软件包，修改目录名称，配置环境变量1234 tar -zxvf mongodb-linux-x86_64-enterprise-rhel70-4.0.0.tgz -C /data/ mv /data/mongodb-linux-x86_64-enterprise-rhel70-4.0.0 /data/mongodbecho "export PATH=/data/mongodb/bin:\$PATH" &gt;&gt; ~/.bash_profilesource ~/.bash_profile 2.4修改配置文件 cat /data/mongodb/mongodb.conf1234567891011121314151617181920212223242526272829303132333435# mongod.conf# for documentation of all options, see:# http://docs.mongodb.org/manual/reference/configuration-options/# where to write logging data.systemLog: destination: file logAppend: true path: /data/mongodb/log/mongod.log# Where and how to store data.storage: dbPath: /data/mongodb/data/ journal: enabled: true# engine:# mmapv1:# wiredTiger:# how the process runsprocessManagement: fork: true # fork and run in background pidFilePath: /data/mongodb/log/mongod.pid # location of pidfile timeZoneInfo: /usr/share/zoneinfo# network interfacesnet: port: 30000 bindIp: 127.0.0.1,10.0.7.51 # Enter 0.0.0.0,:: to bind to all IPv4 and IPv6 addresses or, alternatively, use the net.bindIpAll setting.#security:#operationProfiling:#replication:#sharding:## Enterprise-Only Options#auditLog:#snmp: 2.5修改的配置文件中，如果数据目录，日志目录不存在会报错，需要手动配置：1mkdir -p /data/mongodb/&#123;log,data&#125; 2.6启动mongodb123456shell&gt;/data/mongodb/bin/mongod -f /data/mongodb/mongodb.conf -------- 2018-08-03T08:42:19.873+0000 I CONTROL [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'about to fork child process, waiting until server is ready for connections.forked process: 41022child process started successfully, parent exiting 2.7查看mongodb进程：1234 netstat -tunlp|grep 30000---------tcp 0 0 10.0.7.51:30000 0.0.0.0:* LISTEN 41022/mongod tcp 0 0 127.0.0.1:30000 0.0.0.0:* LISTEN 41022/mongod]]></content>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Zabbix] zabbix3.4监控nginx性能]]></title>
    <url>%2F2018%2F08%2F03%2Fzabbix-nginx%2F</url>
    <content type="text"><![CDATA[1、查看nginx安装HTTP Stub Status1openresty -V 如果”–with-http_stub_status_module”参数，表示没有安装，要手动添加：12在编译nginx 的时候要加上参数 –with-http_stub_status_module，执行./configure &amp;&amp; make就可以了，不用make install。不过，一般情况下都是安装了的。 2、nginx服务器添加配置 2.1添加配置文件nginx_status.confcat /usr/local/openresty/nginx/conf/conf.d/nginx_status.conf12345678server &#123; listen 80; server_name 127.0.0.1; location /basic_status &#123; stub_status; &#125;&#125; 2.2重新加载配置文件1openresty -s reload 2.3获取nginx监控状态123456curl http://127.0.0.1/basic_status-------------Active connections: 1 server accepts handled requests 572 572 764 Reading: 0 Writing: 1 Waiting: 0 nginx监控参数详解123456## Active connections: 对后端发起的活动连接数## Server accepts handled requests: Nginx 总共处理了 572 个连接，成功创建了 572 次握手（没有失败次数），总共处理了 764 个请求## Reading: Nginx 读取到客户端的 Header 信息数## Writing: Nginx 返回给客户端的 Header 信息数## Waiting: 开启 keep-alive 的情况下，这个值等于 active - ( reading + writing ), 意思是 Nginx 已经处理完成，正在等待下一次请求指令的驻留连接## 在访问效率很高，请求很快被处理完毕的情况下，Waiting 数比较多是正常的。如果 reading + writing 数较多，则说明并发访问量很大，正在处理过程中 3、配置zabbix创建存放脚本目录，（我的zabbix安装路径为 /usr/local/zabbix）：12mkdir /usr/local/zabbix/scriptcd /usr/local/zabbix/script 创建存放数据文件（可自行定义）：1touch .status.txt 编辑脚本用于获取nginx监控状态参数：12345678910111213141516171819202122232425echo "#!/bin/bashserver_hostname=127.0.0.1data_file=/usr/local/zabbix/script/.status.txtstatus_data=\`curl -o \$data_file -s http://\$server_hostname/basic_status\`function Active() &#123; awk -F \"[: ]\" '/Active/&#123;print \$4&#125;' \$data_file&#125; function Reading() &#123; awk -F \"[: ]\" '/Reading/ &#123;print \$3&#125;' \$data_file&#125;function Writing() &#123; awk -F \"[: ]\" '/Writing/ &#123;print \$6&#125;' \$data_file&#125;function Waiting() &#123; awk -F \"[: ]\" '/Waiting/ &#123;print \$9&#125;' \$data_file&#125;\$1" &gt; /usr/local/zabbix/script/nginx_connection.sh 修改目录下所有文件的权限，否则脚本写入文件会有权限问题1chown -R zabbix.zabbix /usr/local/zabbix/script 添加zabbix参数到agent配置文件1234echo \"UserParameter=nginx.Active,sh /usr/local/zabbix/script/nginx_connection.sh ActiveUserParameter=nginx.Writing,sh /usr/local/zabbix/script/nginx_connection.sh WritingUserParameter=nginx.Reading,sh /usr/local/zabbix/script/nginx_connection.sh ReadingUserParameter=nginx.Waiting,sh /usr/local/zabbix/script/nginx_connection.sh Waiting\" &gt;&gt; /usr/local/zabbix/etc/zabbix_agentd.conf 重新启动agent客户端：1/etc/init.d/zabbix_agentd restart 在server端测试是否能获取到参数：1/usr/local/zabbix/bin/zabbix_get -s 10.1.130.47 -k nginx.Active]]></content>
      <tags>
        <tag>linux</tag>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Redis] redis-cluster集群部署]]></title>
    <url>%2F2018%2F08%2F02%2Fredis-cluster%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[0、Redis 集群功能限制123456Redis集群相对单机在功能上有一定限制。key批量操作支持有限。如：MSET``MGET，目前只支持具有相同slot值的key执行批量操作。key事务操作支持有限。支持多key在同一节点上的事务操作，不支持分布在多个节点的事务功能。key作为数据分区的最小粒度，因此不能将一个大的键值对象映射到不同的节点。如：hash、list。不支持多数据库空间。单机下Redis支持16个数据库，集群模式下只能使用一个数据库空间，即db 0。复制结构只支持一层，不支持嵌套树状复制结构。 1、服务器三台，在每台服务器上部署两个redis服务器，节点信息如下：123主1：10.0.7.53-6379, 10.0.7.53-6380主2：10.0.7.50-6379, 10.0.7.50-6380主3：10.0.7.51-6379, 10.0.7.51-6380 2、修改redis.conf配置文件cat /data/redis/etc/redis6379.conf(不同服务器，修改bind对应的ip即可)123456789101112pidfile /data/redis6379/log/redis.pidbind 10.0.7.53port 6379daemonize yeslogfile /data/redis6379/log/redis.logdir /data/redis6379/databases 16maxmemory 1gcluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000appendonly yes cat /data/redis/etc/redis6380.conf(不同服务器，修改bind对应的ip即可)123456789101112pidfile /data/redis6380/log/redis.pidbind 10.0.7.53port 6380daemonize yeslogfile /data/redis6380/log/redis.logdir /data/redis6380/databases 16maxmemory 1gcluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000appendonly yes 创建指定的文件目录(指定目录不存在，启动会报错)12mkdir -p /data/redis6379/log/mkdir -p /data/redis6380/log/ 3、启动redis服务3.1启动服务之前，修改内核参数：12345678910echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled添加到开机自动启动/etc/rc.localecho 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' &gt;&gt; /etc/rc.localchmod +x /etc/rc.d/rc.localecho "vm.overcommit_memory = 1" &gt;&gt; /etc/sysctl.conf使修改环境变量生效sysctl vm.overcommit_memory=1echo 511 &gt; /proc/sys/net/core/somaxconn 3.2在三个服务器上，分别启动redis服务12/data/redis/src/redis-server /data/redis/etc/redis6379.conf/data/redis/src/redis-server /data/redis/etc/redis6380.conf 4、启动成功之后，在指定的dir目录下回生成一个nodes.conf的文件123cat /data/redis6379/nodes.conf4933ef69f73b390098a4431449aeef2e83dacfc0 :0@0 myself,master - 0 0 0 connectedvars currentEpoch 0 lastVoteEpoch 0 该文件记录了节点信息id，不随主机名和端口的改变而改变。 5、开始配置集群5.1配置集群需要安装ruby12345678910yum install -y ruby安装完成之后运行：gem install redis提示报错：----------Fetching: redis-4.0.1.gem (100%)ERROR: Error installing redis: redis requires Ruby version &gt;= 2.2.2.------------默认yum安装ruby版本为2.0.0，但是安装redis-trib.rb运行的环境，最少需要ruby-2.2.2版本 5.2需要先安装rvm，升级ruby版本1234567891011121314151617181920212223242526安装curl：sudo yum install curl安装RVMcurl -L get.rvm.io | bash -s stable安装如果提示没有public key,按提示在命令行输入信息：shell&gt; gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3-----gpg: Can't check signature: No public keyWarning, RVM 1.26.0 introduces signed releases and automated check of signatures when GPG software found. Assuming you trust Michal Papis import the mpapis public key (downloading the signatures).GPG signature verification failed for '/usr/local/rvm/archives/rvm-1.29.4.tgz' - 'https://github.com/rvm/rvm/releases/download/1.29.4/1.29.4.tar.gz.asc'! Try to install GPG v2 and then fetch the public key: gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3------------------------------使rvm环境变量生效source /usr/local/rvm/scripts/rvm查看rvm库中已知的ruby版本rvm list known安装一个ruby版本rvm install 2.5.1使用一个ruby版本rvm use 2.5.1卸载已安装的旧版本rvm remove 2.0.0查看版本ruby --version 5.3再安装redis就可以了1gem install redis 5.4安装完成之后，执行以下命令创建一个新的集群 1/data/redis/src/redis-trib.rb create --replicas 1 10.0.7.53:6379 10.0.7.53:6380 10.0.7.50:6379 10.0.7.50:6380 10.0.7.51:6379 10.0.7.51:6380 –replicas 1：该参数代表为每一个主节点增加一个从节点 123&gt;&gt;&gt; Creating cluster[ERR] Sorry, can&apos;t connect to node 10.0.7.53:6379创建集群报错，确认gem版本没有问题，查看配置文件是否存在requirpass等参数 重新执行创建集群1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&gt;&gt;&gt; Creating cluster&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:10.0.7.53:637910.0.7.50:637910.0.7.51:6379Adding replica 10.0.7.50:6380 to 10.0.7.53:6379Adding replica 10.0.7.51:6380 to 10.0.7.50:6379Adding replica 10.0.7.53:6380 to 10.0.7.51:6379M: 4933ef69f73b390098a4431449aeef2e83dacfc0 10.0.7.53:6379 slots:0-5460 (5461 slots) masterS: 3f757295e082a5fbe237015862d0a502779f3adf 10.0.7.53:6380 replicates 3e31b1ceded27ad5b4114beaebb559d7f5ca465dM: 27eb4895dc9369adcd81d9a00709dfbf6fbbd0d4 10.0.7.50:6379 slots:5461-10922 (5462 slots) masterS: 5b3aa1532d911e1a9b966ec53e3203a3221056e3 10.0.7.50:6380 replicates 4933ef69f73b390098a4431449aeef2e83dacfc0M: 3e31b1ceded27ad5b4114beaebb559d7f5ca465d 10.0.7.51:6379 slots:10923-16383 (5461 slots) masterS: 30aabebd554d34e59584774feebd472f622f1cc2 10.0.7.51:6380 replicates 27eb4895dc9369adcd81d9a00709dfbf6fbbd0d4Can I set the above configuration? (type 'yes' to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 10.0.7.53:6379)M: 4933ef69f73b390098a4431449aeef2e83dacfc0 10.0.7.53:6379 slots:0-5460 (5461 slots) master 1 additional replica(s)M: 27eb4895dc9369adcd81d9a00709dfbf6fbbd0d4 10.0.7.50:6379 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: 3e31b1ceded27ad5b4114beaebb559d7f5ca465d 10.0.7.51:6379 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 30aabebd554d34e59584774feebd472f622f1cc2 10.0.7.51:6380 slots: (0 slots) slave replicates 27eb4895dc9369adcd81d9a00709dfbf6fbbd0d4S: 3f757295e082a5fbe237015862d0a502779f3adf 10.0.7.53:6380 slots: (0 slots) slave replicates 3e31b1ceded27ad5b4114beaebb559d7f5ca465dS: 5b3aa1532d911e1a9b966ec53e3203a3221056e3 10.0.7.50:6380 slots: (0 slots) slave replicates 4933ef69f73b390098a4431449aeef2e83dacfc0[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.---------------------------------- 5.5集群安装完成，查看集群状态12345678910111213141516171819202122/data/redis/src/redis-trib.rb check 10.0.7.53:6379（或者 进入控制台/data/redis/src/redis-cli -h 10.0.7.53，输入 CLUSTER nodes)&gt;&gt;&gt; Performing Cluster Check (using node 10.0.7.53:6379)M: 4933ef69f73b390098a4431449aeef2e83dacfc0 10.0.7.53:6379 slots:0-5460 (5461 slots) master 1 additional replica(s)M: 27eb4895dc9369adcd81d9a00709dfbf6fbbd0d4 10.0.7.50:6379 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: 3e31b1ceded27ad5b4114beaebb559d7f5ca465d 10.0.7.51:6379 slots:10923-16383 (5461 slots) master 1 additional replica(s)S: 30aabebd554d34e59584774feebd472f622f1cc2 10.0.7.51:6380 slots: (0 slots) slave replicates 27eb4895dc9369adcd81d9a00709dfbf6fbbd0d4S: 3f757295e082a5fbe237015862d0a502779f3adf 10.0.7.53:6380 slots: (0 slots) slave replicates 3e31b1ceded27ad5b4114beaebb559d7f5ca465dS: 5b3aa1532d911e1a9b966ec53e3203a3221056e3 10.0.7.50:6380 slots: (0 slots) slave replicates 4933ef69f73b390098a4431449aeef2e83dacfc0[OK] All nodes agree about slots configuration. 6、集群安装完成，测试集群是否正常运行6.1杀掉任意一个主节点，从节点自动切换为主，1234567891011121314151617/data/redis/src/redis-trib.rb check 10.0.7.53:6380&gt;&gt;&gt; Performing Cluster Check (using node 10.0.7.53:6380)S: 3f757295e082a5fbe237015862d0a502779f3adf 10.0.7.53:6380 slots: (0 slots) slave replicates 3e31b1ceded27ad5b4114beaebb559d7f5ca465dM: 27eb4895dc9369adcd81d9a00709dfbf6fbbd0d4 10.0.7.50:6379 slots:5461-10922 (5462 slots) master 1 additional replica(s)M: 5b3aa1532d911e1a9b966ec53e3203a3221056e3 10.0.7.50:6380 slots:0-5460 (5461 slots) master 0 additional replica(s)S: 30aabebd554d34e59584774feebd472f622f1cc2 10.0.7.51:6380 slots: (0 slots) slave replicates 27eb4895dc9369adcd81d9a00709dfbf6fbbd0d4M: 3e31b1ceded27ad5b4114beaebb559d7f5ca465d 10.0.7.51:6379 slots:10923-16383 (5461 slots) master 1 additional replica(s) 6.2重新启动别杀掉的节点之后，该节点自动变为从：1234567891011121314151617181920/data/redis/src/redis-trib.rb check 10.0.7.53:6380&gt;&gt;&gt; Performing Cluster Check (using node 10.0.7.53:6380)S: 3f757295e082a5fbe237015862d0a502779f3adf 10.0.7.53:6380 slots: (0 slots) slave replicates 3e31b1ceded27ad5b4114beaebb559d7f5ca465dM: 27eb4895dc9369adcd81d9a00709dfbf6fbbd0d4 10.0.7.50:6379 slots:5461-10922 (5462 slots) master 1 additional replica(s)S: 4933ef69f73b390098a4431449aeef2e83dacfc0 10.0.7.53:6379 slots: (0 slots) slave replicates 5b3aa1532d911e1a9b966ec53e3203a3221056e3M: 5b3aa1532d911e1a9b966ec53e3203a3221056e3 10.0.7.50:6380 slots:0-5460 (5461 slots) master 1 additional replica(s)S: 30aabebd554d34e59584774feebd472f622f1cc2 10.0.7.51:6380 slots: (0 slots) slave replicates 27eb4895dc9369adcd81d9a00709dfbf6fbbd0d4M: 3e31b1ceded27ad5b4114beaebb559d7f5ca465d 10.0.7.51:6379 slots:10923-16383 (5461 slots) master 1 additional replica(s)]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Linux] centos7.4--yum install gcc报错]]></title>
    <url>%2F2018%2F08%2F02%2Fcentos7.4yum%E5%AE%89%E8%A3%85gcc%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[[yum install gcc执行报错如下：]12345678910111213141516171819202122232425262728293031323334353637383940414243Loaded plugins: fastestmirror, langpacksLoading mirror speeds from cached hostfileResolving Dependencies--&gt; Running transaction check---&gt; Package gcc.x86_64 0:4.8.5-28.el7_5.1 will be installed--&gt; Processing Dependency: libgomp = 4.8.5-28.el7_5.1 for package: gcc-4.8.5-28.el7_5.1.x86_64--&gt; Processing Dependency: cpp = 4.8.5-28.el7_5.1 for package: gcc-4.8.5-28.el7_5.1.x86_64--&gt; Processing Dependency: glibc-devel &gt;= 2.2.90-12 for package: gcc-4.8.5-28.el7_5.1.x86_64--&gt; Processing Dependency: libmpfr.so.4()(64bit) for package: gcc-4.8.5-28.el7_5.1.x86_64--&gt; Processing Dependency: libmpc.so.3()(64bit) for package: gcc-4.8.5-28.el7_5.1.x86_64--&gt; Running transaction check---&gt; Package cpp.x86_64 0:4.8.5-28.el7_5.1 will be installed---&gt; Package glibc-devel.x86_64 0:2.17-222.el7 will be installed--&gt; Processing Dependency: glibc-headers = 2.17-222.el7 for package: glibc-devel-2.17-222.el7.x86_64--&gt; Processing Dependency: glibc = 2.17-222.el7 for package: glibc-devel-2.17-222.el7.x86_64--&gt; Processing Dependency: glibc-headers for package: glibc-devel-2.17-222.el7.x86_64---&gt; Package libgomp.x86_64 0:4.8.5-11.el7 will be updated---&gt; Package libgomp.x86_64 0:4.8.5-28.el7_5.1 will be an update---&gt; Package libmpc.x86_64 0:1.0.1-3.el7 will be installed---&gt; Package mpfr.x86_64 0:3.1.1-4.el7 will be installed--&gt; Running transaction check---&gt; Package glibc.x86_64 0:2.17-157.el7_3.5 will be updated--&gt; Processing Dependency: glibc = 2.17-157.el7_3.5 for package: glibc-common-2.17-157.el7_3.5.x86_64---&gt; Package glibc.x86_64 0:2.17-222.el7 will be an update---&gt; Package glibc-headers.x86_64 0:2.17-222.el7 will be installed--&gt; Processing Dependency: kernel-headers &gt;= 2.2.1 for package: glibc-headers-2.17-222.el7.x86_64--&gt; Processing Dependency: kernel-headers for package: glibc-headers-2.17-222.el7.x86_64--&gt; Running transaction check---&gt; Package glibc.x86_64 0:2.17-157.el7_3.5 will be updated--&gt; Processing Dependency: glibc = 2.17-157.el7_3.5 for package: glibc-common-2.17-157.el7_3.5.x86_64---&gt; Package kernel-headers.x86_64 0:3.10.0-862.9.1.el7 will be installed--&gt; Finished Dependency ResolutionError: Package: glibc-common-2.17-157.el7_3.5.x86_64 (@CentOS-Updates) Requires: glibc = 2.17-157.el7_3.5 Removing: glibc-2.17-157.el7_3.5.x86_64 (@CentOS-Updates) glibc = 2.17-157.el7_3.5 Updated By: glibc-2.17-222.el7.x86_64 (base) glibc = 2.17-222.el7 You could try using --skip-broken to work around the problem** Found 3 pre-existing rpmdb problem(s), 'yum check' output follows:glibc-common-2.17-222.el7.x86_64 is a duplicate with glibc-common-2.17-157.el7_3.5.x86_64glibc-common-2.17-222.el7.x86_64 has missing requires of glibc = ('0', '2.17', '222.el7')libgcc-4.8.5-28.el7_5.1.x86_64 is a duplicate with libgcc-4.8.5-11.el7.x86_64 产生该问题的主要原因是：在系统upgrade的时候，残存了上一个版本的软件包 [解决办法：]首先安装 yum-utils 套件1yum install yum-utils 执行clean duplicate package 1package-cleanup --cleandupes 重新安装glibc1yum reinstall glibc glibc-common libgcc 安装完成之后再安装gcc1yum install gcc]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Redis] redis-sentinel集群配置（一主两从三哨兵）]]></title>
    <url>%2F2018%2F08%2F01%2Fredis-sentinel%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE(%E4%B8%80%E4%B8%BB%E4%B8%A4%E4%BB%8E%E4%B8%89%E5%93%A8%E5%85%B5)%2F</url>
    <content type="text"><![CDATA[1、所有节点安装redis节点信息：主1：10.0.7.53从2：10.0.7.50从3：10.0.7.51除了在三个节点部署redis服务，再分别部署sentinel。 2、配置主从，在主节点配置文件添加如下配置：主节点1配置：123456789101112131415161718pidfile /data/redis/log/redis.pidbind 10.0.7.53port 6379daemonize yeslogfile "/data/redis/log/redis.log"databases 16maxmemory 1grequirepass 123456masterauth 123456slave-serve-stale-data yesslave-read-only yesrepl-diskless-sync norepl-diskless-sync-delay 5repl-ping-slave-period 10repl-timeout 60repl-disable-tcp-nodelay norepl-backlog-size 5mbrepl-backlog-ttl 3600 从节点2配置：12345678910111213141516171819pidfile /data/redis/log/redis.pidbind 10.0.7.50port 6379daemonize yeslogfile "/data/redis/log/redis.log"databases 16maxmemory 1gslaveof 10.0.7.53 6379masterauth 123456requirepass 123456slave-serve-stale-data yesslave-read-only yesrepl-diskless-sync norepl-diskless-sync-delay 5repl-ping-slave-period 10repl-timeout 60repl-disable-tcp-nodelay norepl-backlog-size 5mbrepl-backlog-ttl 3600 从节点3配置：12345678910111213141516171819pidfile /data/redis/log/redis.pidbind 10.0.7.50port 6379daemonize yeslogfile "/data/redis/log/redis.log"databases 16maxmemory 1gslaveof 10.0.7.53 6379masterauth 123456requirepass 123456slave-serve-stale-data yesslave-read-only yesrepl-diskless-sync norepl-diskless-sync-delay 5repl-ping-slave-period 10repl-timeout 60repl-disable-tcp-nodelay norepl-backlog-size 5mbrepl-backlog-ttl 3600 上面配置相关参数详解：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#复制选项，slave复制对应的master。slaveof &lt;masterip&gt; &lt;masterport&gt;#如果master设置了requirepass，那么slave要连上master，需要有master的密码才行。masterauth就是用来配置master的密码，这样可以在连上master后进行认证。masterauth &lt;master-password&gt; ------------------------------------- 如果主库配置了密码，从库没有添加这行数据，会报一下错误： # Unexpected reply to PSYNC from master: -NOAUTH Authentication required. * Retrying with SYNC... # MASTER aborted replication with an error: NOAUTH Authentication required. -------------------------------------#当从库同主机失去连接或者复制正在进行，从机库有两种运行方式：1) 如果slave-serve-stale-data设置为yes(默认设置)，从库会继续响应客户端的请求。2) 如果slave-serve-stale-data设置为no，除去INFO和SLAVOF命令之外的任何请求都会返回一个错误”SYNC with master in progress”。slave-serve-stale-data yes#作为从服务器，默认情况下是只读的（yes），可以修改成NO，用于写（不建议）。slave-read-only yes#是否使用socket方式复制数据。目前redis复制提供两种方式，disk和socket。如果新的slave连上来或者重连的slave无法部分同步，就会执行全量同步，master会生成rdb文件。有2种方式：disk方式是master创建一个新的进程把rdb文件保存到磁盘，再把磁盘上的rdb文件传递给slave。socket是master创建一个新的进程，直接把rdb文件以socket的方式发给slave。disk方式的时候，当一个rdb保存的过程中，多个slave都能共享这个rdb文件。socket的方式就的一个个slave顺序复制。在磁盘速度缓慢，网速快的情况下推荐用socket方式。repl-diskless-sync no#diskless复制的延迟时间，防止设置为0。一旦复制开始，节点不会再接收新slave的复制请求直到下一个rdb传输。所以最好等待一段时间，等更多的slave连上来。repl-diskless-sync-delay 5#slave根据指定的时间间隔向服务器发送ping请求。时间间隔可以通过 repl_ping_slave_period 来设置，默认10秒。# repl-ping-slave-period 10#复制连接超时时间。master和slave都有超时时间的设置。master检测到slave上次发送的时间超过repl-timeout，即认为slave离线，清除该slave信息。slave检测到上次和master交互的时间超过repl-timeout，则认为master离线。需要注意的是repl-timeout需要设置一个比repl-ping-slave-period更大的值，不然会经常检测到超时。# repl-timeout 60#是否禁止复制tcp链接的tcp nodelay参数，可传递yes或者no。默认是no，即使用tcp nodelay。如果master设置了yes来禁止tcp nodelay设置，在把数据复制给slave的时候，会减少包的数量和更小的网络带宽。但是这也可能带来数据的延迟。默认我们推荐更小的延迟，但是在数据量传输很大的场景下，建议选择yes。repl-disable-tcp-nodelay no#复制缓冲区大小，这是一个环形复制缓冲区，用来保存最新复制的命令。这样在slave离线的时候，不需要完全复制master的数据，如果可以执行部分同步，只需要把缓冲区的部分数据复制给slave，就能恢复正常复制状态。缓冲区的大小越大，slave离线的时间可以更长，复制缓冲区只有在有slave连接的时候才分配内存。没有slave的一段时间，内存会被释放出来，默认1m。# repl-backlog-size 5mb#master没有slave一段时间会释放复制缓冲区的内存，repl-backlog-ttl用来设置该时间长度。单位为秒。# repl-backlog-ttl 3600#当master不可用，Sentinel会根据slave的优先级选举一个master。最低的优先级的slave，当选master。而配置成0，永远不会被选举。slave-priority 100#redis提供了可以让master停止写入的方式，如果配置了min-slaves-to-write，健康的slave的个数小于N，mater就禁止写入。master最少得有多少个健康的slave存活才能执行写命令。这个配置虽然不能保证N个slave都一定能接收到master的写操作，但是能避免没有足够健康的slave的时候，master不能写入来避免数据丢失。设置为0是关闭该功能。# min-slaves-to-write 3#延迟小于min-slaves-max-lag秒的slave才认为是健康的slave。# min-slaves-max-lag 10 3、修改哨兵配置文件cat sentinel.conf123456789101112131415port 26379dir "/tmp/sentinel"logfile "/tmp/sentinel.log"daemonize yessentinel monitor mymaster 10.0.7.53 6379 2sentinel down-after-milliseconds mymaster 5000sentinel parallel-syncs mymaster 1sentinel auth-pass mymaster 123456sentinel failover-timeout mymaster 180000sentinel config-epoch mymaster 2sentinel leader-epoch mymaster 2sentinel known-slave mymaster 10.0.7.51 6379sentinel known-slave mymaster 10.0.7.50 6379sentinel current-epoch 2protected-mode no 指定的dir路径若不存在需要手动创建：1mkdir -p /tmp/sentinel sentinel相关配置参数详解：12345678910111213#sentinel monitor mymaster 10.0.7.53 6379 2// 当前Sentinel节点监控 127.0.0.1:6379 这个主节点// 2代表判断主节点失败至少需要2个Sentinel节点节点同意// mymaster是主节点的别名#sentinel down-after-milliseconds mymaster 30000//每个Sentinel节点都要定期PING命令来判断Redis数据节点和其余Sentinel节点是否可达，如果超过30000毫秒且没有回复，则判定不可达#sentinel parallel-syncs mymaster 1//当Sentinel节点集合对主节点故障判定达成一致时，Sentinel领导者节点会做故障转移操作，选出新的主节点，原来的从节点会向新的主节点发起复制操作，限制每次向新的主节点发起复制操作的从节点个数为1#sentinel failover-timeout mymaster 180000//故障转移超时时间为180000毫秒 4、启动服务各服务器启动redis服务1/data/redis/src/redis-server /data/redis/etc/redis.conf 各服务器启动sentinel服务1/data/redis/src/redis-sentinel /data/redis/sentinel.conf 查看集群信息1/data/redis/src/redis-cli -h 127.0.0.1 -p 26379 INFO Sentinel]]></content>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Python] python-2.7升级到python-3.7]]></title>
    <url>%2F2018%2F07%2F30%2Fpython-2.7%E5%8D%87%E7%BA%A7%E5%88%B0python-3.7%2F</url>
    <content type="text"><![CDATA[[下载、解压python3.7]12wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tar.xztar -xvf Python-3.7.0.tar.xz [安装编译]123cd Python-3.7.0/./configure --prefix=/usr/local/python3.7make &amp;&amp; make install [安装报错]12ModuleNotFoundError: No module named '_ctypes'make: *** [install] Error 1 解决方法： 安装libffi-devel yum install libffi-devel 重新编译安装 make &amp;&amp; make install [备份旧版本的python]123456789ll /usr/bin/python*lrwxrwxrwx. 1 root root 7 Apr 10 19:35 /usr/bin/python -&gt; python2lrwxrwxrwx. 1 root root 9 Apr 10 19:35 /usr/bin/python2 -&gt; python2.7-rwxr-xr-x. 1 root root 7136 Aug 4 2017 /usr/bin/python2.7-------------------------一般自带系统已经做好了python2.7的备份，直接替换掉python即可如果没有备份，使用一些命令备份：mv /usr/bin/python /usr/bin/python_old #备份旧的python [新版本python软连接到python]12345rm -rf /usr/bin/python #需要删除旧版的python,否则报错ln -s /usr/local/python3.7/bin/python3.7 /usr/bin/python #添加软连接python -V #查看python版本2.7版本没有pip,升级到python3.7后，自带有pip,做一个pip的软连接即可ln -s /usr/local/python3.7/bin/pip3 /usr/bin/pip [升级完python之后，yum命令失效，需修改配置文件]使用yum命令报以下错误：12345 yum clean all File "/usr/bin/yum", line 30 except KeyboardInterrupt, e: ^SyntaxError: invalid syntax 解决 yum 不可用：1234修改/usr/bin/yum配置文件#!/usr/bin/python 改成： #!/usr/bin/python2.7重新测试yum是否正常：yum clean all [升级完python之后，yum使用过程中，额外问题：]123456yum install tree -y ----------------------------------报错：File "/usr/libexec/urlgrabber-ext-down", line 28 except OSError, e:--------------------------------- 解决方法：12修改/usr/libexec/urlgrabber-ext-down配置文件#!/usr/bin/python 改成： #!/usr/bin/python2.7]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[About Me]]></title>
    <url>%2Fabout%2Findex.html</url>
    <content type="text"><![CDATA[愿你被这个世界温柔以待，即使生活总以刻薄荒芜相欺！]]></content>
  </entry>
  <entry>
    <title><![CDATA[tags]]></title>
    <url>%2Ftags%2Findex.html</url>
    <content type="text"></content>
  </entry>
</search>
